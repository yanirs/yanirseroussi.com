<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Making Bayesian A/B testing more accessible | Yanir Seroussi | Data science and beyond</title>
<meta name=keywords content="a/b testing,analytics,causality,data science,split testing,statistics">
<meta name=description content="Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople.">
<meta name=author content="Yanir Seroussi">
<link rel=canonical href=https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/>
<link crossorigin=anonymous href=/yanirseroussi.com/assets/css/stylesheet.min.51e68192da3381c9040a063242bafad56d6d28666fff1f9e523f9eaad0207a83.css integrity="sha256-UeaBktozgckECgYyQrr61W1tKGZv/x+eUj+eqtAgeoM=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/yanirseroussi.com/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirs.github.io/yanirseroussi.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://yanirs.github.io/yanirseroussi.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://yanirs.github.io/yanirseroussi.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://yanirs.github.io/yanirseroussi.com/apple-touch-icon.png>
<link rel=mask-icon href=https://yanirs.github.io/yanirseroussi.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Making Bayesian A/B testing more accessible">
<meta property="og:description" content="Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/">
<meta property="og:image" content="https://yanirs.github.io/yanirseroussi.com/bayesian-split-testing-calculator.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2016-06-19T10:32:15+00:00">
<meta property="article:modified_time" content="2016-06-19T10:32:15+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://yanirs.github.io/yanirseroussi.com/bayesian-split-testing-calculator.png">
<meta name=twitter:title content="Making Bayesian A/B testing more accessible">
<meta name=twitter:description content="Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirs.github.io/yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Making Bayesian A/B testing more accessible","item":"https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Making Bayesian A/B testing more accessible","name":"Making Bayesian A\/B testing more accessible","description":"Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you\u0026rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople.","keywords":["a/b testing","analytics","causality","data science","split testing","statistics"],"articleBody":"Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you’re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople. Hence, I decided to build a new Bayesian A/B testing calculator, which aims to make these concepts clear to any user. This post discusses the general problem and existing solutions, followed by a review of the new tool and how it can be improved further.\nThe problem The classic A/B testing problem is as follows. Suppose we run an experiment where we have a control group and a test group. Participants (typically website visitors) are allocated to groups randomly, and each group is presented with a different variant of the website or page (e.g., variant A is assigned to the control group and variant B is assigned to the test group). Our aim is to increase the overall number of binary successes, where success can be defined as clicking a button or opening a new account. Hence, we track the number of trials in each group together with the number of successes. For a given group, the number of successes divided by number of trials is the group’s raw success rate.\nGiven the results of an experiment (trials and successes for each group), there are a few questions we would typically like to answer:\n Should we choose variant A or variant B to maximise our success rate? How much would our success rate change if we chose one variant over the other? Do we have enough data or should we keep experimenting?  It’s important to note some points that might be obvious, but are often overlooked. First, we run an experiment because we assume that it will help us uncover a causal link, where something about A or B is hypothesised to cause people to behave differently, thereby affecting the overall success rate. Second, we want to make a decision and choose either A or B, rather than maintain multiple variants and present the best variant depending on a participant’s features (a problem that’s addressed by contextual bandits, for example). Third, online A/B testing is different from traditional experiments in a lab, because we often have little control over the characteristics of our participants, and when, where, and how they choose to interact with our experiment. This is an important point, because it means that we may need to wait a long time until we get a representative sample of the population. In addition, the raw numbers of trials and successes can’t tell us whether the sample is representative.\nBayesian solutions Many blog posts have been written on how to use Bayesian statistics to answer the above questions, so I won’t get into too much detail here (see the posts by David Robinson, Maciej Kula, Chris Stucchio, and Evan Miller if you need more background). The general idea is that we assume that the success rates for the control and test variants are drawn from Beta(αA, βA) and Beta(αB, βB), respectively, where Beta(α, β) is the beta distribution with shape parameters α and β (which yields values in the [0, 1] interval). As the experiment runs, we update the parameters of the distributions – each success gets added to the group’s α, and each unsuccessful trial gets added to the group’s β. It is often reasonable to assume that the prior (i.e., initial) values of α and β are the same for both variants. If we denote the prior values of the parameters with α and β, and the number of successes and trials for group x with Sx and Tx respectively, we get that the success rates are distributed according to Beta(α + SA, β + TA – SA) for control and Beta(α + SB, β + TB – SB) for test.\nFor example, if α = β = 1, TA = 200, SA = 120, TB = 200, and SB = 100, plotting the probability density functions yields the following chart (A – blue, B – red):\n   Given these distributions, we can calculate the most probable range for the success rate of each variant, and estimate the difference in success rate between the variants. These can be calculated by deriving closed formulas, or by drawing samples from each distribution. In addition, it is important to note that the distributions change as we gather more data, even if the raw success rates don’t. For example, multiplying each count by 10 to obtain TA = 2000, SA = 1200, TB = 2000, and SB = 1000 doesn’t change the success rates, but it does change the distributions – they become much narrower:\n   In the second case we’ve gathered ten times the data, which made the distributions much more distinct. Intuitively, this means we can now be more confident that the success rate of A is higher than that of B. Quantifying this confidence and deciding when to conclude the experiment isn’t straightforward, and should depend on factors that aren’t fully captured by the raw counts. The way I chose to address this issue is presented below, after briefly discussing existing calculators and their limitations.\nExisting online calculators The beauty of frequentist tools for significance testing is that they always give you a simple answer. For example, if we plug the numbers from the first case above (TA = 200, SA = 120, TB = 200, and SB = 100) into Evan Miller’s calculator, we get:\n   Unfortunately, both Bayesian calculators that I’m aware of have some limitations. Plugging the same numbers into the calculators by PeakConversion and Lyst would inform you that the probability of A being best is approximately 0.98, but it won’t tell you what’s the best way forward given this information. PeakConversion also outputs the 95% success rate intervals for A (between 53.1% and 66.7%) and B (between 43.1% and 56.9%), but it doesn’t let users set the prior values α and β (it uses α = β = 0.5). The ability to set priors based on what we know about our experimental setting is an important feature of Bayesian statistics that can help reduce the number of false positives. Hiding the priors in PeakConversion’s calculator makes it easier to use but less powerful than Lyst’s tool. In addition, Lyst’s calculator presents the distribution of differences between the success rates of A and B, i.e., the effect size. This is important because we may not bother implementing certain changes if the effect is negligible, even if the probability of one variant being better than the other is very close to 1.\nDespite being more powerful, I find Lyst’s calculator just a bit too technical. Specifically, setting the α and β priors requires some familiarity with the beta distribution, which many people don’t have. Also, the effect size distribution is important, but can be hard to get one’s head around. Therefore, I decided to extend Lyst’s calculator, aiming to release a new tool that is both powerful and easy to use.\nBuilding the new calculator The source code for Lyst’s calculator is available on GitHub, so I decided to use that as the foundation of the new calculator. The first step was to convert the code from HTML, CSS, and JavaScript to Jade, Sass, and CoffeeScript, and clean up some code duplication. As the calculator is served from my GitHub Pages domain, it was easiest to put all the code in that repository. Once I had an environment and codebase that I was happy with, it was time to make functional changes:\n Change the layout to be responsive, so it’d work well on mobile devices. Enable sharing of results by changing the URL when the input changes. Provide clear instructions, so that the calculator can be used by people who don’t necessarily have a strong background in statistics. Allow users to set priors based on more familiar figures than the beta distribution’s α and β priors. Make a clear and well-justified recommendation on how to proceed.  While the first two changes were straightforward to implement, the other points were somewhat more challenging. Specifically, providing clear explanations that assume little background knowledge isn’t simple, and I still feel that the current version of the new calculator is a bit too wordy (this may be improved in the future based on user feedback – suggestions welcome). Life would be easier if everyone thought of observed values as being drawn from distributions, but in my experience this is not always the case. However, I believe it is important to communicate the reality of uncertainty, so I don’t want to hide it from users of the calculator, even at the price of more elaborate explanations.\nMaking the priors more intuitive was a bit tricky. At first, I thought I’d let users state their prior knowledge in terms of the mean and variance of past performance, relying on the fact that for Beta(α, β) the mean μ is α / (α + β), and the variance σ2 is αβ / (α + β)2(α + β + 1). The problem is that while the mean is simple to set, as it is always in the (0, 1) range, the upper bound for the variance depends on the mean. Specifically, it can be shown that the variance is in the range (0, μ(1 – μ)). Therefore, I decided to let users quantify their uncertainty about the mean as a number u in the range (0, 1), where σ2 = uμ(1 – μ). Having played with the calculator a bit, I think this makes it easier to set good informative priors. It is also worth noting that I considered allowing users to set different priors for the control and test group, but decided against it to reduce complexity. In addition, it makes sense to have the same prior for both groups – if you have a strong belief or knowledge on which one is going to perform better, you probably don’t need to run an experiment.\nOne of the main reasons I decided to build the calculator was because I wanted a tool that outputs a clear recommendation. This proved to be the most challenging (and interesting) part of this project, as there are quite a few options for Bayesian stopping rules. After reading David Robinson’s review of the limitations of a stopping rule based on the expected loss, and a few of the other resources mentioned in his post, I decided to go with a combination of the third and fourth rules tested by John Kruschke. These rules rely on a threshold of caring, which is the minimum effect size that is seen as significant by the user. For example, if we’re running experiments on the conversion rate of a landing page, we may decide that we don’t care if the absolute change in conversion rate is less than 0.1%. Given this threshold and data from the experiment, the following recommendations are possible:\n Stop the experiment and implement either variant, because the difference between the variants is smaller than the threshold. Stop the experiment and implement the winning variant, because the difference between the variants is greater than the threshold. Keep running the experiment, because there isn’t enough data to make a decision.  Formally, Kruschke’s rules work as follows. Given the minimum effect threshold t, we define a region of practical equivalence (ROPE) to zero difference as the interval [-t, t]. Then, we compare the ROPE to the 95% high density interval (HDI) of the distribution of differences between A and B. When comparing the ROPE and HDI, there are three options that correspond to the recommendations above:\n The ROPE is completely contained in the HDI (stop the experiment and implement either variant). The intersection between the ROPE and HDI is empty (stop the experiment and implement the winning variant). The ROPE and HDI only partly overlap (keep running the experiment).  Kruschke’s post shows that making the rule more restrictive by adding a notion of user-settable precision can reduce the rate of false positives. The idea is to stop only if the HDI is narrower than precision multiplied by the width of the ROPE. Intuitively, this forces the experimenter to collect more data because it makes the posterior distributions narrower (as shown by the charts above). I found it hard to explain the idea of precision, and didn’t want to confuse users by adding another parameter, so I decided to use a constant precision value of 0.8. If the ROPE and HDI don’t overlap, the tool makes a recommendation to stop, accompanied by a binary level of confidence: high if the precision condition is met, and low otherwise.\nPutting in the numbers from the running example (TA = 200, SA = 120, TB = 200, and SB = 100) together with a minimum effect of 1%, prior success rate of 50%, and 57.74% uncertainty (equivalent to α = β = 1), we get the following output:\n   The full results also include plots of the distributions and their high density intervals. I’m pretty happy with the richer information provided by the calculator, though it still has some limitations and areas that can be improved.\nLimitations and potential improvements As mentioned above, I’d love to reduce the wordiness of the calculator while keeping it self-contained, but I need some feedback to understand if any explanations are redundant. It’d also be great to reduce the reliance on magic numbers, such as the 95% HDI and 0.8 precision used for generating a recommendation. However, making these settable by users would increase the complexity of using the calculator, which is already harder to use than the frequentist alternative. Nonetheless, it’s important to remember that oversimplification is the reason why it’s easier to make the wrong decision when following the classical approach.\nOther potential changes include switching to a closed-form formula rather than draws from a distribution, comparing more than two variants, and improving Kruschke’s stopping rules by simulating more scenarios than those considered in his post. In addition, I’d like to go beyond binary responses (success/failure) to support continuous rewards (e.g., revenue), and allow users to specify different costs for the variants (e.g., implementing B may cost more than sticking with A).\nFinally, it is important to keep in mind that significance testing can’t tell you whether your sample is representative of the population. For example, if you run an experiment on a very popular website, you can get a sample of thousands of people within a few minutes. Concluding an experiment based on such a sample is probably a bad idea, as it is plausible that you would reach different conclusions if you kept running the experiment for a few days, to reduce the effect that the time of day has on the results. Similarly, a few days may not be enough if your user population behaves differently on weekends – you would need to run the experiment over a few weeks. This can be extended to months and years to rule out seasonal effects, but it is up to the experimenter to weigh the practicality of considering such factors versus the need to make decisions (see articles by Peep Laja, Martin Goodson, Sam Ju, and Kohavi et al. for more details). The main thing to remember is that you just cannot completely eliminate uncertainty and the need to consider background knowledge, which is why I believe that helping more people follow the Bayesian approach is a step in the right direction.\n","wordCount":"2637","inLanguage":"en","image":"https://yanirs.github.io/yanirseroussi.com/bayesian-split-testing-calculator.png","datePublished":"2016-06-19T10:32:15Z","dateModified":"2016-06-19T10:32:15Z","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirs.github.io/yanirseroussi.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://yanirs.github.io/yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Making Bayesian A/B testing more accessible
</h1>
<div class=post-meta>June 19, 2016&nbsp;·&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2016-06-19-making-bayesian-ab-testing-more-accessible/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a>
</div>
</header>
<figure class=entry-cover>
<img loading=lazy srcset="https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_360x0_resize_box_3.png 360w ,https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_480x0_resize_box_3.png 480w ,https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_720x0_resize_box_3.png 720w ,https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_1080x0_resize_box_3.png 1080w ,https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator.png 1280w" sizes="(min-width: 768px) 720px, 100vw" src=https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator.png alt width=1280 height=600>
</figure>
<div class=post-content><p>Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that <a href=http://www.evanmiller.org/how-not-to-run-an-ab-test.html target=_blank rel=noopener>you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance</a>. One way of dealing with this issue is by <a href=http://www.evanmiller.org/bayesian-ab-testing.html target=_blank rel=noopener>following a Bayesian approach</a> to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople. Hence, I decided to build a new <a href=https://yanirs.github.io/tools/split-test-calculator/ target=_blank rel=noopener>Bayesian A/B testing calculator</a>, which aims to make these concepts clear to any user. This post discusses the general problem and existing solutions, followed by a review of the new tool and how it can be improved further.</p>
<h2 id=the-problem>The problem<a hidden class=anchor aria-hidden=true href=#the-problem>#</a></h2>
<p>The classic A/B testing problem is as follows. Suppose we run an experiment where we have a control group and a test group. Participants (typically website visitors) are allocated to groups randomly, and each group is presented with a different variant of the website or page (e.g., variant A is assigned to the control group and variant B is assigned to the test group). Our aim is to increase the overall number of binary <em>successes</em>, where success can be defined as clicking a button or opening a new account. Hence, we track the number of <em>trials</em> in each group together with the number of successes. For a given group, the number of successes divided by number of trials is the group&rsquo;s raw success rate.</p>
<p>Given the results of an experiment (trials and successes for each group), there are a few questions we would typically like to answer:</p>
<ol>
<li>Should we choose variant A or variant B to maximise our success rate?</li>
<li>How much would our success rate change if we chose one variant over the other?</li>
<li>Do we have enough data or should we keep experimenting?</li>
</ol>
<p>It&rsquo;s important to note some points that might be obvious, but are often overlooked. First, we run an experiment because we assume that it will help us uncover a <a href=http://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/>causal link</a>, where something about A or B is hypothesised to cause people to behave differently, thereby affecting the overall success rate. Second, we <em>want</em> to make a decision and choose either A or B, rather than maintain multiple variants and present the best variant depending on a participant&rsquo;s features (a problem that&rsquo;s addressed by <a href=https://en.wikipedia.org/wiki/Multi-armed_bandit#Contextual_Bandit target=_blank rel=noopener>contextual bandits</a>, for example). Third, online A/B testing is different from traditional experiments in a lab, because we often have little control over the characteristics of our participants, and when, where, and how they choose to interact with our experiment. This is an important point, because it means that we may need to wait a long time until we get a representative sample of the population. In addition, the raw numbers of trials and successes can&rsquo;t tell us whether the sample is representative.</p>
<h2 id=bayesian-solutions>Bayesian solutions<a hidden class=anchor aria-hidden=true href=#bayesian-solutions>#</a></h2>
<p>Many blog posts have been written on how to use Bayesian statistics to answer the above questions, so I won&rsquo;t get into too much detail here (see the posts by <a href=http://varianceexplained.org/r/bayesian_ab_baseball/ target=_blank rel=noopener>David Robinson</a>, <a href=http://developers.lyst.com/2014/05/10/bayesian-ab-testing/ target=_blank rel=noopener>Maciej Kula</a>, <a href=https://www.chrisstucchio.com/blog/2014/bayesian_ab_decision_rule.html target=_blank rel=noopener>Chris Stucchio</a>, and <a href=http://www.evanmiller.org/bayesian-ab-testing.html target=_blank rel=noopener>Evan Miller</a> if you need more background). The general idea is that we assume that the success rates for the control and test variants are drawn from Beta(α<sub>A</sub>, β<sub>A</sub>) and Beta(α<sub>B</sub>, β<sub>B</sub>), respectively, where Beta(α, β) is the <a href=https://en.wikipedia.org/wiki/Beta_distribution target=_blank rel=noopener>beta distribution</a> with shape parameters α and β (which yields values in the [0, 1] interval). As the experiment runs, we update the parameters of the distributions – each success gets added to the group&rsquo;s α, and each unsuccessful trial gets added to the group&rsquo;s β. It is often reasonable to assume that the prior (i.e., initial) values of α and β are the same for both variants. If we denote the prior values of the parameters with α<sub></sub> and β<sub></sub>, and the number of successes and trials for group x with S<sub>x</sub> and T<sub>x</sub> respectively, we get that the success rates are distributed according to Beta(α<sub></sub> + S<sub>A</sub>, β<sub></sub> + T<sub>A</sub> – S<sub>A</sub>) for control and Beta(α<sub></sub> + S<sub>B</sub>, β<sub></sub> + T<sub>B</sub> – S<sub>B</sub>) for test.</p>
<p>For example, if α<sub></sub> = β<sub></sub> = 1, T<sub>A</sub> = 200, S<sub>A</sub> = 120, T<sub>B</sub> = 200, and S<sub>B</sub> = 100, plotting the probability density functions yields the following chart (A – blue, B – red):</p>
<figure>
<a href=beta-distributions-examples.png target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 614px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples_hu6083fd67121821db147a70ce91579621_12977_360x0_resize_box_3.png 360w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples_hu6083fd67121821db147a70ce91579621_12977_480x0_resize_box_3.png 480w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples.png 614w," src=https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples.png alt="Beta distributions examples" loading=lazy>
</a>
</figure>
<p>Given these distributions, we can calculate the most probable range for the success rate of each variant, and estimate the difference in success rate between the variants. These can be calculated by <a href=http://www.evanmiller.org/bayesian-ab-testing.html target=_blank rel=noopener>deriving closed formulas</a>, or by <a href=http://varianceexplained.org/r/bayesian_ab_baseball/ target=_blank rel=noopener>drawing samples from each distribution</a>. In addition, it is important to note that the distributions change as we gather more data, even if the raw success rates don&rsquo;t. For example, multiplying each count by 10 to obtain T<sub>A</sub> = 2000, S<sub>A</sub> = 1200, T<sub>B</sub> = 2000, and S<sub>B</sub> = 1000 doesn&rsquo;t change the success rates, but it does change the distributions – they become much narrower:</p>
<figure>
<a href=beta-distributions-examples-narrower.png target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 613px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower_hu8e063ca70db0fd2bd892a5a849263287_11048_360x0_resize_box_3.png 360w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower_hu8e063ca70db0fd2bd892a5a849263287_11048_480x0_resize_box_3.png 480w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower.png 613w," src=https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower.png alt="Narrower beta distributions" loading=lazy>
</a>
</figure>
<p>In the second case we&rsquo;ve gathered ten times the data, which made the distributions much more distinct. Intuitively, this means we can now be more confident that the success rate of A is higher than that of B. Quantifying this confidence and deciding when to conclude the experiment isn&rsquo;t straightforward, and should depend on factors that aren&rsquo;t fully captured by the raw counts. The way I chose to address this issue is presented below, after briefly discussing existing calculators and their limitations.</p>
<h2 id=existing-online-calculators>Existing online calculators<a hidden class=anchor aria-hidden=true href=#existing-online-calculators>#</a></h2>
<p>The beauty of frequentist tools for significance testing is that they always give you a simple answer. For example, if we plug the numbers from the first case above (T<sub>A</sub> = 200, S<sub>A</sub> = 120, T<sub>B</sub> = 200, and S<sub>B</sub> = 100) into <a href=http://www.evanmiller.org/ab-testing/chi-squared.html target=_blank rel="nofollow noopener">Evan Miller&rsquo;s calculator</a>, we get:</p>
<figure>
<a href=http://www.evanmiller.org/ab-testing/chi-squared.html#!120/200;100/200@95 target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 720px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example_hub35458121a77f42ff6b635a9c970e421_37321_360x0_resize_box_3.png 360w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example_hub35458121a77f42ff6b635a9c970e421_37321_480x0_resize_box_3.png 480w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example_hub35458121a77f42ff6b635a9c970e421_37321_720x0_resize_box_3.png 720w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example.png 788w," src=https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example.png alt="Chi-Squared test example" loading=lazy>
</a>
</figure>
<p>Unfortunately, both Bayesian calculators that I&rsquo;m aware of have some limitations. Plugging the same numbers into the calculators by <a href=https://www.peakconversion.com/2012/02/ab-split-test-graphical-calculator/ target=_blank rel="nofollow noopener">PeakConversion</a> and <a href=http://developers.lyst.com/bayesian-calculator/ target=_blank rel="nofollow noopener">Lyst</a> would inform you that the probability of A being best is approximately 0.98, but it won&rsquo;t tell you what&rsquo;s the best way forward given this information. PeakConversion also outputs the 95% success rate intervals for A (between 53.1% and 66.7%) and B (between 43.1% and 56.9%), but it doesn&rsquo;t let users set the prior values α<sub></sub> and β<sub></sub> (it uses α<sub></sub> = β<sub></sub> = 0.5). The ability to set priors based on what we know about our experimental setting is an important feature of Bayesian statistics <a href=http://developers.lyst.com/2014/05/10/bayesian-ab-testing/ target=_blank rel=noopener>that can help reduce the number of false positives</a>. Hiding the priors in PeakConversion&rsquo;s calculator makes it easier to use but less powerful than Lyst&rsquo;s tool. In addition, Lyst&rsquo;s calculator presents the distribution of differences between the success rates of A and B, i.e., the <em>effect size</em>. This is important because we may not bother implementing certain changes if the effect is negligible, even if the probability of one variant being better than the other is very close to 1.</p>
<p>Despite being more powerful, I find Lyst&rsquo;s calculator just a bit too technical. Specifically, setting the α<sub></sub> and β<sub></sub> priors requires some familiarity with the beta distribution, which many people don&rsquo;t have. Also, the effect size distribution is important, but can be hard to get one&rsquo;s head around. Therefore, I decided to extend Lyst&rsquo;s calculator, aiming to release a new tool that is both powerful and easy to use.</p>
<h2 id=building-the-new-calculator>Building the new calculator<a hidden class=anchor aria-hidden=true href=#building-the-new-calculator>#</a></h2>
<p>The source code for Lyst&rsquo;s calculator is <a href=https://github.com/ssaw/BayesianTestJS target=_blank rel=noopener>available on GitHub</a>, so I decided to use that as the foundation of the new calculator. The first step was to convert the code from HTML, CSS, and JavaScript to Jade, Sass, and CoffeeScript, and clean up some code duplication. As the calculator is served from my <a href=https://yanirs.github.io/tools/split-test-calculator/ target=_blank rel=noopener>GitHub Pages domain</a>, it was easiest to put <a href=https://github.com/yanirs/yanirs.github.io/tree/master/tools/split-test-calculator/src target=_blank rel=noopener>all the code</a> in that repository. Once I had an environment and codebase that I was happy with, it was time to make functional changes:</p>
<ul>
<li>Change the layout to be responsive, so it&rsquo;d work well on mobile devices.</li>
<li>Enable sharing of results by changing the URL when the input changes.</li>
<li>Provide clear instructions, so that the calculator can be used by people who don&rsquo;t necessarily have a strong background in statistics.</li>
<li>Allow users to set priors based on more familiar figures than the beta distribution&rsquo;s α<sub></sub> and β<sub></sub> priors.</li>
<li>Make a clear and well-justified recommendation on how to proceed.</li>
</ul>
<p>While the first two changes were straightforward to implement, the other points were somewhat more challenging. Specifically, providing clear explanations that assume little background knowledge isn&rsquo;t simple, and I still feel that the current version of the new calculator is a bit too wordy (this may be improved in the future based on user feedback – suggestions welcome). Life would be easier if everyone thought of observed values as being drawn from distributions, but in my experience this is not always the case. However, I believe it is important to communicate the reality of uncertainty, so I don&rsquo;t want to hide it from users of the calculator, even at the price of more elaborate explanations.</p>
<p>Making the priors more intuitive was a bit tricky. At first, I thought I&rsquo;d let users state their prior knowledge in terms of the mean and variance of past performance, relying on the fact that for Beta(α, β) the mean μ is α / (α + β), and the variance σ<sup>2</sup> is αβ / (α + β)<sup>2</sup>(α + β + 1). The problem is that while the mean is simple to set, as it is always in the (0, 1) range, the upper bound for the variance depends on the mean. Specifically, <a href=http://stats.stackexchange.com/a/12239 target=_blank rel=noopener>it can be shown</a> that the variance is in the range (0, μ(1 – μ)). Therefore, I decided to let users quantify their uncertainty about the mean as a number <em>u</em> in the range (0, 1), where σ<sup>2</sup> = <em>u</em>μ(1 – μ). Having played with the calculator a bit, I think this makes it easier to set good informative priors. It is also worth noting that I considered allowing users to set different priors for the control and test group, but decided against it to reduce complexity. In addition, it makes sense to have the same prior for both groups – if you have a strong belief or knowledge on which one is going to perform better, you probably don&rsquo;t need to run an experiment.</p>
<p>One of the main reasons I decided to build the calculator was because I wanted a tool that outputs a clear recommendation. This proved to be the most challenging (and interesting) part of this project, as there are quite a few options for Bayesian stopping rules. After reading <a href=http://varianceexplained.org/r/bayesian-ab-testing/ target=_blank rel=noopener>David Robinson&rsquo;s review of the limitations of a stopping rule based on the expected loss</a>, and a few of the other resources mentioned in his post, I decided to go with a combination of <a href=http://doingbayesiandataanalysis.blogspot.com.au/2013/11/optional-stopping-in-data-collection-p.html target=_blank rel=noopener>the third and fourth rules tested by John Kruschke</a>. These rules rely on a threshold of caring, which is the minimum effect size that is seen as significant by the user. For example, if we&rsquo;re running experiments on the conversion rate of a landing page, we may decide that we don&rsquo;t care if the absolute change in conversion rate is less than 0.1%. Given this threshold and data from the experiment, the following recommendations are possible:</p>
<ol>
<li><em>Stop the experiment and implement either variant</em>, because the difference between the variants is smaller than the threshold.</li>
<li><em>Stop the experiment and implement the winning variant</em>, because the difference between the variants is greater than the threshold.</li>
<li><em>Keep running the experiment</em>, because there isn&rsquo;t enough data to make a decision.</li>
</ol>
<p>Formally, Kruschke&rsquo;s rules work as follows. Given the minimum effect threshold <em>t</em>, we define a region of practical equivalence (ROPE) to zero difference as the interval [-<em>t</em>, <em>t</em>]. Then, we compare the ROPE to the 95% high density interval (HDI) of the distribution of differences between A and B. When comparing the ROPE and HDI, there are three options that correspond to the recommendations above:</p>
<ol>
<li>The ROPE is completely contained in the HDI (stop the experiment and implement either variant).</li>
<li>The intersection between the ROPE and HDI is empty (stop the experiment and implement the winning variant).</li>
<li>The ROPE and HDI only partly overlap (keep running the experiment).</li>
</ol>
<p>Kruschke&rsquo;s post shows that making the rule more restrictive by adding a notion of user-settable <em>precision</em> can reduce the rate of false positives. The idea is to stop only if the HDI is narrower than precision multiplied by the width of the ROPE. Intuitively, this forces the experimenter to collect more data because it makes the posterior distributions narrower (as shown by the charts above). I found it hard to explain the idea of precision, and didn&rsquo;t want to confuse users by adding another parameter, so I decided to use a constant precision value of 0.8. If the ROPE and HDI don&rsquo;t overlap, the tool makes a recommendation to stop, accompanied by a binary level of confidence: <em>high</em> if the precision condition is met, and <em>low</em> otherwise.</p>
<p>Putting in the numbers from the running example (T<sub>A</sub> = 200, S<sub>A</sub> = 120, T<sub>B</sub> = 200, and S<sub>B</sub> = 100) together with a minimum effect of 1%, prior success rate of 50%, and 57.74% uncertainty (equivalent to α<sub></sub> = β<sub></sub> = 1), we get the following output:</p>
<figure>
<a href="https://yanirs.github.io/tools/split-test-calculator/#prior-mean=50,prior-uncertainty=57.74,minimum-effect=1,control-trials=200,control-successes=120,test-trials=200,test-successes=100" target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 464px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/calcualtor-recommendation-example_hud06c7fb294d1b83870b027778dfd8316_38185_360x0_resize_box_3.png 360w,
https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/calcualtor-recommendation-example.png 464w," src=https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/calcualtor-recommendation-example.png alt="Calculator recommendation example" loading=lazy>
</a>
</figure>
<p><a href="https://yanirs.github.io/tools/split-test-calculator/#prior-mean=50,prior-uncertainty=57.74,minimum-effect=1,control-trials=200,control-successes=120,test-trials=200,test-successes=100" target=_blank rel=noopener>The full results</a> also include plots of the distributions and their high density intervals. I&rsquo;m pretty happy with the richer information provided by the calculator, though it still has some limitations and areas that can be improved.</p>
<h2 id=limitations-and-potential-improvements>Limitations and potential improvements<a hidden class=anchor aria-hidden=true href=#limitations-and-potential-improvements>#</a></h2>
<p>As mentioned above, I&rsquo;d love to reduce the wordiness of the calculator while keeping it self-contained, but I need some feedback to understand if any explanations are redundant. It&rsquo;d also be great to reduce the reliance on magic numbers, such as the 95% HDI and 0.8 precision used for generating a recommendation. However, making these settable by users would increase the complexity of using the calculator, which is already harder to use than the frequentist alternative. Nonetheless, it&rsquo;s important to remember that oversimplification is the reason why it&rsquo;s easier to make the wrong decision when following the classical approach.</p>
<p>Other potential changes include <a href=http://varianceexplained.org/r/bayesian_ab_baseball/ target=_blank rel=noopener>switching to a closed-form formula rather than draws from a distribution</a>, comparing more than two variants, and improving Kruschke&rsquo;s stopping rules by simulating more scenarios than those considered in his post. In addition, I&rsquo;d like to go beyond binary responses (success/failure) to support continuous rewards (e.g., revenue), and allow users to specify different costs for the variants (e.g., implementing B may cost more than sticking with A).</p>
<p>Finally, it is important to keep in mind that significance testing can&rsquo;t tell you whether your sample is representative of the population. For example, if you run an experiment on a very popular website, you can get a sample of thousands of people within a few minutes. Concluding an experiment based on such a sample is probably a bad idea, as it is plausible that you would reach different conclusions if you kept running the experiment for a few days, to reduce the effect that the time of day has on the results. Similarly, a few days may not be enough if your user population behaves differently on weekends – you would need to run the experiment over a few weeks. This can be extended to months and years to rule out seasonal effects, but it is up to the experimenter to weigh the practicality of considering such factors versus the need to make decisions (see articles by <a href=http://conversionxl.com/statistical-significance-does-not-equal-validity/ target=_blank rel=noopener>Peep Laja</a>, <a href=http://www.qubit.com/sites/default/files/pdf/mostwinningabtestresultsareillusory_0.pdf target=_blank rel=noopener>Martin Goodson</a>, <a href=https://blog.crazyegg.com/2016/03/22/anti-cookbook-ab-testing/ target=_blank rel=noopener>Sam Ju</a>, and <a href=http://www.exp-platform.com/Documents/2014%20experimentersRulesOfThumb.pdf target=_blank rel=noopener>Kohavi et al.</a> for more details). The main thing to remember is that <strong>you just cannot completely eliminate uncertainty and the need to consider background knowledge</strong>, which is why I believe that helping more people follow the Bayesian approach is a step in the right direction.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/a/b-testing/>a/b testing</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/analytics/>analytics</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/causality/>causality</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/data-science/>data science</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/split-testing/>split testing</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/statistics/>statistics</a></li>
</ul>
</footer><section class=comment-section>
<strong>No comments</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirs.github.io%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>
Comment via GitHub issue
</a>
</section>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://yanirs.github.io/yanirseroussi.com/>Yanir Seroussi | Data science and beyond</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>