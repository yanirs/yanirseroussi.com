<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Making Bayesian A/B testing more accessible | Yanir Seroussi | Data science and beyond</title><meta name=keywords content="a/b testing,analytics,causal inference,data science,statistics"><meta name=description content="Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople."><meta name=author content="Yanir Seroussi"><link rel=canonical href=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/><meta name=google-site-verification content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU"><link crossorigin=anonymous href=/assets/css/stylesheet.min.996d6f326da619bf45c0777c3dd142d5971270a6c5fbc0bc114954fffbb839e3.css integrity="sha256-mW1vMm2mGb9FwHd8PdFC1ZcScKbF+8C8EUlU//u4OeM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirseroussi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yanirseroussi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yanirseroussi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yanirseroussi.com/apple-touch-icon.png><link rel=mask-icon href=https://yanirseroussi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.102.3"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Making Bayesian A/B testing more accessible"><meta property="og:description" content="Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople."><meta property="og:type" content="article"><meta property="og:url" content="https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/"><meta property="og:image" content="https://yanirseroussi.com/bayesian-split-testing-calculator.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-06-19T10:32:15+00:00"><meta property="article:modified_time" content="2022-01-17T09:00:05+10:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yanirseroussi.com/bayesian-split-testing-calculator.png"><meta name=twitter:title content="Making Bayesian A/B testing more accessible"><meta name=twitter:description content="Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Making Bayesian A/B testing more accessible","item":"https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Making Bayesian A/B testing more accessible","name":"Making Bayesian A\/B testing more accessible","description":"Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you\u0026rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople.","keywords":["a/b testing","analytics","causal inference","data science","statistics"],"articleBody":"Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you’re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople. Hence, I decided to build a new Bayesian A/B testing calculator, which aims to make these concepts clear to any user. This post discusses the general problem and existing solutions, followed by a review of the new tool and how it can be improved further.\nThe problem The classic A/B testing problem is as follows. Suppose we run an experiment where we have a control group and a test group. Participants (typically website visitors) are allocated to groups randomly, and each group is presented with a different variant of the website or page (e.g., variant A is assigned to the control group and variant B is assigned to the test group). Our aim is to increase the overall number of binary successes, where success can be defined as clicking a button or opening a new account. Hence, we track the number of trials in each group together with the number of successes. For a given group, the number of successes divided by number of trials is the group’s raw success rate.\nGiven the results of an experiment (trials and successes for each group), there are a few questions we would typically like to answer:\nShould we choose variant A or variant B to maximise our success rate? How much would our success rate change if we chose one variant over the other? Do we have enough data or should we keep experimenting? It’s important to note some points that might be obvious, but are often overlooked. First, we run an experiment because we assume that it will help us uncover a causal link, where something about A or B is hypothesised to cause people to behave differently, thereby affecting the overall success rate. Second, we want to make a decision and choose either A or B, rather than maintain multiple variants and present the best variant depending on a participant’s features (a problem that’s addressed by contextual bandits, for example). Third, online A/B testing is different from traditional experiments in a lab, because we often have little control over the characteristics of our participants, and when, where, and how they choose to interact with our experiment. This is an important point, because it means that we may need to wait a long time until we get a representative sample of the population. In addition, the raw numbers of trials and successes can’t tell us whether the sample is representative.\nBayesian solutions Many blog posts have been written on how to use Bayesian statistics to answer the above questions, so I won’t get into too much detail here (see the posts by David Robinson, Maciej Kula, Chris Stucchio, and Evan Miller if you need more background). The general idea is that we assume that the success rates for the control and test variants are drawn from Beta(αA, βA) and Beta(αB, βB), respectively, where Beta(α, β) is the beta distribution with shape parameters α and β (which yields values in the [0, 1] interval). As the experiment runs, we update the parameters of the distributions – each success gets added to the group’s α, and each unsuccessful trial gets added to the group’s β. It is often reasonable to assume that the prior (i.e., initial) values of α and β are the same for both variants. If we denote the prior values of the parameters with α and β, and the number of successes and trials for group x with Sx and Tx respectively, we get that the success rates are distributed according to Beta(α + SA, β + TA – SA) for control and Beta(α + SB, β + TB – SB) for test.\nFor example, if α = β = 1, TA = 200, SA = 120, TB = 200, and SB = 100, plotting the probability density functions yields the following chart (A – blue, B – red):\nGiven these distributions, we can calculate the most probable range for the success rate of each variant, and estimate the difference in success rate between the variants. These can be calculated by deriving closed formulas, or by drawing samples from each distribution. In addition, it is important to note that the distributions change as we gather more data, even if the raw success rates don’t. For example, multiplying each count by 10 to obtain TA = 2000, SA = 1200, TB = 2000, and SB = 1000 doesn’t change the success rates, but it does change the distributions – they become much narrower:\nIn the second case we’ve gathered ten times the data, which made the distributions much more distinct. Intuitively, this means we can now be more confident that the success rate of A is higher than that of B. Quantifying this confidence and deciding when to conclude the experiment isn’t straightforward, and should depend on factors that aren’t fully captured by the raw counts. The way I chose to address this issue is presented below, after briefly discussing existing calculators and their limitations.\nExisting online calculators The beauty of frequentist tools for significance testing is that they always give you a simple answer. For example, if we plug the numbers from the first case above (TA = 200, SA = 120, TB = 200, and SB = 100) into Evan Miller’s calculator, we get:\nUnfortunately, both Bayesian calculators that I’m aware of have some limitations. Plugging the same numbers into the calculators by PeakConversion and Lyst would inform you that the probability of A being best is approximately 0.98, but it won’t tell you what’s the best way forward given this information. PeakConversion also outputs the 95% success rate intervals for A (between 53.1% and 66.7%) and B (between 43.1% and 56.9%), but it doesn’t let users set the prior values α and β (it uses α = β = 0.5). The ability to set priors based on what we know about our experimental setting is an important feature of Bayesian statistics that can help reduce the number of false positives. Hiding the priors in PeakConversion’s calculator makes it easier to use but less powerful than Lyst’s tool. In addition, Lyst’s calculator presents the distribution of differences between the success rates of A and B, i.e., the effect size. This is important because we may not bother implementing certain changes if the effect is negligible, even if the probability of one variant being better than the other is very close to 1.\nDespite being more powerful, I find Lyst’s calculator just a bit too technical. Specifically, setting the α and β priors requires some familiarity with the beta distribution, which many people don’t have. Also, the effect size distribution is important, but can be hard to get one’s head around. Therefore, I decided to extend Lyst’s calculator, aiming to release a new tool that is both powerful and easy to use.\nBuilding the new calculator The source code for Lyst’s calculator is available on GitHub, so I decided to use that as the foundation of the new calculator. The first step was to convert the code from HTML, CSS, and JavaScript to Jade, Sass, and CoffeeScript, and clean up some code duplication. As the calculator is served from my GitHub Pages domain, it was easiest to put all the code in that repository. Once I had an environment and codebase that I was happy with, it was time to make functional changes:\nChange the layout to be responsive, so it’d work well on mobile devices. Enable sharing of results by changing the URL when the input changes. Provide clear instructions, so that the calculator can be used by people who don’t necessarily have a strong background in statistics. Allow users to set priors based on more familiar figures than the beta distribution’s α and β priors. Make a clear and well-justified recommendation on how to proceed. While the first two changes were straightforward to implement, the other points were somewhat more challenging. Specifically, providing clear explanations that assume little background knowledge isn’t simple, and I still feel that the current version of the new calculator is a bit too wordy (this may be improved in the future based on user feedback – suggestions welcome). Life would be easier if everyone thought of observed values as being drawn from distributions, but in my experience this is not always the case. However, I believe it is important to communicate the reality of uncertainty, so I don’t want to hide it from users of the calculator, even at the price of more elaborate explanations.\nMaking the priors more intuitive was a bit tricky. At first, I thought I’d let users state their prior knowledge in terms of the mean and variance of past performance, relying on the fact that for Beta(α, β) the mean μ is α / (α + β), and the variance σ2 is αβ / (α + β)2(α + β + 1). The problem is that while the mean is simple to set, as it is always in the (0, 1) range, the upper bound for the variance depends on the mean. Specifically, it can be shown that the variance is in the range (0, μ(1 – μ)). Therefore, I decided to let users quantify their uncertainty about the mean as a number u in the range (0, 1), where σ2 = uμ(1 – μ). Having played with the calculator a bit, I think this makes it easier to set good informative priors. It is also worth noting that I considered allowing users to set different priors for the control and test group, but decided against it to reduce complexity. In addition, it makes sense to have the same prior for both groups – if you have a strong belief or knowledge on which one is going to perform better, you probably don’t need to run an experiment.\nOne of the main reasons I decided to build the calculator was because I wanted a tool that outputs a clear recommendation. This proved to be the most challenging (and interesting) part of this project, as there are quite a few options for Bayesian stopping rules. After reading David Robinson’s review of the limitations of a stopping rule based on the expected loss, and a few of the other resources mentioned in his post, I decided to go with a combination of the third and fourth rules tested by John Kruschke. These rules rely on a threshold of caring, which is the minimum effect size that is seen as significant by the user. For example, if we’re running experiments on the conversion rate of a landing page, we may decide that we don’t care if the absolute change in conversion rate is less than 0.1%. Given this threshold and data from the experiment, the following recommendations are possible:\nStop the experiment and implement either variant, because the difference between the variants is smaller than the threshold. Stop the experiment and implement the winning variant, because the difference between the variants is greater than the threshold. Keep running the experiment, because there isn’t enough data to make a decision. Formally, Kruschke’s rules work as follows. Given the minimum effect threshold t, we define a region of practical equivalence (ROPE) to zero difference as the interval [-t, t]. Then, we compare the ROPE to the 95% high density interval (HDI) of the distribution of differences between A and B. When comparing the ROPE and HDI, there are three options that correspond to the recommendations above:\nThe ROPE is completely contained in the HDI (stop the experiment and implement either variant). The intersection between the ROPE and HDI is empty (stop the experiment and implement the winning variant). The ROPE and HDI only partly overlap (keep running the experiment). Kruschke’s post shows that making the rule more restrictive by adding a notion of user-settable precision can reduce the rate of false positives. The idea is to stop only if the HDI is narrower than precision multiplied by the width of the ROPE. Intuitively, this forces the experimenter to collect more data because it makes the posterior distributions narrower (as shown by the charts above). I found it hard to explain the idea of precision, and didn’t want to confuse users by adding another parameter, so I decided to use a constant precision value of 0.8. If the ROPE and HDI don’t overlap, the tool makes a recommendation to stop, accompanied by a binary level of confidence: high if the precision condition is met, and low otherwise.\nPutting in the numbers from the running example (TA = 200, SA = 120, TB = 200, and SB = 100) together with a minimum effect of 1%, prior success rate of 50%, and 57.74% uncertainty (equivalent to α = β = 1), we get the following output:\nThe full results also include plots of the distributions and their high density intervals. I’m pretty happy with the richer information provided by the calculator, though it still has some limitations and areas that can be improved.\nLimitations and potential improvements As mentioned above, I’d love to reduce the wordiness of the calculator while keeping it self-contained, but I need some feedback to understand if any explanations are redundant. It’d also be great to reduce the reliance on magic numbers, such as the 95% HDI and 0.8 precision used for generating a recommendation. However, making these settable by users would increase the complexity of using the calculator, which is already harder to use than the frequentist alternative. Nonetheless, it’s important to remember that oversimplification is the reason why it’s easier to make the wrong decision when following the classical approach.\nOther potential changes include switching to a closed-form formula rather than draws from a distribution, comparing more than two variants, and improving Kruschke’s stopping rules by simulating more scenarios than those considered in his post. In addition, I’d like to go beyond binary responses (success/failure) to support continuous rewards (e.g., revenue), and allow users to specify different costs for the variants (e.g., implementing B may cost more than sticking with A).\nFinally, it is important to keep in mind that significance testing can’t tell you whether your sample is representative of the population. For example, if you run an experiment on a very popular website, you can get a sample of thousands of people within a few minutes. Concluding an experiment based on such a sample is probably a bad idea, as it is plausible that you would reach different conclusions if you kept running the experiment for a few days, to reduce the effect that the time of day has on the results. Similarly, a few days may not be enough if your user population behaves differently on weekends – you would need to run the experiment over a few weeks. This can be extended to months and years to rule out seasonal effects, but it is up to the experimenter to weigh the practicality of considering such factors versus the need to make decisions (see articles by Peep Laja, Martin Goodson, Sam Ju, and Kohavi et al. for more details). The main thing to remember is that you just cannot completely eliminate uncertainty and the need to consider background knowledge, which is why I believe that helping more people follow the Bayesian approach is a step in the right direction.\n","wordCount":"2637","inLanguage":"en","image":"https://yanirseroussi.com/bayesian-split-testing-calculator.png","datePublished":"2016-06-19T10:32:15Z","dateModified":"2022-01-17T09:00:05+10:00","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirseroussi.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://yanirseroussi.com/about/ title=About><span>About</span></a></li><li><a href=https://yanirseroussi.com/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://yanirseroussi.com/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Making Bayesian A/B testing more accessible</h1><div class=post-meta>June 19, 2016&nbsp;·&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2016-06-19-making-bayesian-ab-testing-more-accessible/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a></div></header><figure class=entry-cover><img loading=lazy srcset="https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_360x0_resize_box_3.png 360w ,https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_480x0_resize_box_3.png 480w ,https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_720x0_resize_box_3.png 720w ,https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator_hu2128e6cfab878bae9a83560d8015bf85_45345_1080x0_resize_box_3.png 1080w ,https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator.png 1280w" sizes="(min-width: 768px) 720px, 100vw" src=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/bayesian-split-testing-calculator.png alt width=1280 height=600></figure><div class=post-content><p>Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that <a href=http://www.evanmiller.org/how-not-to-run-an-ab-test.html target=_blank rel=noopener>you&rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance</a>. One way of dealing with this issue is by <a href=http://www.evanmiller.org/bayesian-ab-testing.html target=_blank rel=noopener>following a Bayesian approach</a> to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople. Hence, I decided to build a new <a href=https://yanirs.github.io/tools/split-test-calculator/ target=_blank rel=noopener>Bayesian A/B testing calculator</a>, which aims to make these concepts clear to any user. This post discusses the general problem and existing solutions, followed by a review of the new tool and how it can be improved further.</p><h2 id=the-problem>The problem<a hidden class=anchor aria-hidden=true href=#the-problem>#</a></h2><p>The classic A/B testing problem is as follows. Suppose we run an experiment where we have a control group and a test group. Participants (typically website visitors) are allocated to groups randomly, and each group is presented with a different variant of the website or page (e.g., variant A is assigned to the control group and variant B is assigned to the test group). Our aim is to increase the overall number of binary <em>successes</em>, where success can be defined as clicking a button or opening a new account. Hence, we track the number of <em>trials</em> in each group together with the number of successes. For a given group, the number of successes divided by number of trials is the group&rsquo;s raw success rate.</p><p>Given the results of an experiment (trials and successes for each group), there are a few questions we would typically like to answer:</p><ol><li>Should we choose variant A or variant B to maximise our success rate?</li><li>How much would our success rate change if we chose one variant over the other?</li><li>Do we have enough data or should we keep experimenting?</li></ol><p>It&rsquo;s important to note some points that might be obvious, but are often overlooked. First, we run an experiment because we assume that it will help us uncover a <a href=https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/>causal link</a>, where something about A or B is hypothesised to cause people to behave differently, thereby affecting the overall success rate. Second, we <em>want</em> to make a decision and choose either A or B, rather than maintain multiple variants and present the best variant depending on a participant&rsquo;s features (a problem that&rsquo;s addressed by <a href=https://en.wikipedia.org/wiki/Multi-armed_bandit#Contextual_Bandit target=_blank rel=noopener>contextual bandits</a>, for example). Third, online A/B testing is different from traditional experiments in a lab, because we often have little control over the characteristics of our participants, and when, where, and how they choose to interact with our experiment. This is an important point, because it means that we may need to wait a long time until we get a representative sample of the population. In addition, the raw numbers of trials and successes can&rsquo;t tell us whether the sample is representative.</p><h2 id=bayesian-solutions>Bayesian solutions<a hidden class=anchor aria-hidden=true href=#bayesian-solutions>#</a></h2><p>Many blog posts have been written on how to use Bayesian statistics to answer the above questions, so I won&rsquo;t get into too much detail here (see the posts by <a href=http://varianceexplained.org/r/bayesian_ab_baseball/ target=_blank rel=noopener>David Robinson</a>, <a href=http://developers.lyst.com/2014/05/10/bayesian-ab-testing/ target=_blank rel=noopener>Maciej Kula</a>, <a href=https://www.chrisstucchio.com/blog/2014/bayesian_ab_decision_rule.html target=_blank rel=noopener>Chris Stucchio</a>, and <a href=http://www.evanmiller.org/bayesian-ab-testing.html target=_blank rel=noopener>Evan Miller</a> if you need more background). The general idea is that we assume that the success rates for the control and test variants are drawn from Beta(α<sub>A</sub>, β<sub>A</sub>) and Beta(α<sub>B</sub>, β<sub>B</sub>), respectively, where Beta(α, β) is the <a href=https://en.wikipedia.org/wiki/Beta_distribution target=_blank rel=noopener>beta distribution</a> with shape parameters α and β (which yields values in the [0, 1] interval). As the experiment runs, we update the parameters of the distributions – each success gets added to the group&rsquo;s α, and each unsuccessful trial gets added to the group&rsquo;s β. It is often reasonable to assume that the prior (i.e., initial) values of α and β are the same for both variants. If we denote the prior values of the parameters with α<sub></sub> and β<sub></sub>, and the number of successes and trials for group x with S<sub>x</sub> and T<sub>x</sub> respectively, we get that the success rates are distributed according to Beta(α<sub></sub> + S<sub>A</sub>, β<sub></sub> + T<sub>A</sub> – S<sub>A</sub>) for control and Beta(α<sub></sub> + S<sub>B</sub>, β<sub></sub> + T<sub>B</sub> – S<sub>B</sub>) for test.</p><p>For example, if α<sub></sub> = β<sub></sub> = 1, T<sub>A</sub> = 200, S<sub>A</sub> = 120, T<sub>B</sub> = 200, and S<sub>B</sub> = 100, plotting the probability density functions yields the following chart (A – blue, B – red):</p><figure><a href=beta-distributions-examples.png target=_blank rel=noopener><img sizes="
          (min-width: 768px) 614px,
          100vw
        " srcset="https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples_hu6083fd67121821db147a70ce91579621_12977_360x0_resize_box_3.png 360w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples_hu6083fd67121821db147a70ce91579621_12977_480x0_resize_box_3.png 480w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples.png 614w," src=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples.png alt="Beta distributions examples" loading=lazy></a></figure><p>Given these distributions, we can calculate the most probable range for the success rate of each variant, and estimate the difference in success rate between the variants. These can be calculated by <a href=http://www.evanmiller.org/bayesian-ab-testing.html target=_blank rel=noopener>deriving closed formulas</a>, or by <a href=http://varianceexplained.org/r/bayesian_ab_baseball/ target=_blank rel=noopener>drawing samples from each distribution</a>. In addition, it is important to note that the distributions change as we gather more data, even if the raw success rates don&rsquo;t. For example, multiplying each count by 10 to obtain T<sub>A</sub> = 2000, S<sub>A</sub> = 1200, T<sub>B</sub> = 2000, and S<sub>B</sub> = 1000 doesn&rsquo;t change the success rates, but it does change the distributions – they become much narrower:</p><figure><a href=beta-distributions-examples-narrower.png target=_blank rel=noopener><img sizes="
          (min-width: 768px) 613px,
          100vw
        " srcset="https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower_hu8e063ca70db0fd2bd892a5a849263287_11048_360x0_resize_box_3.png 360w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower_hu8e063ca70db0fd2bd892a5a849263287_11048_480x0_resize_box_3.png 480w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower.png 613w," src=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/beta-distributions-examples-narrower.png alt="Narrower beta distributions" loading=lazy></a></figure><p>In the second case we&rsquo;ve gathered ten times the data, which made the distributions much more distinct. Intuitively, this means we can now be more confident that the success rate of A is higher than that of B. Quantifying this confidence and deciding when to conclude the experiment isn&rsquo;t straightforward, and should depend on factors that aren&rsquo;t fully captured by the raw counts. The way I chose to address this issue is presented below, after briefly discussing existing calculators and their limitations.</p><h2 id=existing-online-calculators>Existing online calculators<a hidden class=anchor aria-hidden=true href=#existing-online-calculators>#</a></h2><p>The beauty of frequentist tools for significance testing is that they always give you a simple answer. For example, if we plug the numbers from the first case above (T<sub>A</sub> = 200, S<sub>A</sub> = 120, T<sub>B</sub> = 200, and S<sub>B</sub> = 100) into <a href=http://www.evanmiller.org/ab-testing/chi-squared.html target=_blank rel="nofollow noopener">Evan Miller&rsquo;s calculator</a>, we get:</p><figure><a href=http://www.evanmiller.org/ab-testing/chi-squared.html#!120/200;100/200@95 target=_blank rel=noopener><img sizes="
          (min-width: 768px) 720px,
          100vw
        " srcset="https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example_hub35458121a77f42ff6b635a9c970e421_37321_360x0_resize_box_3.png 360w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example_hub35458121a77f42ff6b635a9c970e421_37321_480x0_resize_box_3.png 480w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example_hub35458121a77f42ff6b635a9c970e421_37321_720x0_resize_box_3.png 720w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example.png 788w," src=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/chi-squared-test-example.png alt="Chi-Squared test example" loading=lazy></a></figure><p>Unfortunately, both Bayesian calculators that I&rsquo;m aware of have some limitations. Plugging the same numbers into the calculators by <a href=https://www.peakconversion.com/2012/02/ab-split-test-graphical-calculator/ target=_blank rel="nofollow noopener">PeakConversion</a> and <a href=http://developers.lyst.com/bayesian-calculator/ target=_blank rel="nofollow noopener">Lyst</a> would inform you that the probability of A being best is approximately 0.98, but it won&rsquo;t tell you what&rsquo;s the best way forward given this information. PeakConversion also outputs the 95% success rate intervals for A (between 53.1% and 66.7%) and B (between 43.1% and 56.9%), but it doesn&rsquo;t let users set the prior values α<sub></sub> and β<sub></sub> (it uses α<sub></sub> = β<sub></sub> = 0.5). The ability to set priors based on what we know about our experimental setting is an important feature of Bayesian statistics <a href=http://developers.lyst.com/2014/05/10/bayesian-ab-testing/ target=_blank rel=noopener>that can help reduce the number of false positives</a>. Hiding the priors in PeakConversion&rsquo;s calculator makes it easier to use but less powerful than Lyst&rsquo;s tool. In addition, Lyst&rsquo;s calculator presents the distribution of differences between the success rates of A and B, i.e., the <em>effect size</em>. This is important because we may not bother implementing certain changes if the effect is negligible, even if the probability of one variant being better than the other is very close to 1.</p><p>Despite being more powerful, I find Lyst&rsquo;s calculator just a bit too technical. Specifically, setting the α<sub></sub> and β<sub></sub> priors requires some familiarity with the beta distribution, which many people don&rsquo;t have. Also, the effect size distribution is important, but can be hard to get one&rsquo;s head around. Therefore, I decided to extend Lyst&rsquo;s calculator, aiming to release a new tool that is both powerful and easy to use.</p><h2 id=building-the-new-calculator>Building the new calculator<a hidden class=anchor aria-hidden=true href=#building-the-new-calculator>#</a></h2><p>The source code for Lyst&rsquo;s calculator is <a href=https://github.com/ssaw/BayesianTestJS target=_blank rel=noopener>available on GitHub</a>, so I decided to use that as the foundation of the new calculator. The first step was to convert the code from HTML, CSS, and JavaScript to Jade, Sass, and CoffeeScript, and clean up some code duplication. As the calculator is served from my <a href=https://yanirs.github.io/tools/split-test-calculator/ target=_blank rel=noopener>GitHub Pages domain</a>, it was easiest to put <a href=https://github.com/yanirs/yanirs.github.io/tree/master/tools/split-test-calculator/src target=_blank rel=noopener>all the code</a> in that repository. Once I had an environment and codebase that I was happy with, it was time to make functional changes:</p><ul><li>Change the layout to be responsive, so it&rsquo;d work well on mobile devices.</li><li>Enable sharing of results by changing the URL when the input changes.</li><li>Provide clear instructions, so that the calculator can be used by people who don&rsquo;t necessarily have a strong background in statistics.</li><li>Allow users to set priors based on more familiar figures than the beta distribution&rsquo;s α<sub></sub> and β<sub></sub> priors.</li><li>Make a clear and well-justified recommendation on how to proceed.</li></ul><p>While the first two changes were straightforward to implement, the other points were somewhat more challenging. Specifically, providing clear explanations that assume little background knowledge isn&rsquo;t simple, and I still feel that the current version of the new calculator is a bit too wordy (this may be improved in the future based on user feedback – suggestions welcome). Life would be easier if everyone thought of observed values as being drawn from distributions, but in my experience this is not always the case. However, I believe it is important to communicate the reality of uncertainty, so I don&rsquo;t want to hide it from users of the calculator, even at the price of more elaborate explanations.</p><p>Making the priors more intuitive was a bit tricky. At first, I thought I&rsquo;d let users state their prior knowledge in terms of the mean and variance of past performance, relying on the fact that for Beta(α, β) the mean μ is α / (α + β), and the variance σ<sup>2</sup> is αβ / (α + β)<sup>2</sup>(α + β + 1). The problem is that while the mean is simple to set, as it is always in the (0, 1) range, the upper bound for the variance depends on the mean. Specifically, <a href=http://stats.stackexchange.com/a/12239 target=_blank rel=noopener>it can be shown</a> that the variance is in the range (0, μ(1 – μ)). Therefore, I decided to let users quantify their uncertainty about the mean as a number <em>u</em> in the range (0, 1), where σ<sup>2</sup> = <em>u</em>μ(1 – μ). Having played with the calculator a bit, I think this makes it easier to set good informative priors. It is also worth noting that I considered allowing users to set different priors for the control and test group, but decided against it to reduce complexity. In addition, it makes sense to have the same prior for both groups – if you have a strong belief or knowledge on which one is going to perform better, you probably don&rsquo;t need to run an experiment.</p><p>One of the main reasons I decided to build the calculator was because I wanted a tool that outputs a clear recommendation. This proved to be the most challenging (and interesting) part of this project, as there are quite a few options for Bayesian stopping rules. After reading <a href=http://varianceexplained.org/r/bayesian-ab-testing/ target=_blank rel=noopener>David Robinson&rsquo;s review of the limitations of a stopping rule based on the expected loss</a>, and a few of the other resources mentioned in his post, I decided to go with a combination of <a href=http://doingbayesiandataanalysis.blogspot.com.au/2013/11/optional-stopping-in-data-collection-p.html target=_blank rel=noopener>the third and fourth rules tested by John Kruschke</a>. These rules rely on a threshold of caring, which is the minimum effect size that is seen as significant by the user. For example, if we&rsquo;re running experiments on the conversion rate of a landing page, we may decide that we don&rsquo;t care if the absolute change in conversion rate is less than 0.1%. Given this threshold and data from the experiment, the following recommendations are possible:</p><ol><li><em>Stop the experiment and implement either variant</em>, because the difference between the variants is smaller than the threshold.</li><li><em>Stop the experiment and implement the winning variant</em>, because the difference between the variants is greater than the threshold.</li><li><em>Keep running the experiment</em>, because there isn&rsquo;t enough data to make a decision.</li></ol><p>Formally, Kruschke&rsquo;s rules work as follows. Given the minimum effect threshold <em>t</em>, we define a region of practical equivalence (ROPE) to zero difference as the interval [-<em>t</em>, <em>t</em>]. Then, we compare the ROPE to the 95% high density interval (HDI) of the distribution of differences between A and B. When comparing the ROPE and HDI, there are three options that correspond to the recommendations above:</p><ol><li>The ROPE is completely contained in the HDI (stop the experiment and implement either variant).</li><li>The intersection between the ROPE and HDI is empty (stop the experiment and implement the winning variant).</li><li>The ROPE and HDI only partly overlap (keep running the experiment).</li></ol><p>Kruschke&rsquo;s post shows that making the rule more restrictive by adding a notion of user-settable <em>precision</em> can reduce the rate of false positives. The idea is to stop only if the HDI is narrower than precision multiplied by the width of the ROPE. Intuitively, this forces the experimenter to collect more data because it makes the posterior distributions narrower (as shown by the charts above). I found it hard to explain the idea of precision, and didn&rsquo;t want to confuse users by adding another parameter, so I decided to use a constant precision value of 0.8. If the ROPE and HDI don&rsquo;t overlap, the tool makes a recommendation to stop, accompanied by a binary level of confidence: <em>high</em> if the precision condition is met, and <em>low</em> otherwise.</p><p>Putting in the numbers from the running example (T<sub>A</sub> = 200, S<sub>A</sub> = 120, T<sub>B</sub> = 200, and S<sub>B</sub> = 100) together with a minimum effect of 1%, prior success rate of 50%, and 57.74% uncertainty (equivalent to α<sub></sub> = β<sub></sub> = 1), we get the following output:</p><figure><a href="https://yanirs.github.io/tools/split-test-calculator/#prior-mean=50,prior-uncertainty=57.74,minimum-effect=1,control-trials=200,control-successes=120,test-trials=200,test-successes=100" target=_blank rel=noopener><img sizes="
          (min-width: 768px) 464px,
          100vw
        " srcset="https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/calcualtor-recommendation-example_hud06c7fb294d1b83870b027778dfd8316_38185_360x0_resize_box_3.png 360w,
https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/calcualtor-recommendation-example.png 464w," src=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/calcualtor-recommendation-example.png alt="Calculator recommendation example" loading=lazy></a></figure><p><a href="https://yanirs.github.io/tools/split-test-calculator/#prior-mean=50,prior-uncertainty=57.74,minimum-effect=1,control-trials=200,control-successes=120,test-trials=200,test-successes=100" target=_blank rel=noopener>The full results</a> also include plots of the distributions and their high density intervals. I&rsquo;m pretty happy with the richer information provided by the calculator, though it still has some limitations and areas that can be improved.</p><h2 id=limitations-and-potential-improvements>Limitations and potential improvements<a hidden class=anchor aria-hidden=true href=#limitations-and-potential-improvements>#</a></h2><p>As mentioned above, I&rsquo;d love to reduce the wordiness of the calculator while keeping it self-contained, but I need some feedback to understand if any explanations are redundant. It&rsquo;d also be great to reduce the reliance on magic numbers, such as the 95% HDI and 0.8 precision used for generating a recommendation. However, making these settable by users would increase the complexity of using the calculator, which is already harder to use than the frequentist alternative. Nonetheless, it&rsquo;s important to remember that oversimplification is the reason why it&rsquo;s easier to make the wrong decision when following the classical approach.</p><p>Other potential changes include <a href=http://varianceexplained.org/r/bayesian_ab_baseball/ target=_blank rel=noopener>switching to a closed-form formula rather than draws from a distribution</a>, comparing more than two variants, and improving Kruschke&rsquo;s stopping rules by simulating more scenarios than those considered in his post. In addition, I&rsquo;d like to go beyond binary responses (success/failure) to support continuous rewards (e.g., revenue), and allow users to specify different costs for the variants (e.g., implementing B may cost more than sticking with A).</p><p>Finally, it is important to keep in mind that significance testing can&rsquo;t tell you whether your sample is representative of the population. For example, if you run an experiment on a very popular website, you can get a sample of thousands of people within a few minutes. Concluding an experiment based on such a sample is probably a bad idea, as it is plausible that you would reach different conclusions if you kept running the experiment for a few days, to reduce the effect that the time of day has on the results. Similarly, a few days may not be enough if your user population behaves differently on weekends – you would need to run the experiment over a few weeks. This can be extended to months and years to rule out seasonal effects, but it is up to the experimenter to weigh the practicality of considering such factors versus the need to make decisions (see articles by <a href=http://conversionxl.com/statistical-significance-does-not-equal-validity/ target=_blank rel=noopener>Peep Laja</a>, <a href=http://www.qubit.com/sites/default/files/pdf/mostwinningabtestresultsareillusory_0.pdf target=_blank rel=noopener>Martin Goodson</a>, <a href=https://blog.crazyegg.com/2016/03/22/anti-cookbook-ab-testing/ target=_blank rel=noopener>Sam Ju</a>, and <a href=http://www.exp-platform.com/Documents/2014%20experimentersRulesOfThumb.pdf target=_blank rel=noopener>Kohavi et al.</a> for more details). The main thing to remember is that <strong>you just cannot completely eliminate uncertainty and the need to consider background knowledge</strong>, which is why I believe that helping more people follow the Bayesian approach is a step in the right direction.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yanirseroussi.com/tags/a/b-testing/>a/b testing</a></li><li><a href=https://yanirseroussi.com/tags/analytics/>analytics</a></li><li><a href=https://yanirseroussi.com/tags/causal-inference/>causal inference</a></li><li><a href=https://yanirseroussi.com/tags/data-science/>data science</a></li><li><a href=https://yanirseroussi.com/tags/statistics/>statistics</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Making Bayesian A/B testing more accessible on twitter" href="https://twitter.com/intent/tweet/?text=Making%20Bayesian%20A%2fB%20testing%20more%20accessible&url=https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f&hashtags=a%2fbtesting%2canalytics%2ccausalinference%2cdatascience%2cstatistics"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Making Bayesian A/B testing more accessible on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f&title=Making%20Bayesian%20A%2fB%20testing%20more%20accessible&summary=Making%20Bayesian%20A%2fB%20testing%20more%20accessible&source=https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Making Bayesian A/B testing more accessible on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f&title=Making%20Bayesian%20A%2fB%20testing%20more%20accessible"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Making Bayesian A/B testing more accessible on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Making Bayesian A/B testing more accessible on whatsapp" href="https://api.whatsapp.com/send?text=Making%20Bayesian%20A%2fB%20testing%20more%20accessible%20-%20https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Making Bayesian A/B testing more accessible on telegram" href="https://telegram.me/share/url?text=Making%20Bayesian%20A%2fB%20testing%20more%20accessible&url=https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><section class=comment-section><strong>17 comments</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Comment via GitHub issue</a><div class=comment-level-0 id=comment-1236><div class=comment-header><a href=#comment-1236><img class=comment-avatar src="https://www.gravatar.com/avatar/56f2e2c3d5f195f0d70e78194731f571?s=50"><p class=comment-info><strong>John Chew</strong><br><small>2016-06-20 00:23:50</small></p></a></div><div class="comment-body post-content">My hunch is that the % of HDI chosen is of less interest to a user than seeing how each test iteration alters the HDI and shifts the level of overlap between HDI and ROPE toward one of either outcome. In the example given above, would a fair interpretation be that the differences appear weighted more toward the negative than the positive? With precision, shouldn&rsquo;t it be made a function of the minimum effect requested? Larger ROPE require less precision and vice versa?</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1236&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-1 id=comment-1238><div class=comment-header><a href=#comment-1238><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-06-20 12:37:25</small></p></a></div><div class="comment-body post-content">Thanks for your comment, John! I think that it appears weighted more towards the negative because the beta distribution is symmetric when the mean is 0.5 (alpha = beta), and asymmetric in other cases, making it less pointy. According to Kruschke&rsquo;s simulations, using the precision stopping rule makes the success rate estimate closer to the true mean of the underlying distribution than with other stopping rules, which tend to overestimate the success rate. I&rsquo;m not sure we&rsquo;d get the same results if precision were a function of the minimum effect, but I&rsquo;d like to run more simulations to get a better feeling for how it works.</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1238&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-0 id=comment-1481><div class=comment-header><a href=#comment-1481><img class=comment-avatar src="https://www.gravatar.com/avatar/7618c8563a001f0ecc8c661d81d72532?s=50"><p class=comment-info><strong>Сергей Филиппов</strong><br><small>2017-03-23 18:11:43</small></p></a></div><div class="comment-body post-content">Could you tell me please how do you calculate HDI and ROPE?
I am trying to replicate this calculator in R.
Thanks!</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1481&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-1 id=comment-1491><div class=comment-header><a href=#comment-1491><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2017-03-31 08:00:23</small></p></a></div><div class="comment-body post-content">The source code for the calculation is here: <a href=https://github.com/yanirs/yanirs.github.io/blob/master/tools/split-test-calculator/src/bayes.coffee#L139 target=_blank rel=noopener>https://github.com/yanirs/yanirs.github.io/blob/master/tools/split-test-calculator/src/bayes.coffee#L139</a> &ndash; it shouldn&rsquo;t be too hard to translate to R.</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1491&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-0 id=comment-1483><div class=comment-header><a href=#comment-1483><img class=comment-avatar src="https://www.gravatar.com/avatar/a7eaeb8a80e037342bf6d5e2e0e18f82?s=50"><p class=comment-info><strong>Sam Gil</strong><br><small>2017-03-24 17:23:47</small></p></a></div><div class="comment-body post-content"><p>Thanks for the post!</p><p>I&rsquo;m more of a business stakeholder simply trying to improve our testing practices, rather than a data scientist who understands the theories at a detailed level.</p><p>I&rsquo;m a bit confused why, if I enter the default example in your calculator (5000 trials each, 100 successes vs 130), the recommendation is to implement EITHER variant.</p><p>Whereas, using a tool such as the following suggests a 97.8% chance the variant with 130 successes will outperform the control: <a href=https://abtestguide.com/bayesian/ target=_blank rel=noopener>https://abtestguide.com/bayesian/</a></p><p>This calculator also seems to suggest the 130 successes variant should be chosen, not EITHER, as there is 95% confidence the result is not due to chance : <a href=https://abtestguide.com/calc/ target=_blank rel=noopener>https://abtestguide.com/calc/</a></p><p>A secondary question is, if there is no predetermined sample size with the Bayesian approach, how do you plan how long to run the test for? Mainly to deal with stakeholder communication & project planning, but also to avoid peaking.</p><p>Many thanks,
Sam</p></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1483&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-1 id=comment-1496><div class=comment-header><a href=#comment-1496><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2017-04-01 10:57:35</small></p></a></div><div class="comment-body post-content"><p>Thanks for your comment, Sam!</p><p>There are many approaches to Bayesian A/B testing. It looks like the other one you linked to doesn&rsquo;t allow you to specify your prior knowledge about the conversion rate and the decision criterion. Which values did you use for those fields in my calculator?</p><p>As mentioned in the last paragraph of the post, you should really aim for a representative sample of users &ndash; size is only one factor. That being said, you can play with the number of trials in the calculator to get an idea of the required sample size based on the minimum effect criterion.</p></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1496&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-0 id=comment-1527><div class=comment-header><a href=#comment-1527><img class=comment-avatar src="https://www.gravatar.com/avatar/f71979d1ffe00bec8f8b28be74d06bc8?s=50"><p class=comment-info><strong>David S</strong><br><small>2017-05-09 15:56:05</small></p></a></div><div class="comment-body post-content"><p>Great stuff!</p><p>I agree that Peak Conversion makes it easy, but not being able to change the priors makes it limited. Lyst is a very good tool but your use of &ldquo;success rate&rdquo; rather than specifying an alpha and beta makes it much more intuitive.</p><p>I just wanted to confirm my assumption on the &ldquo;Minimum effect&rdquo; field - you said it&rsquo;s absolute, so if I wanted to detect a 10% difference in a 5% success rate, that means I would have to input the value as 0.5?</p></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1527&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-1 id=comment-1530><div class=comment-header><a href=#comment-1530><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2017-05-09 23:54:45</small></p></a></div><div class="comment-body post-content"><p>Thanks David!</p><p>That&rsquo;s correct. If you set the minimum effect field to 0.5, it means that changes in the success rate that are lower than 0.5% are considered equivalent to zero change. In your example, if the success rate changes from 5% to anything between 4.5% and 5.5%, the change is considered to be insubstantial or unimportant. Note that this is different from statistical significance: A change is significant in the statistical sense if it is unlikely to be due to random variation. The decision whether a change is substantial or important depends on your application, which is where the minimum effect threshold comes in.</p></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1530&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-0 id=comment-1817><div class=comment-header><a href=#comment-1817><img class=comment-avatar src="https://www.gravatar.com/avatar/38e2950880f82e1fe5fe4cf876428407?s=50"><p class=comment-info><strong>Georgi Georgiev</strong><br><small>2017-09-20 10:53:39</small></p></a></div><div class="comment-body post-content"><p>Hi Yanir,</p><p>The calculator is simple enough, and easy to use, though it lacks some basic things such as testing more than one variant against a control, which is very often done in practice.</p><p>I, and hopefully others, am interested to learn how information about the stopping rule used enters into the calculations, if at all? After all, ignoring the stopping rule would be like discarding data that can have a critical impact on the decision made. I make an argument about it here: <a href=http://blog.analytics-toolkit.com/2017/bayesian-ab-testing-not-immune-to-optional-stopping-issues/ target=_blank rel=noopener>http://blog.analytics-toolkit.com/2017/bayesian-ab-testing-not-immune-to-optional-stopping-issues/</a> , with lots of references. Another take on the issue I&rsquo;ve recently found is here <a href=http://srmart.in/bayes-optional-stopping/ target=_blank rel=noopener>http://srmart.in/bayes-optional-stopping/</a> .</p><p>Best,
Georgi</p></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1817&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-1 id=comment-1821><div class=comment-header><a href=#comment-1821><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2017-09-21 12:11:43</small></p></a></div><div class="comment-body post-content">Thanks Georgi. I&rsquo;m not sure I understand your question. The stopping rule is explained under &ldquo;Building the new calculator&rdquo;. It&rsquo;s worth reading Kruschke&rsquo;s articles for further explanations. Also, as noted in the final paragraph of the post, no stopping rule can tell you whether your sample is representative of the general population, so results on synthetic data are of limited use.</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1821&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-2 id=comment-1822><div class=comment-header><a href=#comment-1822><img class=comment-avatar src="https://www.gravatar.com/avatar/38e2950880f82e1fe5fe4cf876428407?s=50"><p class=comment-info><strong>Georgi Georgiev</strong><br><small>2017-09-21 13:09:23</small></p></a></div><div class="comment-body post-content"><p>Hi Yanir,</p><p>I&rsquo;ve read the explanation provided, and it left me feeling like the calculator ignores the stopping rule used up to the point when data enters your calculator. The same is confirmed by the calculator interface, where I see no option to specify a stopping rule or number and points of prior observations I&rsquo;ve done on the data I enter. While for the first part one can assume that the user would use your calculator for each look and the stopping rule is then given and the same, the second part clearly means it will have no input on the number and timing of peeks. Given that the stopping rule is data-based, it is certain to introduce heavy bias on the resulting statistics, if my impression that you&rsquo;re not adjusting the calculations to account for peeking/optional stopping in any way are correct. Am I wrong in my assertion?</p><p>Thanks,
Georgi</p></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1822&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-3 id=comment-1824><div class=comment-header><a href=#comment-1824><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2017-09-21 22:29:08</small></p></a></div><div class="comment-body post-content">Thanks for the clarification, Georgi. You&rsquo;re right, the calculator doesn&rsquo;t take the number of peeks into account. As noted in the final paragraph of the post, I don&rsquo;t think it matters as much as other factors, so I&rsquo;m not planning on changing this. As discussed in <a href=https://yanirseroussi.com/2016/05/15/diving-deeper-into-causality-pearl-kleinberg-hill-and-untested-assumptions/>a separate post</a>, statistical significance doesn&rsquo;t imply causality, which is what we really care about. It is just one factor that should be taken into account when deciding to implement changes.</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1824&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-2 id=comment-1829><div class=comment-header><a href=#comment-1829><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2017-09-30 02:34:13</small></p></a></div><div class="comment-body post-content">This recent paper on the limited use of statistical significance testing is worth reading: <a href=http://www.stat.columbia.edu/~gelman/research/unpublished/abandon.pdf target=_blank rel=noopener>http://www.stat.columbia.edu/~gelman/research/unpublished/abandon.pdf</a></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-1829&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-0 id=comment-19613><div class=comment-header><a href=#comment-19613><img class=comment-avatar src="https://www.gravatar.com/avatar/705dd93e666bf6d8a78517544974d254?s=50"><p class=comment-info><strong>LazioB</strong><br><small>2020-07-15 19:32:24</small></p></a></div><div class="comment-body post-content">Yanir hi! Thanks for sharing this content. I got a question regarding something said on the Lyst blog post that I havent seen mentioned here: &ldquo;Both of these problems (repeated testing and low base rate) are much less pronounced when we use an informative prior.&rdquo; Could you please explain how and why this occurs? Or share some paper about it? Because otherwise I cant see in which ways prior selection affects type 1 errors (since you are comparing it to the problems of repeated testing). And also, as my understanding goes, bayesian inference does not control for type 1 errors, its not in its interest to do so, so why would someone choose a prior conditioned on its possible effects on type 1 errors?</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-19613&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-1 id=comment-19833><div class=comment-header><a href=#comment-19833><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2020-08-17 03:23:18</small></p></a></div><div class="comment-body post-content"><p>I&rsquo;m not entirely sure about statements by the Lyst post authors, but I think that it might be because the prior can make it less likely that one would make a call before there is enough data, which is when there is a higher risk of false positives. However, this is also addressed by using a stopping rule like Krushcke&rsquo;s.</p><p>By the way, Kruschke&rsquo;s 2018 paper <a href=https://osf.io/s5vdy/ rel="nofollow ugc">Rejecting or accepting parameter values in Bayesian estimation</a> is a good recent read on the topic.</p></div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-19833&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-0 id=comment-19632><div class=comment-header><a href=#comment-19632><img class=comment-avatar src="https://www.gravatar.com/avatar/d016712f917c1fd5799faf878077c4b5?s=50"><p class=comment-info><strong>mrpickleau</strong><br><small>2020-07-23 10:38:40</small></p></a></div><div class="comment-body post-content">Would you happen to have a bayesian test duration calculator or know of one you can point me to?</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-19632&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div><div class=comment-level-1 id=comment-19832><div class=comment-header><a href=#comment-19832><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2020-08-17 02:56:19</small></p></a></div><div class="comment-body post-content">A quick search yields some results, but I&rsquo;m not sure about the reliability of the estimates. Maybe I&rsquo;ll add it as a feature one day&mldr; :)</div><a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirseroussi.com%2f2016%2f06%2f19%2fmaking-bayesian-ab-testing-more-accessible%2f%23comment-19832&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Reply via GitHub issue</a></div></section></article></main><footer class=footer><span>Text and figures licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank rel=noopener>CC BY-NC-ND 4.0</a> by <a href=https://yanirseroussi.com/about/>Yanir Seroussi</a>, except where noted otherwise  |</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><div class=mailing-list-container><form class=mailing-list action="https://yanirseroussi.us17.list-manage.com/subscribe/post?u=3c08aa3ff27dd92978019febd&id=bc3ab705af" method=post target=_blank novalidate><label for=mailing-list-email>Get new post notifications</label>
<input type=text name=EMAIL id=mailing-list-email placeholder="Email address"><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_3c08aa3ff27dd92978019febd_bc3ab705af tabindex=-1></div><input type=submit value=Subscribe></form><div class=footer>Alternatively, <a href=https://github.com/yanirs/yanirseroussi.com rel=noopener target=_blank>watch on GitHub</a>
or <a href=https://yanirseroussi.com/index.xml>subscribe to RSS feed</a>.</div></div><script>let menu=document.getElementById("menu");menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>