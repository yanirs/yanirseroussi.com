<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Why you should stop worrying about deep learning and deepen your understanding of causality instead | Yanir Seroussi | Data & AI for Nature</title>
<meta name=keywords content="analytics,causal inference,data science,deep learning,insights,machine learning,predictive modelling"><meta name=description content="Causality is often overlooked but is of much higher relevance to most data scientists than deep learning."><meta name=author content="Yanir Seroussi"><link rel=canonical href=https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/><meta name=google-site-verification content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU"><link crossorigin=anonymous href=/assets/css/stylesheet.4b8cc203f6a37bd20ba1ef634068a73cdc702722ce99fa2fde7f35869dbb5563.css integrity="sha256-S4zCA/aje9ILoe9jQGinPNxwJyLOmfov3n81hp27VWM=" rel="preload stylesheet" as=style><link rel=icon href=https://yanirseroussi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yanirseroussi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yanirseroussi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yanirseroussi.com/apple-touch-icon.png><link rel=mask-icon href=https://yanirseroussi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Why you should stop worrying about deep learning and deepen your understanding of causality instead"><meta property="og:description" content="Causality is often overlooked but is of much higher relevance to most data scientists than deep learning."><meta property="og:type" content="article"><meta property="og:url" content="https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/"><meta property="og:image" content="https://yanirseroussi.com/correlation-xkcd.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-02-14T11:04:11+00:00"><meta property="article:modified_time" content="2023-07-06T09:28:02+10:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yanirseroussi.com/correlation-xkcd.png"><meta name=twitter:title content="Why you should stop worrying about deep learning and deepen your understanding of causality instead"><meta name=twitter:description content="Causality is often overlooked but is of much higher relevance to most data scientists than deep learning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Why you should stop worrying about deep learning and deepen your understanding of causality instead","item":"https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Why you should stop worrying about deep learning and deepen your understanding of causality instead","name":"Why you should stop worrying about deep learning and deepen your understanding of causality instead","description":"Causality is often overlooked but is of much higher relevance to most data scientists than deep learning.","keywords":["analytics","causal inference","data science","deep learning","insights","machine learning","predictive modelling"],"articleBody":"Everywhere you go these days, you hear about deep learning’s impressive advancements. New deep learning libraries, tools, and products get announced on a regular basis, making the average data scientist feel like they’re missing out if they don’t hop on the deep learning bandwagon. However, as Kamil Bartocha put it in his post The Inconvenient Truth About Data Science, 95% of tasks do not require deep learning. This is obviously a made up number, but it’s probably an accurate representation of the everyday reality of many data scientists. This post discusses an often-overlooked area of study that is of much higher relevance to most data scientists than deep learning: causality.\nCausality is everywhere An understanding of cause and effect is something that is not unique to humans. For example, the many videos of cats knocking things off tables appear to exemplify experimentation by animals. If you are not familiar with such videos, it can easily be fixed. The thing to notice is that cats appear genuinely curious about what happens when they push an object. And they tend to repeat the experiment to verify that if you push something off, it falls to the ground.\nHumans rely on much more complex causal analysis than that done by cats – an understanding of the long-term effects of one’s actions is crucial to survival. Science, as defined by Wikipedia, is a systematic enterprise that creates, builds and organizes knowledge in the form of testable explanations and predictions about the universe. Causal analysis is key to producing explanations and predictions that are valid and sound, which is why understanding causality is so important to data scientists, traditional scientists, and all humans.\nWhat is causality? It is surprisingly hard to define causality. Just like cats, we all have an intuitive sense of what causality is, but things get complicated on deeper inspection. For example, few people would disagree with the statement that smoking causes cancer. But does it cause cancer immediately? Would smoking a few cigarettes today and never again cause cancer? Do all smokers develop cancer eventually? What about light smokers who live in areas with heavy air pollution?\nSamantha Kleinberg summarises it very well in her book, Why: A Guide to Finding and Using Causes:\nWhile most definitions of causality are based on Hume’s work, none of the ones we can come up with cover all possible cases and each one has counterexamples another does not. For instance, a medication may lead to side effects in only a small fraction of users (so we can’t assume that a cause will always produce an effect), and seat belts normally prevent death but can cause it in some car accidents (so we need to allow for factors that can have mixed producer/preventer roles depending on context).\nThe question often boils down to whether we should see causes as a fundamental building block or force of the world (that can’t be further reduced to any other laws), or if this structure is something we impose. As with nearly every facet of causality, there is disagreement on this point (and even disagreement about whether particular theories are compatible with this notion, which is called causal realism). Some have felt that causes are so hard to find as for the search to be hopeless and, further, that once we have some physical laws, those are more useful than causes anyway. That is, “causes” may be a mere shorthand for things like triggers, pushes, repels, prevents, and so on, rather than a fundamental notion.\nIt is somewhat surprising, given how central the idea of causality is to our daily lives, but there is simply no unified philosophical theory of what causes are, and no single foolproof computational method for finding them with absolute certainty. What makes this even more challenging is that, depending on one’s definition of causality, different factors may be identified as causes in the same situation, and it may not be clear what the ground truth is.\nWhy study causality now? While it’s hard to conclusively prove, it seems to me like interest in formal causal analysis has increased in recent years. My hypothesis is that it’s just a natural progression along the levels of data’s hierarchy of needs. At the start of the big data boom, people were mostly concerned with storing and processing large amounts of data (e.g., using Hadoop, Elasticsearch, or your favourite NoSQL database). Just having your data flowing through pipelines is nice, but not very useful, so the focus switched to reporting and visualisation to extract insights about what happened (commonly known as business intelligence). While having a good picture of what happened is great, it isn’t enough – you can make better decisions if you can predict what’s going to happen, so the focus switched again to predictive analytics. Those who are familiar with predictive analytics know that models often end up relying on correlations between the features and the predicted labels. Using such models without considering the meaning of the variables can lead us to erroneous conclusions, and potentially harmful interventions. For example, based on the following graph we may make a recommendation that the US government decrease its spending on science to reduce the number of suicides by hanging.\nSource: Spurious Correlations by Tyler Vigen Causal analysis aims to identify factors that are independent of spurious correlations, allowing stakeholders to make well-informed decisions. It is all about getting to the top of the DIKW (data-information-knowledge-wisdom) pyramid by understanding why things happen and what we can do to change the world. However, finding true causes can be very hard, especially in cases where you can’t perform experiments. Judea Pearl explains it well:\nWe know, from first principles, that any causal conclusion drawn from observational studies must rest on untested causal assumptions. Cartwright (1989) named this principle ‘no causes in, no causes out,’ which follows formally from the theory of equivalent models (Verma and Pearl, 1990); for any model yielding a conclusion C, one can construct a statistically equivalent model that refutes C and fits the data equally well.\nWhat this means in practice is that you can’t, for example, conclusively prove that smoking causes cancer without making some reasonable assumptions about the mechanisms at play. For ethical reasons, we can’t perform a randomly controlled trial where a test group is forced to smoke for years while a control group is forced not to smoke. Therefore, our conclusions about the causal link between smoking and cancer are drawn from observational studies and an understanding of the mechanisms by which various cancers develop (e.g., the effect of cigarette smoke on individual cells can be studied without forcing people to smoke). Cancer Tobacco companies have exploited this fact for years, making the claim that the probability of both cancer and smoking is raised by some mysterious genetic factors. Fossil fuel and food companies use similar arguments to sell their products and block attempts to regulate their industries (as discussed in previous posts on the hardest parts of data science and nutritionism). Fighting against such arguments is an uphill battle, as it is easy to sow doubt with a few simplistic catchphrases, while proving and communicating causality to laypeople is much harder (or impossible when it comes to deeply-held irrational beliefs).\nMy causality journey is just beginning My interest in formal causal analysis was seeded a couple of years ago, with a reading group that was dedicated to Judea Pearl’s work. We didn’t get very far, as I was a bit disappointed with what causal calculus can and cannot do. This may have been because I didn’t come in with the right expectations – I expected a black box that automatically finds causes. Recently reading Samantha Kleinberg’s excellent book Why: A Guide to Finding and Using Causes has made my expectations somewhat more realistic:\nThousands of years after Aristotle’s seminal work on causality, hundreds of years after Hume gave us two definitions of it, and decades after automated inference became a possibility through powerful new computers, causality is still an unsolved problem. Humans are prone to seeing causality where it does not exist and our algorithms aren’t foolproof. Even worse, once we find a cause it’s still hard to use this information to prevent or produce an outcome because of limits on what information we can collect and how we can understand it. After looking at all the cases where methods haven’t worked and researchers and policy makers have gotten causality really wrong, you might wonder why you should bother.\n[…]\nRather than giving up on causality, what we need to give up on is the idea of having a black box that takes some data straight from its source and emits a stream of causes with no need for interpretation or human intervention. Causal inference is necessary and possible, but it is not perfect and, most importantly, it requires domain knowledge.\nKleinberg’s book is a great general intro to causality, but it intentionally omits the mathematical details behind the various methods. I am now ready to once again go deeper into causality, perhaps starting with Kleinberg’s more technical book, Causality, Probability, and Time. Other recommendations are very welcome!\nCover image source: xkcd: Correlation ","wordCount":"1532","inLanguage":"en","image":"https://yanirseroussi.com/correlation-xkcd.png","datePublished":"2016-02-14T11:04:11Z","dateModified":"2023-07-06T09:28:02+10:00","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data \u0026 AI for Nature","logo":{"@type":"ImageObject","url":"https://yanirseroussi.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data & AI for Nature (Alt + H)">Yanir Seroussi | Data & AI for Nature</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yanirseroussi.com/about/ title=About><span>About</span></a></li><li><a href=https://yanirseroussi.com/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://yanirseroussi.com/consult/ title=Consult><span>Consult</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Why you should stop worrying about deep learning and deepen your understanding of causality instead</h1><div class=post-meta><span title='2016-02-14 11:04:11 +0000 UTC'>February 14, 2016</span>&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2016-02-14-why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a></div></header><figure class=entry-cover><img loading=eager src=https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/correlation-xkcd.png alt></figure><div class=post-content><p>Everywhere you go these days, you hear about deep learning&rsquo;s impressive advancements. New deep learning libraries, tools, and products get announced on a regular basis, making the average data scientist feel like they&rsquo;re missing out if they don&rsquo;t <a href=https://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/ target=_blank rel=noopener>hop on the deep learning bandwagon</a>. However, as Kamil Bartocha put it in his post <a href=https://www.linkedin.com/pulse/inconvenient-truth-data-science-kamil-bartocha target=_blank rel=noopener>The Inconvenient Truth About Data Science</a>, <em>95% of tasks do not require deep learning</em>. This is obviously <a href=http://dilbert.com/strip/2008-05-08 target=_blank rel=noopener>a made up number</a>, but it&rsquo;s probably an accurate representation of the everyday reality of many data scientists. This post discusses an often-overlooked area of study that is of much higher relevance to most data scientists than deep learning: <strong>causality</strong>.</p><h2 id=causality-is-everywhere>Causality is everywhere<a hidden class=anchor aria-hidden=true href=#causality-is-everywhere>#</a></h2><p>An understanding of cause and effect is something that is not unique to humans. For example, the many videos of cats knocking things off tables appear to exemplify experimentation by animals. If you are not familiar with such videos, <a href="https://www.youtube.com/results?search_query=cat+knocking+stuff+off" target=_blank rel=noopener>it can easily be fixed</a>. The thing to notice is that cats appear genuinely curious about what happens when they push an object. And they tend to repeat the experiment to verify that if you push something off, it falls to the ground.</p><p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/UoUEQYjYgf4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p>Humans rely on much more complex causal analysis than that done by cats – an understanding of the long-term effects of one&rsquo;s actions is crucial to survival. <a href=https://en.wikipedia.org/wiki/Science target=_blank rel=noopener>Science, as defined by Wikipedia</a>, <em>is a systematic enterprise that creates, builds and organizes knowledge in the form of testable explanations and predictions about the universe</em>. Causal analysis is key to producing explanations and predictions that are valid and sound, which is why understanding causality is so important to data scientists, traditional scientists, and all humans.</p><h2 id=what-is-causality>What is causality?<a hidden class=anchor aria-hidden=true href=#what-is-causality>#</a></h2><p>It is surprisingly hard to define causality. Just like cats, we all have an intuitive sense of what causality is, but things get complicated on deeper inspection. For example, few people would disagree with the statement that <em>smoking causes cancer</em>. But does it cause cancer immediately? Would smoking a few cigarettes today and never again cause cancer? Do all smokers develop cancer eventually? What about light smokers who live in areas with heavy air pollution?</p><p>Samantha Kleinberg summarises it very well in her book, <a href=http://www.skleinberg.org/why/ target=_blank rel=noopener>Why: A Guide to Finding and Using Causes</a>:</p><blockquote><p>While most definitions of causality are based on <a href=https://en.wikipedia.org/wiki/David_Hume target=_blank rel=noopener>Hume&rsquo;s work</a>, none of the ones we can come up with cover all possible cases and each one has counterexamples another does not. For instance, a medication may lead to side effects in only a small fraction of users (so we can&rsquo;t assume that a cause will always produce an effect), and seat belts normally prevent death but can cause it in some car accidents (so we need to allow for factors that can have mixed producer/preventer roles depending on context).</p><p>The question often boils down to whether we should see causes as a fundamental building block or force of the world (that can&rsquo;t be further reduced to any other laws), or if this structure is something we impose. As with nearly every facet of causality, there is disagreement on this point (and even disagreement about whether particular theories are compatible with this notion, which is called causal realism). Some have felt that causes are so hard to find as for the search to be hopeless and, further, that once we have some physical laws, those are more useful than causes anyway. That is, &ldquo;causes&rdquo; may be a mere shorthand for things like triggers, pushes, repels, prevents, and so on, rather than a fundamental notion.</p><p>It is somewhat surprising, given how central the idea of causality is to our daily lives, but there is simply no unified philosophical theory of what causes are, and no single foolproof computational method for finding them with absolute certainty. What makes this even more challenging is that, depending on one’s definition of causality, different factors may be identified as causes in the same situation, and it may not be clear what the ground truth is.</p></blockquote><h2 id=why-study-causality-now>Why study causality now?<a hidden class=anchor aria-hidden=true href=#why-study-causality-now>#</a></h2><p>While it&rsquo;s hard to conclusively prove, it seems to me like interest in formal causal analysis has increased in recent years. My hypothesis is that it&rsquo;s just a natural progression along the levels of <a href=https://yanirseroussi.com/2014/08/17/datas-hierarchy-of-needs/>data&rsquo;s hierarchy of needs</a>. At the start of the big data boom, people were mostly concerned with storing and processing large amounts of data (e.g., using Hadoop, Elasticsearch, or your favourite NoSQL database). Just having your data flowing through pipelines is nice, but not very useful, so the focus switched to reporting and visualisation to extract insights about what happened (commonly known as business intelligence). While having a good picture of what happened is great, it isn&rsquo;t enough – you can make better decisions if you can predict what&rsquo;s going to happen, so the focus switched again to predictive analytics. Those who are familiar with predictive analytics know that models often end up relying on correlations between the features and the predicted labels. Using such models without considering the meaning of the variables can lead us to erroneous conclusions, and potentially harmful interventions. For example, based on the following graph we may make a recommendation that the US government decrease its spending on science to reduce the number of suicides by hanging.</p><figure><a href=us-science-spending-versus-suicides.png target=_blank rel=noopener><img sizes="(min-width: 768px) 720px,
100vw" srcset="https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/us-science-spending-versus-suicides_hucb19a666efd495d868358d2e56c5c43f_82139_360x0_resize_box_3.png 360w,
https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/us-science-spending-versus-suicides_hucb19a666efd495d868358d2e56c5c43f_82139_480x0_resize_box_3.png 480w,
https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/us-science-spending-versus-suicides_hucb19a666efd495d868358d2e56c5c43f_82139_720x0_resize_box_3.png 720w,
https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/us-science-spending-versus-suicides_hucb19a666efd495d868358d2e56c5c43f_82139_1080x0_resize_box_3.png 1080w,
https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/us-science-spending-versus-suicides_hucb19a666efd495d868358d2e56c5c43f_82139_1500x0_resize_box_3.png 1500w," src=https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/us-science-spending-versus-suicides_hucb19a666efd495d868358d2e56c5c43f_82139_800x0_resize_box_3.png alt="US science spending versus suicides" loading=lazy></a><figcaption><p>Source: <a href=http://www.tylervigen.com/spurious-correlations target=_blank rel=noopener>Spurious Correlations by Tyler Vigen</a></p></figcaption></figure><p>Causal analysis aims to identify factors that are independent of spurious correlations, allowing stakeholders to make well-informed decisions. It is all about <a href=https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/>getting to the top of the DIKW (data-information-knowledge-wisdom) pyramid</a> by understanding <strong>why</strong> things happen and what we can do to change the world. However, finding true causes can be very hard, especially in cases where you can&rsquo;t perform experiments. <a href=http://ftp.cs.ucla.edu/pub/stat_ser/r391.pdf target=_blank rel=noopener>Judea Pearl explains it well</a>:</p><blockquote><p>We know, from first principles, that any causal conclusion drawn from observational studies must rest on untested causal assumptions. Cartwright (1989) named this principle ‘no causes in, no causes out,&rsquo; which follows formally from the theory of equivalent models (Verma and Pearl, 1990); for any model yielding a conclusion C, one can construct a statistically equivalent model that refutes C and fits the data equally well.</p></blockquote><p>What this means in practice is that you can&rsquo;t, for example, conclusively prove that smoking causes cancer without making some reasonable assumptions about the mechanisms at play. For ethical reasons, we can&rsquo;t perform a randomly controlled trial where a test group is forced to smoke for years while a control group is forced not to smoke. Therefore, our conclusions about the causal link between smoking and cancer are drawn from observational studies and an understanding of the mechanisms by which various cancers develop (e.g., the effect of cigarette smoke on individual cells can be studied without forcing people to smoke). <del>Cancer</del> Tobacco companies have exploited this fact for years, making the claim that the probability of both cancer and smoking is raised by some mysterious genetic factors. Fossil fuel and food companies use similar arguments to sell their products and block attempts to regulate their industries (as discussed in previous posts on <a href=https://yanirseroussi.com/2015/11/23/the-hardest-parts-of-data-science/>the hardest parts of data science</a> and <a href=https://yanirseroussi.com/2015/10/19/nutritionism-and-the-need-for-complex-models-to-explain-complex-phenomena/>nutritionism</a>). Fighting against such arguments is an uphill battle, as it is easy to sow doubt with a few simplistic catchphrases, while proving and communicating causality to laypeople is much harder (or <a href=http://www.sciencealert.com/new-study-links-climate-change-denials-with-conspiracy-theories target=_blank rel=noopener>impossible when it comes to deeply-held irrational beliefs</a>).</p><h2 id=my-causality-journey-is-just-beginning>My causality journey is just beginning<a hidden class=anchor aria-hidden=true href=#my-causality-journey-is-just-beginning>#</a></h2><p>My interest in formal causal analysis was seeded a couple of years ago, with a reading group that was dedicated to Judea Pearl&rsquo;s work. We didn&rsquo;t get very far, as I was a bit disappointed with what causal calculus can and cannot do. This may have been because I didn&rsquo;t come in with the right expectations – I expected a black box that automatically finds causes. Recently reading Samantha Kleinberg&rsquo;s excellent book <a href=http://www.skleinberg.org/why/ target=_blank rel=noopener>Why: A Guide to Finding and Using Causes</a> has made my expectations somewhat more realistic:</p><blockquote><p>Thousands of years after Aristotle&rsquo;s seminal work on causality, hundreds of years after Hume gave us two definitions of it, and decades after automated inference became a possibility through powerful new computers, causality is still an unsolved problem. Humans are prone to seeing causality where it does not exist and our algorithms aren&rsquo;t foolproof. Even worse, once we find a cause it&rsquo;s still hard to use this information to prevent or produce an outcome because of limits on what information we can collect and how we can understand it. After looking at all the cases where methods haven’t worked and researchers and policy makers have gotten causality really wrong, you might wonder why you should bother.</p><p>[&mldr;]</p><p>Rather than giving up on causality, what we need to give up on is the idea of having a black box that takes some data straight from its source and emits a stream of causes with no need for interpretation or human intervention. Causal inference is necessary and possible, but it is not perfect and, most importantly, it requires domain knowledge.</p></blockquote><p>Kleinberg&rsquo;s book is a great general intro to causality, but it intentionally omits the mathematical details behind the various methods. I am now ready to once again go deeper into causality, perhaps starting with Kleinberg&rsquo;s more technical book, <a href=http://www.skleinberg.org/causality_book/index.html target=_blank rel=noopener>Causality, Probability, and Time</a>. Other recommendations are very welcome!</p><p style=font-size:80%><i>Cover image source: <a href=https://xkcd.com/552/ target=_blank rel=noopener>xkcd: Correlation</a></i></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yanirseroussi.com/tags/analytics/>analytics</a></li><li><a href=https://yanirseroussi.com/tags/causal-inference/>causal inference</a></li><li><a href=https://yanirseroussi.com/tags/data-science/>data science</a></li><li><a href=https://yanirseroussi.com/tags/deep-learning/>deep learning</a></li><li><a href=https://yanirseroussi.com/tags/insights/>insights</a></li><li><a href=https://yanirseroussi.com/tags/machine-learning/>machine learning</a></li><li><a href=https://yanirseroussi.com/tags/predictive-modelling/>predictive modelling</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Why you should stop worrying about deep learning and deepen your understanding of causality instead on x" href="https://x.com/intent/tweet/?text=Why%20you%20should%20stop%20worrying%20about%20deep%20learning%20and%20deepen%20your%20understanding%20of%20causality%20instead&amp;url=https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f&amp;hashtags=analytics%2ccausalinference%2cdatascience%2cdeeplearning%2cinsights%2cmachinelearning%2cpredictivemodelling"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why you should stop worrying about deep learning and deepen your understanding of causality instead on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f&amp;title=Why%20you%20should%20stop%20worrying%20about%20deep%20learning%20and%20deepen%20your%20understanding%20of%20causality%20instead&amp;summary=Why%20you%20should%20stop%20worrying%20about%20deep%20learning%20and%20deepen%20your%20understanding%20of%20causality%20instead&amp;source=https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why you should stop worrying about deep learning and deepen your understanding of causality instead on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f&title=Why%20you%20should%20stop%20worrying%20about%20deep%20learning%20and%20deepen%20your%20understanding%20of%20causality%20instead"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why you should stop worrying about deep learning and deepen your understanding of causality instead on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why you should stop worrying about deep learning and deepen your understanding of causality instead on whatsapp" href="https://api.whatsapp.com/send?text=Why%20you%20should%20stop%20worrying%20about%20deep%20learning%20and%20deepen%20your%20understanding%20of%20causality%20instead%20-%20https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why you should stop worrying about deep learning and deepen your understanding of causality instead on telegram" href="https://telegram.me/share/url?text=Why%20you%20should%20stop%20worrying%20about%20deep%20learning%20and%20deepen%20your%20understanding%20of%20causality%20instead&amp;url=https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why you should stop worrying about deep learning and deepen your understanding of causality instead on ycombinator" href="https://news.ycombinator.com/submitlink?t=Why%20you%20should%20stop%20worrying%20about%20deep%20learning%20and%20deepen%20your%20understanding%20of%20causality%20instead&u=https%3a%2f%2fyanirseroussi.com%2f2016%2f02%2f14%2fwhy-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><section class=comment-section><p class="post-content contact-cta">Public comments are closed, but I love hearing from readers. Feel free to
<a href=/about/#contact-me target=_blank>contact me</a> with your thoughts.</p><div class=comment-level-0 id=comment-1073><div class=comment-header><a href=#comment-1073><img class=comment-avatar src="https://www.gravatar.com/avatar/831fe3b72cff0094cf7287a9a22f4ee2?s=50"><p class=comment-info><strong>chewiebeans</strong><br><small>2016-02-15 01:03:41</small></p></a></div><div class="comment-body post-content">It seems to me that causality is another of our thought conveniences, just one more attempt at linearising our frustratingly non-linear existence, akin to teaching with Newtonian physics, segue to Einstein’s relativistic mechanics when the kids are ready (if ever). Cyclic systems can self-perpetuate in non-repeating cycles (chaos theory) but also respond with or resist change arising from external inputs. I believe when people speak of causality, what they are really thinking about (and desiring) is a conversation around stability versus volatility.</div></div><div class=comment-level-1 id=comment-1078><div class=comment-header><a href=#comment-1078><img class=comment-avatar src="https://www.gravatar.com/avatar/cd2a2b049bc7e4b0f79965e1f54ba25c?s=50"><p class=comment-info><strong>willw9</strong><br><small>2016-02-15 15:31:12</small></p></a></div><div class="comment-body post-content">Great comment.</div></div><div class=comment-level-0 id=comment-1075><div class=comment-header><a href=#comment-1075><img class=comment-avatar src="https://www.gravatar.com/avatar/3a974c8365594f5920b6b20ac70c9da9?s=50"><p class=comment-info><strong>Jim Savage</strong><br><small>2016-02-15 03:04:50</small></p></a></div><div class="comment-body post-content"><p>Hey Yanir - great post.</p><p>If you&rsquo;ve not already, you should read Mostly Harmless Econometrics. They take quite a different approach to causality than Pearl (though there is a lot of conceptual overlap). It definitely helps build intuition for the topic. It&rsquo;s also worth reading the relevant mid-70s papers from Rubin.</p></div></div><div class=comment-level-1 id=comment-1076><div class=comment-header><a href=#comment-1076><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-02-15 08:20:06</small></p></a></div><div class="comment-body post-content">Thanks for the pointers, Jim! I&rsquo;ll check those resources out.</div></div><div class=comment-level-1 id=comment-1257><div class=comment-header><a href=#comment-1257><img class=comment-avatar src="https://www.gravatar.com/avatar/76e0485a85f7f65d1ce164fc79a2c888?s=50"><p class=comment-info><strong>Will Lowe (@conjugateprior)</strong><br><small>2016-08-02 02:32:44</small></p></a></div><div class="comment-body post-content">It&rsquo;s not a different approach. The notation is different but the two frameworks (Pearl&rsquo;s and Neyman-Rubin) have been proved equivalent.</div></div><div class=comment-level-0 id=comment-1077><div class=comment-header><a href=#comment-1077><img class=comment-avatar src="https://www.gravatar.com/avatar/79adc945afb3363bd9275758c06a14a6?s=50"><p class=comment-info><strong>M Edward/Ed Borasky (@znmeb)</strong><br><small>2016-02-15 10:01:19</small></p></a></div><div class="comment-body post-content">I took a look at the Amazon sample for Causality, Probability and Time but I doubt if I&rsquo;ll buy it just yet. I&rsquo;ve got Judea Pearl&rsquo;s Probabilistic Reasoning in Intelligent Systems already and think I want to work through that in a programming language (R is my first choice) before buying any more books. ;-)</div></div><div class=comment-level-0 id=comment-1079><div class=comment-header><a href=#comment-1079><img class=comment-avatar src="https://www.gravatar.com/avatar/7e01dcd2246f3af62589e9b3c70cffe9?s=50"><p class=comment-info><strong>Joanna</strong><br><small>2016-02-15 16:01:01</small></p></a></div><div class="comment-body post-content">I appreciate this post. I teach General Psychology, and this is a central issue that I present to my students. In the meantime, I regularly come across articles, in peer-reviewed as wells as mainstream publications, which discuss correlational data as if it were supporting a causal relationship. As I tell my students, one of the difficulties is the use of the word &ldquo;factor&rdquo; in both types of discussions. In correlation, factors are pieces of information which give you a more likely guess about an unknown piece of information. In causation, factors are things that contribute to something else existing. Both concepts feed the mind&rsquo;s desire to find patterns in the relevant world which inform our decisions/behaviors so that we can continue living, hopefully in a pleasant state. We are often tricked by these patterns (illusions, etc.), but most of the time they pan out in a beneficial way. Making the leap from &ldquo;this is how things tend to work in my immediate experience&rdquo; to &ldquo;this is how things work everywhere for everyone&rdquo; is where theories are born, where science lives, and where we often make mistakes along the way. Proceed with caution from observation to theory, but by all means, proceed!</div></div><div class=comment-level-0 id=comment-1082><div class=comment-header><a href=#comment-1082><img class=comment-avatar src="https://www.gravatar.com/avatar/123acf099145a4a605d9ac2f65ecd277?s=50"><p class=comment-info><strong>Kyle Gagnon</strong><br><small>2016-02-16 01:33:53</small></p></a></div><div class="comment-body post-content">Really great read! This is something many of my colleagues have discussed in the past. Here&rsquo;s an article that might help us get closer to causality with observational data: <a href=http://goo.gl/MP7WQo target=_blank rel=noopener>http://goo.gl/MP7WQo</a>, and here is a video about it: <a href="https://www.youtube.com/watch?v=uhONGgfx8Do" target=_blank rel=noopener>https://www.youtube.com/watch?v=uhONGgfx8Do</a></div></div><div class=comment-level-0 id=comment-1087><div class=comment-header><a href=#comment-1087><img class=comment-avatar src="https://www.gravatar.com/avatar/22d41e5b6ff197cd7900c0514d1bd305?s=50"><p class=comment-info><strong>Boris Gorelik</strong><br><small>2016-02-18 09:56:34</small></p></a></div><div class="comment-body post-content">The problem with the search for causality (or, more generally, explainability) is that in many cases, it is &ldquo;not interesting&rdquo;. If I click on Google search results, neither me nor Google algorithm developers are truly interested how the algorithm decided to rank Page A before Page B. It is OK for me, as an end user, not to care about those details, as much as I don&rsquo;t care hydraulics every time I take a shower. Is it OK for me, as a data scientist, not to care about the reasons behind my models? Honestly, I don&rsquo;t yet know.</div></div><div class=comment-level-1 id=comment-1093><div class=comment-header><a href=#comment-1093><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-02-18 20:42:41</small></p></a></div><div class="comment-body post-content"><p>I agree that in many cases the reasoning behind models isn&rsquo;t interesting, as long as the models produce satisfactory results. Web search is actually a good example. Yes, many end users don&rsquo;t really care how Google ranks pages, but SEO practitioners go to great lengths to understand search algorithms and get pages to rank well (see <a href=https://moz.com/search-ranking-factors target=_blank rel=noopener>https://moz.com/search-ranking-factors</a> for example).</p><p>As data scientists, it&rsquo;s important to consider model stability in production. Sculley et al. said it well in their paper on machine learning technical debt (<a href=http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf%29 target=_blank rel=noopener>http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf)</a>: &ldquo;Machine learning systems often have a difficult time distinguishing the impact of correlated features. This may not seem like a major problem: if two features are always correlated, but only one is truly causal, it may still seem okay to ascribe credit to both and rely on their observed co-occurrence. However, if the world suddenly stops making these features co-occur, prediction behavior may change significantly.&rdquo;</p><p>Finally, in many cases what we really care about is interventionality. I don&rsquo;t think it&rsquo;s a real word, but what it means is that you don&rsquo;t really care whether A causes B, you want to know whether intervening to change A would change B. These inferences are critical in fields like medicine and marketing, but we can look at an example from the world of blogging, which is probably more relevant to you. Many bloggers would like to attract more readers. A possible costly intervention would be to switch platforms from WordPress to Medium. Cheaper interventions may be changing the site&rsquo;s layout, writing titles that get people interested, and posting links to your content on relevant channels. Another intervention would be trying to post at different times (as implied by WordPress insights and discussed in <a href=http://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/%29>http://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/)</a>. Obviously, one would like to apply the interventions with the highest return on investment first, and data that helps with ranking the interventions is very interesting.</p></div></div><div class=comment-level-0 id=comment-1088><div class=comment-header><a href=#comment-1088><img class=comment-avatar src="https://www.gravatar.com/avatar/efedc95784b33ec9dbc8ae53e5a5cbd7?s=50"><p class=comment-info><strong>Greg Gandenberger</strong><br><small>2016-02-18 15:53:34</small></p></a></div><div class="comment-body post-content">James Woodward&rsquo;s <em>Making Things Happen</em> gives a fantastic, relatively non-technical analysis of causation that fits well with Pearl&rsquo;s approach.</div></div><div class=comment-level-1 id=comment-1089><div class=comment-header><a href=#comment-1089><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-02-18 18:56:04</small></p></a></div><div class="comment-body post-content">Thanks! I&rsquo;ll check it out.</div></div><div class=comment-level-0 id=comment-1090><div class=comment-header><a href=#comment-1090><img class=comment-avatar src="https://www.gravatar.com/avatar/5ae7cb133b8f140346083a4885fa1569?s=50"><p class=comment-info><strong>Joel Kreager</strong><br><small>2016-02-18 18:58:51</small></p></a></div><div class="comment-body post-content">I&rsquo;ve been thinking about this lately quite a bit. The fact that I can type this comment and send it across the internet rests on the ability to create a completely controlled causal environment. Inside the computer, all noise and randomness is kept below the threshold of the data, and every process is completely causal. Meanwhile, outside the computer, most measurements are mostly noise, and extracting any sort of causal relation is very difficult and often impossible. My mind seems to have some sort of idea of cause as something like the interaction of balls on a pool table. The que ball strikes the eight ball and knocks it into the corner pocket, etc. But when one tries to measure things, mostly one finds nothing like this. Instead, one finds that some measurements tend to be found with other measurements most of the time, but not all of the time. Cause thus seems a statistical thing, and in no way absolute. I have difficulty reconciling the two views. One thing that occured to me to investigate, was the manner in which several huge internet outages developed involving the BGP protocol. It seemed to me that every individual packet must experience a completely causal path, but the aggregate turns into the statistical causal form we most usually deal with. I haven&rsquo;t followed up with this idea so far, however</div></div><div class=comment-level-1 id=comment-1099><div class=comment-header><a href=#comment-1099><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-02-20 09:41:25</small></p></a></div><div class="comment-body post-content">Interesting. I think that one of the dividing factors between traditional software engineering and data science is the attitude towards uncertainty. Whereas, as you say, coding is all about creating a controlled deterministic environment, data science and statistics thrive on uncertainty. It&rsquo;s similar with computer networks as well, where there is always a non-deterministic element (e.g., packets may be lost, arrive out-of-order, or come in bursts).</div></div><div class=comment-level-0 id=comment-1091><div class=comment-header><a href=#comment-1091><img class=comment-avatar src="https://www.gravatar.com/avatar/efedc95784b33ec9dbc8ae53e5a5cbd7?s=50"><p class=comment-info><strong>Greg Gandenberger</strong><br><small>2016-02-18 20:02:05</small></p></a></div><div class="comment-body post-content"><p><em>Causation, Prediction, and Search</em> is also seminal (<a href=https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/scottd/fullbook.pdf%29 target=_blank rel=noopener>https://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/scottd/fullbook.pdf)</a>.</p><p>Disclosure: I did my PhD just down the street from the authors of <em>Causation, Prediction, and Search</em>, and Woodward was on my thesis committee.</p></div></div><div class=comment-level-0 id=comment-1092><div class=comment-header><a href=#comment-1092><img class=comment-avatar src="https://www.gravatar.com/avatar/efedc95784b33ec9dbc8ae53e5a5cbd7?s=50"><p class=comment-info><strong>Greg Gandenberger</strong><br><small>2016-02-18 20:12:58</small></p></a></div><div class="comment-body post-content"><p>There is a subtle difference between Woodward&rsquo;s approach and that of Pearl and of Spirtes et al., which Glymour discusses in the following places:</p><p><a href=https://www.ncbi.nlm.nih.gov/pubmed/24887161 target=_blank rel=noopener>https://www.ncbi.nlm.nih.gov/pubmed/24887161</a>
<a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1280&amp;amp;context=philosophy" target=_blank rel=noopener>http://repository.cmu.edu/cgi/viewcontent.cgi?article=1280&amp;context=philosophy</a></p><p>Basically, Woodward starts with the notion of an intervention on a variable and defines other concepts (e.g. direct cause) in terms of it, whereas Pearl and Spirtes et al. start with the notion of direct cause. One consequence of this difference is that properties like sex and race that cannot be intervened upon in a straightforward way cannot be causes for Woodward, strictly speaking, but can be for Pearl and Spirtes et al. This is a fine point, however, and it&rsquo;s very nearly true that they simply provide alternative formulations of the same theory, with Woodward focusing on conceptual issues and the others focus on methodology.</p></div></div><div class=comment-level-1 id=comment-1098><div class=comment-header><a href=#comment-1098><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-02-20 09:26:23</small></p></a></div><div class="comment-body post-content">Thanks for all the pointers, Greg! I&rsquo;ll definitely check them out. Personally, I have a slight bias towards Pearl, as he is my academic grandfather (he was my advisor&rsquo;s advisor), but I&rsquo;m keen on learning as much as possible on all the different approaches to causality. It is a fascinating area!</div></div><div class=comment-level-0 id=comment-1095><div class=comment-header><a href=#comment-1095><img class=comment-avatar src="https://www.gravatar.com/avatar/3c84c9b8370a1242028b7f5f8cbb21b0?s=50"><p class=comment-info><strong>jasonhand24</strong><br><small>2016-02-19 15:12:30</small></p></a></div><div class="comment-body post-content">&ldquo;Thinking, Fast & Slow&rdquo; touches on some of this in later chapters. Some algebra is used to help illustrate the deception causality and efforts towards finding it can cause.</div></div><div class=comment-level-1 id=comment-1097><div class=comment-header><a href=#comment-1097><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-02-19 21:44:31</small></p></a></div><div class="comment-body post-content">Thanks! That book has been on my to-read list for a while now.</div></div><div class=comment-level-0 id=comment-1111><div class=comment-header><a href=#comment-1111><img class=comment-avatar src="https://www.gravatar.com/avatar/7d959875c747233a7578b7830e9ab384?s=50"><p class=comment-info><strong>jozvison</strong><br><small>2016-02-25 04:59:57</small></p></a></div><div class="comment-body post-content">Reblogged this on <a href=https://jozvison.wordpress.com/2016/02/25/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/ rel=nofollow>jozvison</a>.</div></div><div class=comment-level-0 id=comment-1112><div class=comment-header><a href=#comment-1112><img class=comment-avatar src="https://www.gravatar.com/avatar/1dc04229b204b5bd03fef0b7c0561e42?s=50"><p class=comment-info><strong>Jerome</strong><br><small>2016-02-25 17:28:47</small></p></a></div><div class="comment-body post-content">Great Post, Thanks Yanir.
I have been afraid of being lost in Big data ie swarmed by such a vast amount of correlations.
So understanding causality is important to make better better decision.
Not being a data scientist but a startuper, my approach is to trying to understand how many signals I perceive and what storytelling I do with them; then what vision of reality I get&mldr;D Kahneman it&rsquo;s great help as we should always be aware that our story is made up of only a small proportion of all signals and the causilty we build into a story it&rsquo;s only one among many.
So, how we improve the process??
Still searching;)
Jerome</div></div><div class=comment-level-1 id=comment-1114><div class=comment-header><a href=#comment-1114><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2016-02-25 19:49:34</small></p></a></div><div class="comment-body post-content">Thanks Jerome. I think the main things that we should do in everyday life is be aware of biases and keep questioning things. The Five Whys method (<a href=https://en.wikipedia.org/wiki/5_Whys target=_blank rel=noopener>https://en.wikipedia.org/wiki/5_Whys</a>) is an example of such deep questioning.</div></div><div class=comment-level-0 id=comment-1253><div class=comment-header><a href=#comment-1253><img class=comment-avatar src="https://www.gravatar.com/avatar/8ae304d3e2cae45be6dabe907d7c0637?s=50"><p class=comment-info><strong>Félix Balazard</strong><br><small>2016-08-01 19:05:52</small></p></a></div><div class="comment-body post-content">I am tangentially interested in that fundamental topic. I enjoyed a lot a short research course by marloes mathuis on causality in high dimensions. There were both interesting algorithms and mathematics results maybe consistency. It&rsquo;s worth a look !</div></div><div class=comment-level-0 id=comment-1255><div class=comment-header><a href=#comment-1255><img class=comment-avatar src="https://www.gravatar.com/avatar/57d87bea2ff0024f7c30361e59bf9072?s=50"><p class=comment-info><strong>urishin</strong><br><small>2016-08-01 21:04:59</small></p></a></div><div class="comment-body post-content"><p>Great writeup!
I think causality will become something that data scientists will need to acknowledge and think about more explicitly.</p><p>Coming from machine learning, it took me a while to wrap my head around the subtle but important differences in the way similar ideas are used in prediction vs. causal inference.</p><p>Recently I gave an ICML tutorial about causality, together with David Sontag. This might be of interest to your readers as a starting point, particulalry for people who are well-versed in ML. It&rsquo;s here:
<a href=https://www.cs.nyu.edu/~shalit/tutorial.html target=_blank rel=noopener>www.cs.nyu.edu/~shalit/tutorial.html</a></p></div></div><div class=comment-level-0 id=comment-1256><div class=comment-header><a href=#comment-1256><img class=comment-avatar src="https://www.gravatar.com/avatar/9d7c0633fe9e782859ada015a4254e65?s=50"><p class=comment-info><strong>brandonrohrer</strong><br><small>2016-08-02 00:23:52</small></p></a></div><div class="comment-body post-content">Hi Yanir,
Thanks for addressing this huge gap in how we interpret data! Models that handle causality well will take us from finding cats in images to solving more subtle problems in robotics and adaptive systems. The topic deserves more attention than it gets. Thanks for keeping it in the spotlight.
Brandon</div></div><div class=comment-level-0 id=comment-1275><div class=comment-header><a href=#comment-1275><img class=comment-avatar src="https://www.gravatar.com/avatar/ba93a0603ff68abb505a24f6cbe91024?s=50"><p class=comment-info><strong>Qi</strong><br><small>2016-08-08 20:38:55</small></p></a></div><div class="comment-body post-content">My understanding is that causality is always the central focus of science. Machine learning/data mining is a relatively more recent thing, and its greatest benefit lies in solving complex prediction problems. But I think causality study and data mining can help each other. For the purpose of understanding causality, data mining can be used in an exploratory way that helps scientists to generate theories (I think it is possible to study the features (i.e., hidden units) extracted by deep learning networks), then experiments, longitudinal studies and traditional stats can be used to test the theories. For the purpose of solving practical prediction problems, theories developed from causality studies can help identify useful features as input to the machine learning algorithms. In fact, this was done all the times especially before deep learning became popular. I agree that scientists should improve their understanding of causality, but picking up new technologies that take advantage of modern computers and large data won&rsquo;t really hurt.</div></div><div class=comment-level-0 id=comment-1370><div class=comment-header><a href=#comment-1370><img class=comment-avatar src="https://www.gravatar.com/avatar/85d2ef7b3fd426e74d1164ff75eb5af1?s=50"><p class=comment-info><strong>Digital Cosmology (@DCosmology)</strong><br><small>2016-12-08 16:17:50</small></p></a></div><div class="comment-body post-content"><p>&ldquo;According to Nicolas Malebranche and other seventeenth century Cartesian occasionalists, what we actually call causes are really no more than
occasions on which, in accordance with his own laws, God acts to bring about the effect. If one were to replace the notion of God in the occasionalism doctrine by the notion of a mechanism, then a modern (ormechanical) occasionalist could assert that what we actually call causes are no more than occasions on which a mechanism acts to bring about the effect. "</p><p>More info here:http://fqxi.org/community/forum/topic/846 and here <a href=http://www.digitalcosmology.com/Blog/beyond-intelligent-design/ target=_blank rel=noopener>http://www.digitalcosmology.com/Blog/beyond-intelligent-design/</a></p></div></div><div class=comment-level-0 id=comment-1460><div class=comment-header><a href=#comment-1460><img class=comment-avatar src="https://www.gravatar.com/avatar/4d173c6cea40e15f74dbbc24690e911d?s=50"><p class=comment-info><strong>Jason Vondersmith</strong><br><small>2017-02-27 17:49:55</small></p></a></div><div class="comment-body post-content">I really enjoyed this post. I&rsquo;m obviously familiar with the causation vs correlation argument but never stopped to think about what would define causation. I like the example about the seat belt (an argument I often have with my wife). Thanks for the post!</div></div><div class=comment-level-0 id=comment-2003><div class=comment-header><a href=#comment-2003><img class=comment-avatar src="https://www.gravatar.com/avatar/b8560c6b1bfc8adc05563a997406974a?s=50"><p class=comment-info><strong>david</strong><br><small>2017-12-03 13:06:21</small></p></a></div><div class="comment-body post-content"><p>Hi Yanir,</p><p>Great post. If you are serious abut exploring causality further: check out Transfer Entropy and related information theory inspired measures: I assure you, you will be amazed.</p></div></div><div class=comment-level-1 id=comment-2005><div class=comment-header><a href=#comment-2005><img class=comment-avatar src="https://www.gravatar.com/avatar/dda019c47a6183120608a6aeac2db6c5?s=50"><p class=comment-info><strong>Yanir Seroussi</strong><br><small>2017-12-03 20:32:53</small></p></a></div><div class="comment-body post-content">Thanks, David! I&rsquo;ll check it out.</div></div></section></article></main><footer class=footer><span>Text and figures licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank rel=noopener>CC BY-NC-ND 4.0</a> by <a href=https://yanirseroussi.com/about/>Yanir Seroussi</a>, except where noted otherwise  |</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><div class=mailing-list-container><form class=mailing-list action="https://yanirseroussi.us17.list-manage.com/subscribe/post?u=3c08aa3ff27dd92978019febd&amp;id=bc3ab705af" method=post target=_blank novalidate><label for=mailing-list-email>Get new post notifications</label>
<input type=text name=EMAIL id=mailing-list-email placeholder="Email address"><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_3c08aa3ff27dd92978019febd_bc3ab705af tabindex=-1></div><input type=submit value=Subscribe></form><div class=footer>Alternatively, <a href=https://yanirseroussi.com/index.xml>subscribe to RSS feed</a>.</div></div><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>