<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) | Yanir Seroussi | Data & AI for Startup Impact</title>
<meta name=keywords content="data science,gradient boosting,Kaggle,Kaggle competition,machine learning,predictive modelling,search engine optimisation"><meta name=description content="My team&rsquo;s solution to the Yandex Search Personalisation competition (finished 9th out of 194 teams)."><meta name=author content="Yanir Seroussi"><link rel=canonical href=https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/><meta name=google-site-verification content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU"><link crossorigin=anonymous href=/assets/css/stylesheet.0139a50c6e3f53193500e07972ba88238beaf3384629640b34fa4fd38dc956f6.css integrity="sha256-ATmlDG4/Uxk1AOB5crqII4vq8zhGKWQLNPpP043JVvY=" rel="preload stylesheet" as=style><link rel=icon href=https://yanirseroussi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yanirseroussi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yanirseroussi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yanirseroussi.com/apple-touch-icon.png><link rel=mask-icon href=https://yanirseroussi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)"><meta property="og:description" content="My team&rsquo;s solution to the Yandex Search Personalisation competition (finished 9th out of 194 teams)."><meta property="og:type" content="article"><meta property="og:url" content="https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/"><meta property="og:image" content="https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/rating.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2015-02-11T06:34:17+00:00"><meta property="article:modified_time" content="2024-01-16T09:56:03+10:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/rating.png"><meta name=twitter:title content="Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)"><meta name=twitter:description content="My team&rsquo;s solution to the Yandex Search Personalisation competition (finished 9th out of 194 teams)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Browse Posts","item":"https://yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)","item":"https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)","name":"Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)","description":"My team\u0026rsquo;s solution to the Yandex Search Personalisation competition (finished 9th out of 194 teams).","keywords":["data science","gradient boosting","Kaggle","Kaggle competition","machine learning","predictive modelling","search engine optimisation"],"articleBody":"This is the second and last post summarising my team’s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.\nTo quickly recap the first post, Yandex released a 16GB dataset of query \u0026 click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results. Relevance is determined by time spent on each clicked result (non-clicked results are deemed irrelevant), and overall performance is scored using the normalised discounted cumulative gain (NDCG) measure. No data about the content of sites or queries was given – each query in the dataset is a list of token IDs and each result is a (url ID, domain ID) pair.\nFirst steps: memory-based heuristics My initial approach wasn’t very exciting: it involved iterating through the data, summarising it in one way or another, and assigning new relevance scores to each (user, session, query) combination. In this early stage I also implemented an offline validation framework, which is an important part of every Kaggle competition: in this case I simply set aside the last three days of data for local testing, because the test dataset that was used for the leaderboard consisted of three days of log data.\nSomewhat surprisingly, my heuristics worked quite well and put me in a top-10 position on the leaderboard. It seems like the barrier of entry for this competition was higher than for other Kaggle competitions due to the size of the data and the fact that it wasn’t given as preprocessed feature vectors. This was evident from questions on the forum, where people noted that they were having trouble downloading and looking at the data.\nThe heuristic models that worked well included:\nReranking based on mean relevance (this just swapped positions 9 \u0026 10, probably because users are more likely to click the last result) Reranking based on mean relevance for (query, url) and (query, domain) pairs (non-personalised improvements) Downranking urls observed previously in a session Each one of the heuristic models was set to output relevance scores. The models were then ensembled by simply summing the relevance scores.\nThen, I started playing with a collaborative-filtering-inspired matrix factorisation model for predicting relevance, which didn’t work too well. At around that time, I got too busy with other stuff and decided to quit while I’m ahead.\nGetting more serious with some team work and LambdaMART A few weeks after quitting, I somehow volunteered to organise Kaggle teams for newbies at the Sydney Data Science Meetup group. At that point I was joined by my teammates, which served as a good motivation to do more stuff.\nThe first thing we tried was another heuristic model I read about in one of the papers suggested by the organisers: just reranking based on the fact that people often repeat queries as a navigational aid (e.g., search for Facebook and click Facebook). Combined in a simple linear model with the other heuristics, this put us at #4. Too easy 🙂\nWith all the new motivation, it was time to read more papers and start doing things properly. We ended up using Ranklib’s LambdaMART implementation as one of our main models, and also used LambdaMART to combine the various models (the old heuristics still helped the overall score, as did the matrix factorisation model).\nUsing LambdaMART made it possible to directly optimise the NDCG measure, turning the key problem into feature engineering, i.e., finding good features to feed into the model. Explaining how LambdaMART works is beyond the scope of this post (see this paper for an in-depth discussion), but the basic idea (which is also shared by other learning to rank algorithms) is that rather than trying to solve the hard problem of predicting relevance (i.e., a regression problem), the algorithm tries to predict the ranking that yields the best results according to a user-chosen measure.\nWe tried many features for the LambdaMART model, but after feature selection (using a method learned from Phil Brierley’s talk) the best features turned out to be:\npercentage_recurrent_term_ids: percentage of term IDs from the test query that appeared previously in the session — indicates if this query refines previous queries query_mean_ndcg: historical NDCG for this query — indicates how satisfied people are with the results of this query. Interestingly, we also tried query click entropy, but it performed worse. Probably because we’re optimising the NDCG rather than click-through rate. query_num_unique_serps: how many different result pages were shown for this query query_mean_result_dwell_time: how much time on average people spend per result for this query user_mean_ndcg: like query_mean_ndcg, but for users — a low NDCG indicates that this user is likely to be dissatisfied with the results. As for query_mean_ndcg, adding this feature yielded better results than using the user’s click entropy. user_num_click_actions_with_relevance_0: over the history of this user, how many of their clicks had relevance 0 (i.e., short dwell time). Interestingly, user_num_click_actions_with_relevance_1 and user_num_click_actions_with_relevance_2 were found to be less useful. user_num_query_actions: number of queries performed by the user rank: the original rank, as assigned by Yandex previous_query_url_relevance_in_session: modelling repeated results within a session, e.g., if a (query, url) pair was already found irrelevant in this session, the user may not want to see it again previous_url_relevance_in_session: the same as previous_query_url_relevance_in_session, but for a url regardless of the query user_query_url_relevance_sum: over the entire history of the user, not just the session user_normalised_rank_relevance: how relevant does the user usually find this rank? The idea is that some people are more likely to go through all the results than others query_url_click_probability: estimated simply as num_query_url_clicks / num_query_url_occurrences (across all the users) average_time_on_page: how much time people spend on this url on average Our best submission ended up placing us at the 9th place (out of 194 teams), which is respectable. Things got a bit more interesting towards the end of the competition – if we had used the original heuristic model that put at #4 early on, we would have finished 18th.\nConclusion I really enjoyed this competition. The data was well-organised and well-defined, which is not something you get in every competition (or in “real life”). Its size did present some challenges, but we stuck to using flat files and some preprocessing and other tricks to speed things up (e.g., I got to use Cython for the first time). It was good to learn how learning to rank algorithms work and get some insights on search personalisation. As is often the case with Kaggle competitions, this was time well spent.\n","wordCount":"1114","inLanguage":"en","image":"https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/rating.png","datePublished":"2015-02-11T06:34:17Z","dateModified":"2024-01-16T09:56:03+10:00","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data \u0026 AI for Startup Impact","logo":{"@type":"ImageObject","url":"https://yanirseroussi.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data & AI for Startup Impact (Alt + H)">Yanir Seroussi | Data & AI for Startup Impact</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://yanirseroussi.com/about/ title=About><span>About</span></a></li><li><a href=https://yanirseroussi.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yanirseroussi.com/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://yanirseroussi.com/consult/ title=Consulting><span>Consulting</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)</h1><div class=post-meta><span title='2015-02-11 06:34:17 +0000 UTC'>February 11, 2015</span></div></header><figure class=entry-cover><img loading=eager src=https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/rating.png alt></figure><div class=post-content><p>This is the second and last post summarising my team&rsquo;s solution for the <a href=https://www.kaggle.com/c/yandex-personalized-web-search-challenge target=_blank rel=noopener>Yandex search personalisation Kaggle competition</a>. <a href=https://yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/ title="Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)">See the first post</a> for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.</p><p>To quickly recap the <a href=https://yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/ title="Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)">first post</a>, Yandex released a 16GB dataset of query & click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results. Relevance is determined by time spent on each clicked result (non-clicked results are deemed irrelevant), and overall performance is scored using the <a href=https://en.wikipedia.org/wiki/Discounted_cumulative_gain target=_blank rel=noopener>normalised discounted cumulative gain (NDCG) measure</a>. No data about the content of sites or queries was given – each query in the dataset is a list of token IDs and each result is a (url ID, domain ID) pair.</p><h3 id=first-steps-memory-based-heuristics>First steps: memory-based heuristics<a hidden class=anchor aria-hidden=true href=#first-steps-memory-based-heuristics>#</a></h3><p>My initial approach wasn&rsquo;t very exciting: it involved iterating through the data, summarising it in one way or another, and assigning new relevance scores to each (user, session, query) combination. In this early stage I also implemented an offline validation framework, <a href=https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/ title="How to (almost) win Kaggle competitions">which is an important part of every Kaggle competition</a>: in this case I simply set aside the last three days of data for local testing, because the test dataset that was used for the leaderboard consisted of three days of log data.</p><p>Somewhat surprisingly, my heuristics worked quite well and put me in a top-10 position on the leaderboard. It seems like the barrier of entry for this competition was higher than for other Kaggle competitions due to the size of the data and the fact that it wasn&rsquo;t given as preprocessed feature vectors. This was evident from questions on the forum, where people noted that they were having trouble downloading and looking at the data.</p><p>The heuristic models that worked well included:</p><ul><li>Reranking based on mean relevance (this just swapped positions 9 & 10, probably because users are more likely to click the last result)</li><li>Reranking based on mean relevance for (query, url) and (query, domain) pairs (non-personalised improvements)</li><li>Downranking urls observed previously in a session</li></ul><p>Each one of the heuristic models was set to output relevance scores. The models were then ensembled by simply summing the relevance scores.</p><p>Then, I started playing with a <a href=https://en.wikipedia.org/wiki/Collaborative_filtering target=_blank rel=noopener>collaborative-filtering</a>-inspired matrix factorisation model for predicting relevance, which didn&rsquo;t work too well. At around that time, I got too busy with other stuff and decided to quit while I&rsquo;m ahead.</p><h3 id=getting-more-serious-with-some-team-work-and-lambdamart>Getting more serious with some team work and LambdaMART<a hidden class=anchor aria-hidden=true href=#getting-more-serious-with-some-team-work-and-lambdamart>#</a></h3><p>A few weeks after quitting, I somehow volunteered to organise Kaggle teams for newbies at the <a href=http://www.meetup.com/Data-Science-Sydney/ target=_blank rel=noopener>Sydney Data Science Meetup group</a>. At that point I was joined by my teammates, which served as a good motivation to do more stuff.</p><p>The first thing we tried was another heuristic model I read about in one of the <a href=https://www.kaggle.com/c/yandex-personalized-web-search-challenge/details/related-papers target=_blank rel=noopener>papers suggested by the organisers</a>: just reranking based on the fact that people often repeat queries as a navigational aid (e.g., search for Facebook and click Facebook). Combined in a simple linear model with the other heuristics, this put us at #4. Too easy 🙂</p><p>With all the new motivation, it was time to read more papers and start doing things properly. We ended up using <a href=http://sourceforge.net/p/lemur/wiki/RankLib/ target=_blank rel=noopener>Ranklib&rsquo;s LambdaMART implementation</a> as one of our main models, and also used LambdaMART to combine the various models (the old heuristics still helped the overall score, as did the matrix factorisation model).</p><p>Using LambdaMART made it possible to directly optimise the NDCG measure, turning the key problem into feature engineering, i.e., finding good features to feed into the model. Explaining how LambdaMART works is beyond the scope of this post (<a href=http://research.microsoft.com/pubs/132652/MSR-TR-2010-82.pdf target=_blank rel=noopener>see this paper for an in-depth discussion</a>), but the basic idea (which is also shared by other <a href=https://en.wikipedia.org/wiki/Learning_to_rank target=_blank rel=noopener>learning to rank</a> algorithms) is that rather than trying to solve the hard problem of predicting relevance (i.e., a regression problem), the algorithm tries to predict the ranking that yields the best results according to a user-chosen measure.</p><p>We tried many features for the LambdaMART model, but after feature selection (using a method learned from <a href=http://anotherdataminingblog.blogspot.com.au/2013/10/techniques-to-improve-accuracy-of-your_17.html target=_blank rel=noopener>Phil Brierley&rsquo;s talk</a>) the best features turned out to be:</p><ul><li>percentage_recurrent_term_ids: percentage of term IDs from the test query that appeared previously in the session — indicates if this query refines previous queries</li><li>query_mean_ndcg: historical NDCG for this query — indicates how satisfied people are with the results of this query. Interestingly, we also tried query click entropy, but it performed worse. Probably because we&rsquo;re optimising the NDCG rather than click-through rate.</li><li>query_num_unique_serps: how many different result pages were shown for this query</li><li>query_mean_result_dwell_time: how much time on average people spend per result for this query</li><li>user_mean_ndcg: like query_mean_ndcg, but for users — a low NDCG indicates that this user is likely to be dissatisfied with the results. As for query_mean_ndcg, adding this feature yielded better results than using the user&rsquo;s click entropy.</li><li>user_num_click_actions_with_relevance_0: over the history of this user, how many of their clicks had relevance 0 (i.e., short dwell time). Interestingly, user_num_click_actions_with_relevance_1 and user_num_click_actions_with_relevance_2 were found to be less useful.</li><li>user_num_query_actions: number of queries performed by the user</li><li>rank: the original rank, as assigned by Yandex</li><li>previous_query_url_relevance_in_session: modelling repeated results within a session, e.g., if a (query, url) pair was already found irrelevant in this session, the user may not want to see it again</li><li>previous_url_relevance_in_session: the same as previous_query_url_relevance_in_session, but for a url regardless of the query</li><li>user_query_url_relevance_sum: over the entire history of the user, not just the session</li><li>user_normalised_rank_relevance: how relevant does the user usually find this rank? The idea is that some people are more likely to go through all the results than others</li><li>query_url_click_probability: estimated simply as num_query_url_clicks / num_query_url_occurrences (across all the users)</li><li>average_time_on_page: how much time people spend on this url on average</li></ul><p>Our best submission ended up placing us at the 9th place (out of 194 teams), which is respectable. Things got a bit more interesting towards the end of the competition – if we had used the original heuristic model that put at #4 early on, we would have finished 18th.</p><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>I really enjoyed this competition. The data was well-organised and well-defined, which is not something you get in every competition (or in &ldquo;real life&rdquo;). Its size did present some challenges, but we stuck to using flat files and some preprocessing and other tricks to speed things up (e.g., I got to use <a href=http://cython.org/ target=_blank rel=noopener>Cython</a> for the first time). It was good to learn how learning to rank algorithms work and get some insights on search personalisation. As is often the case with Kaggle competitions, this was time well spent.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yanirseroussi.com/tags/data-science/>Data Science</a></li><li><a href=https://yanirseroussi.com/tags/gradient-boosting/>Gradient Boosting</a></li><li><a href=https://yanirseroussi.com/tags/kaggle/>Kaggle</a></li><li><a href=https://yanirseroussi.com/tags/kaggle-competition/>Kaggle Competition</a></li><li><a href=https://yanirseroussi.com/tags/machine-learning/>Machine Learning</a></li><li><a href=https://yanirseroussi.com/tags/predictive-modelling/>Predictive Modelling</a></li><li><a href=https://yanirseroussi.com/tags/search-engine-optimisation/>Search Engine Optimisation</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) on x" href="https://x.com/intent/tweet/?text=Learning%20to%20rank%20for%20personalised%20search%20%28Yandex%20Search%20Personalisation%20%e2%80%93%20Kaggle%20Competition%20Summary%20%e2%80%93%20Part%202%29&amp;url=https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f&amp;hashtags=datascience%2cgradientboosting%2cKaggle%2cKagglecompetition%2cmachinelearning%2cpredictivemodelling%2csearchengineoptimisation"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f&amp;title=Learning%20to%20rank%20for%20personalised%20search%20%28Yandex%20Search%20Personalisation%20%e2%80%93%20Kaggle%20Competition%20Summary%20%e2%80%93%20Part%202%29&amp;summary=Learning%20to%20rank%20for%20personalised%20search%20%28Yandex%20Search%20Personalisation%20%e2%80%93%20Kaggle%20Competition%20Summary%20%e2%80%93%20Part%202%29&amp;source=https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f&title=Learning%20to%20rank%20for%20personalised%20search%20%28Yandex%20Search%20Personalisation%20%e2%80%93%20Kaggle%20Competition%20Summary%20%e2%80%93%20Part%202%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) on whatsapp" href="https://api.whatsapp.com/send?text=Learning%20to%20rank%20for%20personalised%20search%20%28Yandex%20Search%20Personalisation%20%e2%80%93%20Kaggle%20Competition%20Summary%20%e2%80%93%20Part%202%29%20-%20https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) on telegram" href="https://telegram.me/share/url?text=Learning%20to%20rank%20for%20personalised%20search%20%28Yandex%20Search%20Personalisation%20%e2%80%93%20Kaggle%20Competition%20Summary%20%e2%80%93%20Part%202%29&amp;url=https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Learning%20to%20rank%20for%20personalised%20search%20%28Yandex%20Search%20Personalisation%20%e2%80%93%20Kaggle%20Competition%20Summary%20%e2%80%93%20Part%202%29&u=https%3a%2f%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><a href=/contact/#mailing-list-email target=_blank aria-label="subscribe to mailing list" class=mailing-list-link id=mailing-list-link>Subscribe
</a><script>const mailingListButton=document.getElementById("mailing-list-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mailingListButton.style.visibility="visible",mailingListButton.style.opacity="1"):(mailingListButton.style.visibility="hidden",mailingListButton.style.opacity="0")}</script><div class=mailing-list-container><script src=https://f.convertkit.com/ckjs/ck.5.js></script><form class="mailing-list seva-form formkit-form" action=https://app.convertkit.com/forms/6549537/subscriptions method=post data-sv-form=6549537 data-uid=9157759fce data-format=inline data-version=5 data-options='{"settings":{"after_subscribe":{"action":"message","redirect_url":"","success_message":"Success! Now check your email to confirm your subscription."},"recaptcha":{"enabled":false},"return_visitor":{"action":"show","custom_content":""}},"version":"5"}'><div data-style=clean><ul class="formkit-alert formkit-alert-error" data-element=errors data-group=alert></ul><div data-element=fields data-stacked=false><label for=mailing-list-email>Get weekly posts in your mailbox</label>
<input id=mailing-list-email name=email_address aria-label="Email address" placeholder="Email address" required type=email>
<button data-element=submit>Subscribe</button></div></div></form><div class=footer>Join hundreds of subscribers. No spam or AI-generated slop. Unsubscribe any time.</div></div><section class=comment-section><p class="post-content contact-cta">Public comments are closed, but I love hearing from readers. Feel free to
<a href=/contact/ target=_blank>contact me</a> with your thoughts.</p><div class=comment-level-0 id=comment-2052><div class=comment-header><a href=#comment-2052><img class=comment-avatar src="https://www.gravatar.com/avatar/1fc0fb275ad25219b5c017921c5e71ec?s=50"><p class=comment-info><strong>nitin</strong><br><small>2017-12-17 13:02:37</small></p></a></div><div class="comment-body post-content"><p>I do not understand how your featureset helped the model to learn anything.
For example,
user_num_query_actions: number of queries performed by the user</p><p>How will it affect the order of search results for a new/test query.</p></div></div></section></article></main><div class=global-footer><div class=footer><span>Text and figures licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank rel=noopener>CC BY-NC-ND 4.0</a> by <a href=https://yanirseroussi.com/about/>Yanir Seroussi</a>, except where noted otherwise&nbsp;&nbsp;|</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
      <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></div></div><script>const menuTrigger=document.querySelector("#menu-trigger"),menuElem=document.querySelector(".menu");menuTrigger.addEventListener("click",function(){menuElem.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){menuTrigger.contains(e.target)||menuElem.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>