<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2) | Yanir Seroussi | Data science and beyond</title>
<meta name=keywords content="data science,gradient boosting,kaggle,kaggle competition,machine learning,predictive modelling,search engine optimisation">
<meta name=description content="This is the second and last post summarising my team&rsquo;s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.
To quickly recap the first post, Yandex released a 16GB dataset of query & click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results.">
<meta name=author content="Yanir Seroussi">
<link rel=canonical href=https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/>
<link crossorigin=anonymous href=/yanirseroussi.com/assets/css/stylesheet.min.51e68192da3381c9040a063242bafad56d6d28666fff1f9e523f9eaad0207a83.css integrity="sha256-UeaBktozgckECgYyQrr61W1tKGZv/x+eUj+eqtAgeoM=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/yanirseroussi.com/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirs.github.io/yanirseroussi.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://yanirs.github.io/yanirseroussi.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://yanirs.github.io/yanirseroussi.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://yanirs.github.io/yanirseroussi.com/apple-touch-icon.png>
<link rel=mask-icon href=https://yanirs.github.io/yanirseroussi.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)">
<meta property="og:description" content="This is the second and last post summarising my team&rsquo;s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.
To quickly recap the first post, Yandex released a 16GB dataset of query & click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/">
<meta property="og:image" content="https://yanirs.github.io/yanirseroussi.com/rating.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2015-02-11T06:34:17+00:00">
<meta property="article:modified_time" content="2015-02-11T06:34:17+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://yanirs.github.io/yanirseroussi.com/rating.png">
<meta name=twitter:title content="Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)">
<meta name=twitter:description content="This is the second and last post summarising my team&rsquo;s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.
To quickly recap the first post, Yandex released a 16GB dataset of query & click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirs.github.io/yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)","item":"https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)","name":"Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)","description":"This is the second and last post summarising my team\u0026rsquo;s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.\nTo quickly recap the first post, Yandex released a 16GB dataset of query \u0026amp; click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results.","keywords":["data science","gradient boosting","kaggle","kaggle competition","machine learning","predictive modelling","search engine optimisation"],"articleBody":"This is the second and last post summarising my team’s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.\nTo quickly recap the first post, Yandex released a 16GB dataset of query \u0026 click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results. Relevance is determined by time spent on each clicked result (non-clicked results are deemed irrelevant), and overall performance is scored using the normalised discounted cumulative gain (NDCG) measure. No data about the content of sites or queries was given – each query in the dataset is a list of token IDs and each result is a (url ID, domain ID) pair.\nFirst steps: memory-based heuristics My initial approach wasn’t very exciting: it involved iterating through the data, summarising it in one way or another, and assigning new relevance scores to each (user, session, query) combination. In this early stage I also implemented an offline validation framework, which is an important part of every Kaggle competition: in this case I simply set aside the last three days of data for local testing, because the test dataset that was used for the leaderboard consisted of three days of log data.\nSomewhat surprisingly, my heuristics worked quite well and put me in a top-10 position on the leaderboard. It seems like the barrier of entry for this competition was higher than for other Kaggle competitions due to the size of the data and the fact that it wasn’t given as preprocessed feature vectors. This was evident from questions on the forum, where people noted that they were having trouble downloading and looking at the data.\nThe heuristic models that worked well included:\n Reranking based on mean relevance (this just swapped positions 9 \u0026 10, probably because users are more likely to click the last result) Reranking based on mean relevance for (query, url) and (query, domain) pairs (non-personalised improvements) Downranking urls observed previously in a session  Each one of the heuristic models was set to output relevance scores. The models were then ensembled by simply summing the relevance scores.\nThen, I started playing with a collaborative-filtering-inspired matrix factorisation model for predicting relevance, which didn’t work too well. At around that time, I got too busy with other stuff and decided to quit while I’m ahead.\nGetting more serious with some team work and LambdaMART A few weeks after quitting, I somehow volunteered to organise Kaggle teams for newbies at the Sydney Data Science Meetup group. At that point I was joined by my teammates, which served as a good motivation to do more stuff.\nThe first thing we tried was another heuristic model I read about in one of the papers suggested by the organisers: just reranking based on the fact that people often repeat queries as a navigational aid (e.g., search for Facebook and click Facebook). Combined in a simple linear model with the other heuristics, this put us at #4. Too easy 🙂\nWith all the new motivation, it was time to read more papers and start doing things properly. We ended up using Ranklib’s LambdaMART implementation as one of our main models, and also used LambdaMART to combine the various models (the old heuristics still helped the overall score, as did the matrix factorisation model).\nUsing LambdaMART made it possible to directly optimise the NDCG measure, turning the key problem into feature engineering, i.e., finding good features to feed into the model. Explaining how LambdaMART works is beyond the scope of this post (see this paper for an in-depth discussion), but the basic idea (which is also shared by other learning to rank algorithms) is that rather than trying to solve the hard problem of predicting relevance (i.e., a regression problem), the algorithm tries to predict the ranking that yields the best results according to a user-chosen measure.\nWe tried many features for the LambdaMART model, but after feature selection (using a method learned from Phil Brierley’s talk) the best features turned out to be:\n percentage_recurrent_term_ids: percentage of term IDs from the test query that appeared previously in the session — indicates if this query refines previous queries query_mean_ndcg: historical NDCG for this query — indicates how satisfied people are with the results of this query. Interestingly, we also tried query click entropy, but it performed worse. Probably because we’re optimising the NDCG rather than click-through rate. query_num_unique_serps: how many different result pages were shown for this query query_mean_result_dwell_time: how much time on average people spend per result for this query user_mean_ndcg: like query_mean_ndcg, but for users — a low NDCG indicates that this user is likely to be dissatisfied with the results. As for query_mean_ndcg, adding this feature yielded better results than using the user’s click entropy. user_num_click_actions_with_relevance_0: over the history of this user, how many of their clicks had relevance 0 (i.e., short dwell time). Interestingly, user_num_click_actions_with_relevance_1 and user_num_click_actions_with_relevance_2 were found to be less useful. user_num_query_actions: number of queries performed by the user rank: the original rank, as assigned by Yandex previous_query_url_relevance_in_session: modelling repeated results within a session, e.g., if a (query, url) pair was already found irrelevant in this session, the user may not want to see it again previous_url_relevance_in_session: the same as previous_query_url_relevance_in_session, but for a url regardless of the query user_query_url_relevance_sum: over the entire history of the user, not just the session user_normalised_rank_relevance: how relevant does the user usually find this rank? The idea is that some people are more likely to go through all the results than others query_url_click_probability: estimated simply as num_query_url_clicks / num_query_url_occurrences (across all the users) average_time_on_page: how much time people spend on this url on average  Our best submission ended up placing us at the 9th place (out of 194 teams), which is respectable. Things got a bit more interesting towards the end of the competition – if we had used the original heuristic model that put at #4 early on, we would have finished 18th.\nConclusion I really enjoyed this competition. The data was well-organised and well-defined, which is not something you get in every competition (or in “real life”). Its size did present some challenges, but we stuck to using flat files and some preprocessing and other tricks to speed things up (e.g., I got to use Cython for the first time). It was good to learn how learning to rank algorithms work and get some insights on search personalisation. As is often the case with Kaggle competitions, this was time well spent.\n","wordCount":"1114","inLanguage":"en","image":"https://yanirs.github.io/yanirseroussi.com/rating.png","datePublished":"2015-02-11T06:34:17Z","dateModified":"2015-02-11T06:34:17Z","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirs.github.io/yanirseroussi.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://yanirs.github.io/yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)
</h1>
<div class=post-meta>February 11, 2015&nbsp;·&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2015-02-11-learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a>
</div>
</header>
<figure class=entry-cover>
<img loading=lazy srcset="https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/rating.png 250w" sizes="(min-width: 768px) 720px, 100vw" src=https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/rating.png alt width=250 height=264>
</figure>
<div class=post-content><p>This is the second and last post summarising my team&rsquo;s solution for the <a href=https://www.kaggle.com/c/yandex-personalized-web-search-challenge target=_blank rel=noopener>Yandex search personalisation Kaggle competition</a>. <a href=http://yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/ title="Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)">See the first post</a> for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.</p>
<p>To quickly recap the <a href=http://yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/ title="Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)">first post</a>, Yandex released a 16GB dataset of query & click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results. Relevance is determined by time spent on each clicked result (non-clicked results are deemed irrelevant), and overall performance is scored using the <a href=https://en.wikipedia.org/wiki/Discounted_cumulative_gain target=_blank rel=noopener>normalised discounted cumulative gain (NDCG) measure</a>. No data about the content of sites or queries was given – each query in the dataset is a list of token IDs and each result is a (url ID, domain ID) pair.</p>
<h3 id=first-steps-memory-based-heuristics>First steps: memory-based heuristics<a hidden class=anchor aria-hidden=true href=#first-steps-memory-based-heuristics>#</a></h3>
<p>My initial approach wasn&rsquo;t very exciting: it involved iterating through the data, summarising it in one way or another, and assigning new relevance scores to each (user, session, query) combination. In this early stage I also implemented an offline validation framework, <a href=http://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/ title="How to (almost) win Kaggle competitions">which is an important part of every Kaggle competition</a>: in this case I simply set aside the last three days of data for local testing, because the test dataset that was used for the leaderboard consisted of three days of log data.</p>
<p>Somewhat surprisingly, my heuristics worked quite well and put me in a top-10 position on the leaderboard. It seems like the barrier of entry for this competition was higher than for other Kaggle competitions due to the size of the data and the fact that it wasn&rsquo;t given as preprocessed feature vectors. This was evident from questions on the forum, where people noted that they were having trouble downloading and looking at the data.</p>
<p>The heuristic models that worked well included:</p>
<ul>
<li>Reranking based on mean relevance (this just swapped positions 9 & 10, probably because users are more likely to click the last result)</li>
<li>Reranking based on mean relevance for (query, url) and (query, domain) pairs (non-personalised improvements)</li>
<li>Downranking urls observed previously in a session</li>
</ul>
<p>Each one of the heuristic models was set to output relevance scores. The models were then ensembled by simply summing the relevance scores.</p>
<p>Then, I started playing with a <a href=https://en.wikipedia.org/wiki/Collaborative_filtering target=_blank rel=noopener>collaborative-filtering</a>-inspired matrix factorisation model for predicting relevance, which didn&rsquo;t work too well. At around that time, I got too busy with other stuff and decided to quit while I&rsquo;m ahead.</p>
<h3 id=getting-more-serious-with-some-team-work-and-lambdamart>Getting more serious with some team work and LambdaMART<a hidden class=anchor aria-hidden=true href=#getting-more-serious-with-some-team-work-and-lambdamart>#</a></h3>
<p>A few weeks after quitting, I somehow volunteered to organise Kaggle teams for newbies at the <a href=http://www.meetup.com/Data-Science-Sydney/ target=_blank rel=noopener>Sydney Data Science Meetup group</a>. At that point I was joined by my teammates, which served as a good motivation to do more stuff.</p>
<p>The first thing we tried was another heuristic model I read about in one of the <a href=https://www.kaggle.com/c/yandex-personalized-web-search-challenge/details/related-papers target=_blank rel=noopener>papers suggested by the organisers</a>: just reranking based on the fact that people often repeat queries as a navigational aid (e.g., search for Facebook and click Facebook). Combined in a simple linear model with the other heuristics, this put us at #4. Too easy 🙂</p>
<p>With all the new motivation, it was time to read more papers and start doing things properly. We ended up using <a href=http://sourceforge.net/p/lemur/wiki/RankLib/ target=_blank rel=noopener>Ranklib&rsquo;s LambdaMART implementation</a> as one of our main models, and also used LambdaMART to combine the various models (the old heuristics still helped the overall score, as did the matrix factorisation model).</p>
<p>Using LambdaMART made it possible to directly optimise the NDCG measure, turning the key problem into feature engineering, i.e., finding good features to feed into the model. Explaining how LambdaMART works is beyond the scope of this post (<a href=http://research.microsoft.com/pubs/132652/MSR-TR-2010-82.pdf target=_blank rel=noopener>see this paper for an in-depth discussion</a>), but the basic idea (which is also shared by other <a href=https://en.wikipedia.org/wiki/Learning_to_rank target=_blank rel=noopener>learning to rank</a> algorithms) is that rather than trying to solve the hard problem of predicting relevance (i.e., a regression problem), the algorithm tries to predict the ranking that yields the best results according to a user-chosen measure.</p>
<p>We tried many features for the LambdaMART model, but after feature selection (using a method learned from <a href=http://anotherdataminingblog.blogspot.com.au/2013/10/techniques-to-improve-accuracy-of-your_17.html target=_blank rel=noopener>Phil Brierley&rsquo;s talk</a>) the best features turned out to be:</p>
<ul>
<li>percentage_recurrent_term_ids: percentage of term IDs from the test query that appeared previously in the session — indicates if this query refines previous queries</li>
<li>query_mean_ndcg: historical NDCG for this query — indicates how satisfied people are with the results of this query. Interestingly, we also tried query click entropy, but it performed worse. Probably because we&rsquo;re optimising the NDCG rather than click-through rate.</li>
<li>query_num_unique_serps: how many different result pages were shown for this query</li>
<li>query_mean_result_dwell_time: how much time on average people spend per result for this query</li>
<li>user_mean_ndcg: like query_mean_ndcg, but for users — a low NDCG indicates that this user is likely to be dissatisfied with the results. As for query_mean_ndcg, adding this feature yielded better results than using the user&rsquo;s click entropy.</li>
<li>user_num_click_actions_with_relevance_0: over the history of this user, how many of their clicks had relevance 0 (i.e., short dwell time). Interestingly, user_num_click_actions_with_relevance_1 and user_num_click_actions_with_relevance_2 were found to be less useful.</li>
<li>user_num_query_actions: number of queries performed by the user</li>
<li>rank: the original rank, as assigned by Yandex</li>
<li>previous_query_url_relevance_in_session: modelling repeated results within a session, e.g., if a (query, url) pair was already found irrelevant in this session, the user may not want to see it again</li>
<li>previous_url_relevance_in_session: the same as previous_query_url_relevance_in_session, but for a url regardless of the query</li>
<li>user_query_url_relevance_sum: over the entire history of the user, not just the session</li>
<li>user_normalised_rank_relevance: how relevant does the user usually find this rank? The idea is that some people are more likely to go through all the results than others</li>
<li>query_url_click_probability: estimated simply as num_query_url_clicks / num_query_url_occurrences (across all the users)</li>
<li>average_time_on_page: how much time people spend on this url on average</li>
</ul>
<p>Our best submission ended up placing us at the 9th place (out of 194 teams), which is respectable. Things got a bit more interesting towards the end of the competition – if we had used the original heuristic model that put at #4 early on, we would have finished 18th.</p>
<h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3>
<p>I really enjoyed this competition. The data was well-organised and well-defined, which is not something you get in every competition (or in &ldquo;real life&rdquo;). Its size did present some challenges, but we stuck to using flat files and some preprocessing and other tricks to speed things up (e.g., I got to use <a href=http://cython.org/ target=_blank rel=noopener>Cython</a> for the first time). It was good to learn how learning to rank algorithms work and get some insights on search personalisation. As is often the case with Kaggle competitions, this was time well spent.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/data-science/>data science</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/gradient-boosting/>gradient boosting</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/kaggle/>kaggle</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/kaggle-competition/>kaggle competition</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/machine-learning/>machine learning</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/predictive-modelling/>predictive modelling</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/search-engine-optimisation/>search engine optimisation</a></li>
</ul>
</footer><section class=comment-section>
<strong>1 comment</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirs.github.io%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>
Comment via GitHub issue
</a>
<div class=comment-level-0 id=comment-2052>
<div class=comment-header>
<a href=#comment-2052>
<img class=comment-avatar src="https://www.gravatar.com/avatar/1fc0fb275ad25219b5c017921c5e71ec?s=50">
<p class=comment-info>
<strong>nitin</strong><br>
<small>2017-12-17 13:02:37</small>
</p>
</a>
</div>
<div class="comment-body post-content">
<p>I do not understand how your featureset helped the model to learn anything.
For example,
user_num_query_actions: number of queries performed by the user</p>
<p>How will it affect the order of search results for a new/test query.</p>
</div>
<a class="comment-button comment-button-small" href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New reply to https%3a%2f%2fyanirs.github.io%2fyanirseroussi.com%2f2015%2f02%2f11%2flearning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2%2f%23comment-2052&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>
Reply via GitHub issue
</a>
</div>
</section>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://yanirs.github.io/yanirseroussi.com/>Yanir Seroussi | Data science and beyond</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>