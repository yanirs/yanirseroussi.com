<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Learning about deep learning through album cover classification | Yanir Seroussi | Data science and beyond</title>
<meta name=keywords content="data science,deep learning,machine learning,predictive modelling">
<meta name=description content="In the past month, I&rsquo;ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.
Initial steps summary The following points were discussed in detail in the previous post on this project.">
<meta name=author content="Yanir Seroussi">
<link rel=canonical href=https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/>
<link crossorigin=anonymous href=/yanirseroussi.com/assets/css/stylesheet.min.51e68192da3381c9040a063242bafad56d6d28666fff1f9e523f9eaad0207a83.css integrity="sha256-UeaBktozgckECgYyQrr61W1tKGZv/x+eUj+eqtAgeoM=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/yanirseroussi.com/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirs.github.io/yanirseroussi.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://yanirs.github.io/yanirseroussi.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://yanirs.github.io/yanirseroussi.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://yanirs.github.io/yanirseroussi.com/apple-touch-icon.png>
<link rel=mask-icon href=https://yanirs.github.io/yanirseroussi.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Learning about deep learning through album cover classification">
<meta property="og:description" content="In the past month, I&rsquo;ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.
Initial steps summary The following points were discussed in detail in the previous post on this project.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/">
<meta property="og:image" content="https://yanirs.github.io/yanirseroussi.com/bandcamp-album-covers-by-genre.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2015-07-06T22:21:42+00:00">
<meta property="article:modified_time" content="2015-07-06T22:21:42+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://yanirs.github.io/yanirseroussi.com/bandcamp-album-covers-by-genre.png">
<meta name=twitter:title content="Learning about deep learning through album cover classification">
<meta name=twitter:description content="In the past month, I&rsquo;ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.
Initial steps summary The following points were discussed in detail in the previous post on this project.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirs.github.io/yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Learning about deep learning through album cover classification","item":"https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learning about deep learning through album cover classification","name":"Learning about deep learning through album cover classification","description":"In the past month, I\u0026rsquo;ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.\nInitial steps summary The following points were discussed in detail in the previous post on this project.","keywords":["data science","deep learning","machine learning","predictive modelling"],"articleBody":"In the past month, I’ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.\nInitial steps summary The following points were discussed in detail in the previous post on this project.\n The problem I chose to work on is classifying Bandcamp album covers by genre, using a balanced dataset of 10,000 images from 10 different genres. The experimental code is based on Lasagne, and is available on GitHub. Having set up the environment for running experiments on a GPU, the plan was to get Lasagne’s examples working on my dataset, and then iteratively read tutorials/papers/books, implement ideas, play with parameters, and visualise parts of the network until I’m satisfied with the results.  Preliminary experiments and learning resources I hit several issues when adapting Lasagne’s example code to my dataset. The key issue is that the example code is based on the MNIST digits dataset. That dataset’s images are 28×28 grayscale, and my dataset’s images are 350×350 RGB. This difference led to the training loss quickly diverging when running the example code without any changes. It turns out that simply lowering the learning rate resolves this issue, though the initial results I got were still not much better than random. In general, it appears that everything works on the MNIST digits dataset, so choosing to work on my own dataset made things more challenging (which is a good thing).\nThe main learning resource I used is the excellent notes for the Stanford course Convolutional Neural Networks for Visual Recognition. The notes are very clear, contain up-to-date information from recent publications, and include many practical tips for successful training of convolutional networks (convnets). In addition, I read some other tutorials and a few papers. These are summarised in a separate page.\nThe first step after getting the MNIST examples working on my dataset was to extend the code to enable more flexible architectures. My main focus was on vanilla convnets, i.e., networks with several convolutional layers, where each convolutional layer is optionally followed by a max-pooling layer, and the convolutional layers are followed by multiple dense/fully-connected layers and dropout layers. To allow for easy experimentation, the specification of the network can be done from the command line. For example, to train an AlexNet architecture:\n$ python manage.py run_experiment \\  --dataset-path /path/to/dataset \\  --model-architecture ConvNet \\  --model-params num_conv_layers=5:num_dense_layers=2:lc0_num_filters=48:lc0_filter_size=11:lc0_stride=4:lc0_mp=True:lm0_pool_size=3:lm0_stride=2:lc1_num_filters=128:lc1_filter_size=5:lc1_mp=True:lm1_pool_size=3:lm1_stride=2:lc2_num_filters=192:lc2_filter_size=3:lc3_num_filters=192:lc3_filter_size=3:lc4_num_filters=128:lc4_filter_size=3:lc4_mp=True:lm4_pool_size=3:lm4_stride=2:ld0_num_units=2048:ld1_num_units=2048 This can obviously be a bit of a mouthful, so common architectures are also defined in the code with parameters that can be overridden. For instance, to train an AlexNet with 64 filters in the first layer instead of 48:\n$ python manage.py run_experiment \\  --dataset-path /path/to/dataset \\  --model-architecture AlexNet \\  --model-params lc0_num_filters=64 There are many more command line flags (possibly too many), which make it easy to both tinker with various settings, and also run more rigorous experiments. My initial tinkering with convnets didn’t yield impressive results in terms of predictive accuracy on my dataset. It turned out that this was partly due to the lack of preprocessing – the less exciting but crucial part of any predictive modelling work.\nThe importance of preprocessing My initial focus was on getting things to work on the dataset without worrying too much about preprocessing. I haven’t done any image classification work in the past, so I had to learn about the right type of preprocessing to use. I kept it pretty simple and applied the following transformations:\n Downsampling: all images were scaled down to 256×256. I played briefly with other sizes, but decided on this size to make it easy to use models pretrained on ImageNet. Cropping \u0026 mirroring: during training time, each image was cropped to random 224×224 slices. Deterministic slices were used in test time. In addition, each crop was mirrored horizontally. In most cases I used ten overall crops. Again, these numbers were chosen for comparability with ImageNet-trained models. Mean subtraction: the training mean of each pixel was subtracted from each instance. Shuffling: probably the most important preprocessing step. Initially I had the instances sorted by their class, as an artifact of the way the dataset was constructed. Due to the relatively small number of instances the network sees in each batch, this meant that in each epoch, the network first fitted on all the instances from class 1, then all the instances from class 2, etc. This led to very poor performance, which was fixed by shuffling the data once at the start of the training procedure (shuffling every epoch could potentially make things even better).  Baselines After building the experimental environment and a fair bit of tinkering, I decided it was time for some more serious experiments. The results of my initial games were rather disappointing – slightly better than a random baseline, which yields an accuracy score of 10%. Therefore, I ran some baselines to get an idea of what’s possible on this dataset.\nThe first baseline I tried was a random forest with 1,000 trees, which yielded 15.25% accuracy. This baseline was trained directly on the pixel values without any preprocessing other than downsampling. It’s worth noting that the downsampling size didn’t make much of a difference to this baseline (I tried a few values in the range 50×50-350×350). This baseline was also not particularly sensitive to whether RGB or grayscale values were used to represent the images.\nThe next experiments were with baselines that utilised pretrained Caffe models. Training a random forest with 1,000 trees on features extracted from the highest fully-connected layer (fc7) in the CaffeNet and VGGNet-19 models yielded accuracies of 16.72% and 16.40% respectively. This was pretty disappointing, as I expected these features to perform much better. The reason may be that album covers are very different from ImageNet images, and the representations in fc7 are too specific to ImageNet. Indeed, when fine-tuning the CaffeNet model (following the procedure outlined here), I got the best accuracy on the dataset: 22.60%. Using Caffe to train the same network from scratch didn’t even get close to this accuracy. However, I didn’t try to tune Caffe’s learning parameters. Instead, I went back to running experiments with my code.\nIt’s worth noting that the classes identified by the CaffeNet model often have little to do with the actual content of the image. Better baseline results may be obtained by using models that were pretrained on a richer dataset than ImageNet. The following table presents three example covers together with the top-five classes identified by the CaffeNet model for each image. The tags assigned by Clarifai’s API are also presented for comparison. From this example, it looks like Clarifai’s model is more successful at identifying the correct elements than the CaffeNet model, indicating that a baseline that uses the Clarifai tags may yield competitive performance.\n   Album CaffeNet Clarifai        October by Wille P\nhiphop_rap    digital clock, spotlight, jack-o'-lantern, volcano, traffic light tree, landscape, sunset, desert, sun, sunrise, nature, evening, sky, travel      Demo by Blackrat\nmetal    spider web, barn spider, chain, bubble, fountain skull, bone, nobody, death, vector, help, horror, medicine, black and white, tattoo      The Kool-Aid Album by Mr. Merge\nsoul    dishrag, paper towel, honeycomb, envelope, chain mail symbol, nobody, sign, illustration, color, flag, text, stripes, business, character    Training from scratch My initial experiments were with various convnet architectures, where I manually varied the filter sizes and number of layers to have a reasonable number of parameters and ensure that the model is trainable on a GPU with 4GB of memory. As mentioned, this approach yielded unimpressive results. Following the relative success of the fine-tuned CaffeNet baseline, I decided to run more rigorous experiments on variants of AlexNet (which is very similar to CaffeNet).\nGiven the large number of hyperparameters that need to be set when training deep convnets, I realised that setting values manually or via grid search is unlikely to yield the best results. To address this, I used hyperopt to search for the best configuration of values. The hyperparameters that were included in the search were the learning method (Nesterov momentum versus Adam with their respective parameters), the learning rate, whether crops are mirrored or not, the number of crops to use (1 or 5), dropout probabilities, the number of hidden units in the fully-connected layers, and the number of filters in each convolutional layer.\nEach configuration suggested by hyperopt was trained for 10 epochs, and the promising setups were trained until results stopped improving. The results of the search were rather disappointing, with the best accuracy being 17.19%. However, I learned a lot by finding hyperparameters in this manner – in the past I’ve only used a combination of manual settings with grid search.\nThere are many possible reasons for why the results are so poor. It could be that there’s just too little data to train a good classifier, which is supported by the inability to beat the fine-tuned results. This is in line with the results obtained by Zeiler and Fergus (2013), who found that convnets pretrained on ImageNet performed much better on the Caltech-101 and Caltech-256 datasets than the same networks trained from scratch. However, it could also be that I just didn’t run enough experiments – I definitely feel like I haven’t explored everything as well as I’d like. In addition, I’m still building my intuition for what works and why. I should work more on visualising the way the network learns to uncover more hidden gotchas in addition to those I’ve already found. Finally, it could be that it’s just too hard to distinguish between covers from the genres I chose for the study.\nIdeas for future work There are many avenues for improving on the work I’ve done so far. The code could definitely be made more robust and better tested, optimised and parallelised. It would be worth investing more in hyperparameter and architecture search, including incorporation of ideas from non-vanilla convnets (e.g., GoogLeNet). This search should be guided by visualisation and a deeper understanding of the trained networks, which may also come from analysing class-level accuracy (certain genres seem to be easier to distinguish than others). In addition, more sophisticated preprocessing may yield improved results.\nIf the goal were to get the best possible performance on my dataset, I’d invest in establishing the human performance baseline on the dataset by running some tests with Mechanical Turk. My guess is that humans would perform better than the algorithms tested so far due to access to external knowledge. Therefore, incorporating external knowledge in the form of manual features or additional data sources may yield the most substantial performance boosts. For example, text on an album cover may contain important clues about its genre, and models pretrained on style datasets may be more suitable than ImageNet models. In addition, it may be beneficial to use a model to detect multiple elements in images where the universe is not restricted to ImageNet classes. This approach was taken by Alexandre Passant, who used Clarifai’s API to tag and classify doom metal and K-pop album covers. Finally, using several different models in an ensemble is likely to help squeeze a bit more accuracy out of the dataset.\nAnother direction that may be worth exploring is using image data for recommendation work. The reason I chose to work on this problem was my exposure to album covers through my work on Bandcamp Recommender – a music recommendation system. It is well-known that visual elements influence the way users interact with recommender systems. This is especially true in Bandcamp Recommender’s case, as users see the album covers before they choose to play them. This leads me to conjecture that considering features that describe the album covers when generating recommendations would increase user interaction with the system. However, it’s hard to tell whether it’d increase the overall relevance of the results. You can’t judge an album by its cover. Or can you…?\nConclusion While I’ve learned a lot from working on this project, there’s still much more to discover. It was especially great to learn some generally-applicable lessons about hyperparameter optimisation and improvements to vanilla gradient descent. Despite the many potential ways of improving performance on my dataset, my next steps in the field would probably include working on problems for which obtaining a good solution is feasible and useful. For example, I have some ideas for applications to marine creature identification.\nFeedback and suggestions are always welcome. Please feel free to contact me privately or via the comments section.\nAcknowledgement: Thanks to Brian Basham and Diogo Moitinho de Almeida for useful tips and discussions.\n","wordCount":"2117","inLanguage":"en","image":"https://yanirs.github.io/yanirseroussi.com/bandcamp-album-covers-by-genre.png","datePublished":"2015-07-06T22:21:42Z","dateModified":"2015-07-06T22:21:42Z","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirs.github.io/yanirseroussi.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://yanirs.github.io/yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Learning about deep learning through album cover classification
</h1>
<div class=post-meta>July 6, 2015&nbsp;·&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2015-07-06-learning-about-deep-learning-through-album-cover-classification/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a>
</div>
</header>
<figure class=entry-cover>
<img loading=lazy srcset="https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_360x0_resize_box_3.png 360w ,https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_480x0_resize_box_3.png 480w ,https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_720x0_resize_box_3.png 720w ,https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_1080x0_resize_box_3.png 1080w ,https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre.png 1259w" sizes="(min-width: 768px) 720px, 100vw" src=https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre.png alt width=1259 height=649>
</figure>
<div class=post-content><p>In the past month, I&rsquo;ve spent some time on <a href=http://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/>my album cover classification project</a>. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.</p>
<h3 id=initial-steps-summary>Initial steps summary<a hidden class=anchor aria-hidden=true href=#initial-steps-summary>#</a></h3>
<p>The following points were discussed in detail in the <a href=http://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/>previous post on this project</a>.</p>
<ul>
<li>The problem I chose to work on is classifying Bandcamp album covers by genre, using a balanced dataset of 10,000 images from 10 different genres.</li>
<li>The experimental code is based on <a href=http://lasagne.readthedocs.org/en/latest/ target=_blank rel=noopener>Lasagne</a>, and is <a href=https://github.com/yanirs/bandcamp-deep-learning/ target=_blank rel=noopener>available on GitHub</a>.</li>
<li>Having set up the environment for running experiments on a GPU, the plan was to get Lasagne&rsquo;s examples working on my dataset, and then iteratively read tutorials/papers/books, implement ideas, play with parameters, and visualise parts of the network until I&rsquo;m satisfied with the results.</li>
</ul>
<h3 id=preliminary-experiments-and-learning-resources>Preliminary experiments and learning resources<a hidden class=anchor aria-hidden=true href=#preliminary-experiments-and-learning-resources>#</a></h3>
<p>I hit several issues when adapting Lasagne&rsquo;s example code to my dataset. The key issue is that the example code is based on the MNIST digits dataset. That dataset&rsquo;s images are 28×28 grayscale, and my dataset&rsquo;s images are 350×350 RGB. This difference led to the training loss quickly diverging when running the example code without any changes. It turns out that simply lowering the learning rate resolves this issue, though the initial results I got were still not much better than random. In general, it appears that everything works on the MNIST digits dataset, so choosing to work on my own dataset made things more challenging (which is a good thing).</p>
<p>The main learning resource I used is the excellent notes for the Stanford course <a href=http://cs231n.github.io/ target=_blank rel=noopener>Convolutional Neural Networks for Visual Recognition</a>. The notes are very clear, contain up-to-date information from recent publications, and include many practical tips for successful training of convolutional networks (convnets). In addition, I read some other tutorials and a few papers. These are summarised in <a href=http://yanirseroussi.com/deep-learning-resources/>a separate page</a>.</p>
<p>The first step after getting the MNIST examples working on my dataset was to extend the code to enable more flexible architectures. My main focus was on vanilla convnets, i.e., networks with several convolutional layers, where each convolutional layer is optionally followed by a max-pooling layer, and the convolutional layers are followed by multiple dense/fully-connected layers and dropout layers. To allow for easy experimentation, the specification of the network can be done from the command line. For example, to train an <a href=http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf target=_blank rel=noopener>AlexNet</a> architecture:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ python manage.py run_experiment <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --dataset-path /path/to/dataset <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --model-architecture ConvNet <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --model-params num_conv_layers<span style=color:#f92672>=</span>5:num_dense_layers<span style=color:#f92672>=</span>2:lc0_num_filters<span style=color:#f92672>=</span>48:lc0_filter_size<span style=color:#f92672>=</span>11:lc0_stride<span style=color:#f92672>=</span>4:lc0_mp<span style=color:#f92672>=</span>True:lm0_pool_size<span style=color:#f92672>=</span>3:lm0_stride<span style=color:#f92672>=</span>2:lc1_num_filters<span style=color:#f92672>=</span>128:lc1_filter_size<span style=color:#f92672>=</span>5:lc1_mp<span style=color:#f92672>=</span>True:lm1_pool_size<span style=color:#f92672>=</span>3:lm1_stride<span style=color:#f92672>=</span>2:lc2_num_filters<span style=color:#f92672>=</span>192:lc2_filter_size<span style=color:#f92672>=</span>3:lc3_num_filters<span style=color:#f92672>=</span>192:lc3_filter_size<span style=color:#f92672>=</span>3:lc4_num_filters<span style=color:#f92672>=</span>128:lc4_filter_size<span style=color:#f92672>=</span>3:lc4_mp<span style=color:#f92672>=</span>True:lm4_pool_size<span style=color:#f92672>=</span>3:lm4_stride<span style=color:#f92672>=</span>2:ld0_num_units<span style=color:#f92672>=</span>2048:ld1_num_units<span style=color:#f92672>=</span><span style=color:#ae81ff>2048</span>
</code></pre></div><p>This can obviously be a bit of a mouthful, so common architectures are also defined in the code with parameters that can be overridden. For instance, to train an AlexNet with 64 filters in the first layer instead of 48:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ python manage.py run_experiment <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  --dataset-path /path/to/dataset <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  --model-architecture AlexNet <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  --model-params lc0_num_filters<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>
</code></pre></div><p>There are many more command line flags (possibly too many), which make it easy to both tinker with various settings, and also run more rigorous experiments. My initial tinkering with convnets didn&rsquo;t yield impressive results in terms of predictive accuracy on my dataset. It turned out that this was partly due to the lack of preprocessing – the less exciting but crucial part of any predictive modelling work.</p>
<h3 id=the-importance-of-preprocessing>The importance of preprocessing<a hidden class=anchor aria-hidden=true href=#the-importance-of-preprocessing>#</a></h3>
<p>My initial focus was on getting things to work on the dataset without worrying too much about preprocessing. I haven&rsquo;t done any image classification work in the past, so I had to learn about the right type of preprocessing to use. I kept it pretty simple and applied the following transformations:</p>
<ul>
<li>Downsampling: all images were scaled down to 256×256. I played briefly with other sizes, but decided on this size to make it easy to use models pretrained on ImageNet.</li>
<li>Cropping & mirroring: during training time, each image was cropped to random 224×224 slices. Deterministic slices were used in test time. In addition, each crop was mirrored horizontally. In most cases I used ten overall crops. Again, these numbers were chosen for comparability with ImageNet-trained models.</li>
<li>Mean subtraction: the training mean of each pixel was subtracted from each instance.</li>
<li>Shuffling: probably the most important preprocessing step. Initially I had the instances sorted by their class, as an artifact of the way the dataset was constructed. Due to the relatively small number of instances the network sees in each batch, this meant that in each epoch, the network first fitted on all the instances from class 1, then all the instances from class 2, etc. This led to very poor performance, which was fixed by shuffling the data once at the start of the training procedure (shuffling every epoch could potentially make things even better).</li>
</ul>
<h3 id=baselines>Baselines<a hidden class=anchor aria-hidden=true href=#baselines>#</a></h3>
<p>After building the experimental environment and a fair bit of tinkering, I decided it was time for some more serious experiments. The results of my initial games were rather disappointing – slightly better than a random baseline, which yields an accuracy score of 10%. Therefore, I ran some baselines to get an idea of what&rsquo;s possible on this dataset.</p>
<p>The first baseline I tried was a random forest with 1,000 trees, which yielded 15.25% accuracy. This baseline was trained directly on the pixel values without any preprocessing other than downsampling. It&rsquo;s worth noting that the downsampling size didn&rsquo;t make much of a difference to this baseline (I tried a few values in the range 50×50-350×350). This baseline was also not particularly sensitive to whether RGB or grayscale values were used to represent the images.</p>
<p>The next experiments were with baselines that utilised pretrained <a href=http://caffe.berkeleyvision.org/ target=_blank rel=noopener>Caffe</a> models. Training a random forest with 1,000 trees on features extracted from the highest fully-connected layer (fc7) in the <a href=http://caffe.berkeleyvision.org/model_zoo.html target=_blank rel=noopener>CaffeNet</a> and <a href=https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md target=_blank rel=noopener>VGGNet-19</a> models yielded accuracies of 16.72% and 16.40% respectively. This was pretty disappointing, as I expected these features to perform much better. The reason may be that album covers are very different from ImageNet images, and the representations in fc7 are too specific to ImageNet. Indeed, when fine-tuning the CaffeNet model (following the procedure outlined <a href=http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html target=_blank rel=noopener>here</a>), I got the best accuracy on the dataset: 22.60%. Using Caffe to train the same network from scratch didn&rsquo;t even get close to this accuracy. However, I didn&rsquo;t try to tune Caffe&rsquo;s learning parameters. Instead, I went back to running experiments with my code.</p>
<p>It&rsquo;s worth noting that the classes identified by the CaffeNet model often have little to do with the actual content of the image. Better baseline results may be obtained by using models that were pretrained on a richer dataset than ImageNet. The following table presents three example covers together with the top-five classes identified by the CaffeNet model for each image. The tags assigned by <a href=http://clarifai.com target=_blank rel=noopener>Clarifai&rsquo;s API</a> are also presented for comparison. From this example, it looks like Clarifai&rsquo;s model is more successful at identifying the correct elements than the CaffeNet model, indicating that a baseline that uses the Clarifai tags may yield competitive performance.</p>
<table>
<thead>
<tr>
<th>Album</th>
<th>CaffeNet</th>
<th>Clarifai</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<figure>
<a href=october-by-wille-p.jpg target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 700px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p_huec3f3336b90e00edb3fbaf0f6322101b_268530_360x0_resize_q75_box.jpg 360w,
https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p_huec3f3336b90e00edb3fbaf0f6322101b_268530_480x0_resize_q75_box.jpg 480w,
https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p.jpg 700w," src=https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p.jpg alt="October by Wille P
hiphop_rap" loading=lazy>
</a><figcaption>
<p><a href=https://emigrant.bandcamp.com/album/october>October by Wille P</a><br><strong>hiphop_rap</strong>
</p>
</figcaption>
</figure>
</td>
<td>digital clock, spotlight, jack-o'-lantern, volcano, traffic light</td>
<td>tree, landscape, sunset, desert, sun, sunrise, nature, evening, sky, travel</td>
</tr>
<tr>
<td>
<figure>
<a href=demo-by-blackrat.jpg target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 700px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat_hud07da48a6ae0f5535eeb701dd098ca1f_138990_360x0_resize_q75_box.jpg 360w,
https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat_hud07da48a6ae0f5535eeb701dd098ca1f_138990_480x0_resize_q75_box.jpg 480w,
https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat.jpg 700w," src=https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat.jpg alt="Demo by Blackrat
metal" loading=lazy>
</a><figcaption>
<p><a href=https://blackrat.bandcamp.com/album/demo>Demo by Blackrat</a><br><strong>metal</strong>
</p>
</figcaption>
</figure>
</td>
<td>spider web, barn spider, chain, bubble, fountain</td>
<td>skull, bone, nobody, death, vector, help, horror, medicine, black and white, tattoo</td>
</tr>
<tr>
<td>
<figure>
<a href=the-kool-aid-album-by-mr-merge.jpg target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 700px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge_hu440cc385fb81a579248ed53d13e0d97e_182230_360x0_resize_q75_box.jpg 360w,
https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge_hu440cc385fb81a579248ed53d13e0d97e_182230_480x0_resize_q75_box.jpg 480w,
https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge.jpg 700w," src=https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge.jpg alt="The Kool-Aid Album by Mr. Merge
soul" loading=lazy>
</a><figcaption>
<p><a href=https://redesignyourmindmuzik.bandcamp.com/album/the-kool-aid-album>The Kool-Aid Album by Mr. Merge</a><br><strong>soul</strong>
</p>
</figcaption>
</figure>
</td>
<td>dishrag, paper towel, honeycomb, envelope, chain mail</td>
<td>symbol, nobody, sign, illustration, color, flag, text, stripes, business, character</td>
</tr>
</tbody>
</table>
<h3 id=training-from-scratch>Training from scratch<a hidden class=anchor aria-hidden=true href=#training-from-scratch>#</a></h3>
<p>My initial experiments were with various convnet architectures, where I manually varied the filter sizes and number of layers to have a reasonable number of parameters and ensure that the model is trainable on a GPU with 4GB of memory. As mentioned, this approach yielded unimpressive results. Following the relative success of the fine-tuned CaffeNet baseline, I decided to run more rigorous experiments on variants of AlexNet (which is very similar to CaffeNet).</p>
<p>Given the large number of hyperparameters that need to be set when training deep convnets, I realised that setting values manually or via grid search is unlikely to yield the best results. To address this, I used <a href=https://github.com/hyperopt/hyperopt target=_blank rel=noopener>hyperopt</a> to search for the best configuration of values. The hyperparameters that were included in the search were the learning method (Nesterov momentum versus Adam with their respective parameters), the learning rate, whether crops are mirrored or not, the number of crops to use (1 or 5), dropout probabilities, the number of hidden units in the fully-connected layers, and the number of filters in each convolutional layer.</p>
<p>Each configuration suggested by hyperopt was trained for 10 epochs, and the promising setups were trained until results stopped improving. The results of the search were rather disappointing, with the best accuracy being 17.19%. However, I learned a lot by finding hyperparameters in this manner – in the past I&rsquo;ve only used a combination of manual settings with grid search.</p>
<p>There are many possible reasons for why the results are so poor. It could be that there&rsquo;s just too little data to train a good classifier, which is supported by the inability to beat the fine-tuned results. This is in line with the results obtained by <a href=http://arxiv.org/pdf/1311.2901v3.pdf target=_blank rel=noopener>Zeiler and Fergus (2013)</a>, who found that convnets pretrained on ImageNet performed much better on the Caltech-101 and Caltech-256 datasets than the same networks trained from scratch. However, it could also be that I just didn&rsquo;t run enough experiments – I definitely feel like I haven&rsquo;t explored everything as well as I&rsquo;d like. In addition, I&rsquo;m still building my intuition for what works and why. I should work more on visualising the way the network learns to uncover more hidden gotchas in addition to those I&rsquo;ve already found. Finally, it could be that it&rsquo;s just too hard to distinguish between covers from the genres I chose for the study.</p>
<h3 id=ideas-for-future-work>Ideas for future work<a hidden class=anchor aria-hidden=true href=#ideas-for-future-work>#</a></h3>
<p>There are many avenues for improving on the work I&rsquo;ve done so far. The code could definitely be made more robust and better tested, optimised and parallelised. It would be worth investing more in hyperparameter and architecture search, including incorporation of ideas from non-vanilla convnets (e.g., <a href=http://arxiv.org/pdf/1409.4842.pdf target=_blank rel=noopener>GoogLeNet</a>). This search should be guided by visualisation and a deeper understanding of the trained networks, which may also come from analysing class-level accuracy (certain genres seem to be easier to distinguish than others). In addition, more sophisticated preprocessing may yield improved results.</p>
<p>If the goal were to get the best possible performance on my dataset, I&rsquo;d invest in establishing the human performance baseline on the dataset by running some tests with Mechanical Turk. My guess is that humans would perform better than the algorithms tested so far due to access to external knowledge. Therefore, incorporating external knowledge in the form of manual features or additional data sources may yield the most substantial performance boosts. For example, text on an album cover may contain important clues about its genre, and models pretrained on style datasets may be more suitable than ImageNet models. In addition, it may be beneficial to use a model to detect multiple elements in images where the universe is not restricted to ImageNet classes. This approach was taken by <a href=http://apassant.net/2015/05/14/album-covers-music-deep-learning/ target=_blank rel=noopener>Alexandre Passant, who used Clarifai&rsquo;s API to tag and classify doom metal and K-pop album covers</a>. Finally, using several different models in an ensemble is likely to help squeeze a bit more accuracy out of the dataset.</p>
<p>Another direction that may be worth exploring is using image data for recommendation work. The reason I chose to work on this problem was my exposure to album covers through my work on <a href=http://www.bcrecommender.com target=_blank rel=noopener>Bandcamp Recommender – a music recommendation system</a>. It is well-known that visual elements influence the way users interact with recommender systems. This is especially true in Bandcamp Recommender&rsquo;s case, as users see the album covers before they choose to play them. This leads me to conjecture that considering features that describe the album covers when generating recommendations would increase user interaction with the system. However, it&rsquo;s hard to tell whether it&rsquo;d increase the overall relevance of the results. You can&rsquo;t judge an album by its cover. Or can you&mldr;?</p>
<h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3>
<p>While I&rsquo;ve learned a lot from working on this project, there&rsquo;s still much more to discover. It was especially great to learn some generally-applicable lessons about hyperparameter optimisation and improvements to vanilla gradient descent. Despite the many potential ways of improving performance on my dataset, my next steps in the field would probably include working on problems for which obtaining a good solution is feasible and useful. For example, I have some ideas for applications to marine creature identification.</p>
<p>Feedback and suggestions are always welcome. Please feel free to <a href=http://yanirseroussi.com/about/>contact me privately</a> or via the comments section.</p>
<p><small><strong>Acknowledgement:</strong> Thanks to Brian Basham and Diogo Moitinho de Almeida for useful tips and discussions.</small></p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/data-science/>data science</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/deep-learning/>deep learning</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/machine-learning/>machine learning</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/predictive-modelling/>predictive modelling</a></li>
</ul>
</footer><section class=comment-section>
<strong>No comments</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirs.github.io%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>
Comment via GitHub issue
</a>
</section>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://yanirs.github.io/yanirseroussi.com/>Yanir Seroussi | Data science and beyond</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>