<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Learning about deep learning through album cover classification | Yanir Seroussi | Data science and beyond</title>
<meta name="keywords" content="data science, deep learning, machine learning, predictive modelling">
<meta name="description" content="Progress on my album cover classification project, highlighting lessons that would be useful to others who are getting started with deep learning.">
<meta name="author" content="Yanir Seroussi">
<link rel="canonical" href="https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/">
<meta name="google-site-verification" content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b86afe5b8c5ad5aeefbb0ec1f0c399ad2bd45f884b225bdee80081a8d52b5dc0.css" integrity="sha256-uGr&#43;W4xa1a7vuw7B8MOZrSvUX4hLIlve6ACBqNUrXcA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://yanirseroussi.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yanirseroussi.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yanirseroussi.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yanirseroussi.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://yanirseroussi.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Learning about deep learning through album cover classification" />
<meta property="og:description" content="Progress on my album cover classification project, highlighting lessons that would be useful to others who are getting started with deep learning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/" />
<meta property="og:image" content="https://yanirseroussi.com/bandcamp-album-covers-by-genre.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2015-07-06T22:21:42+00:00" />
<meta property="article:modified_time" content="2023-07-06T09:28:02+10:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://yanirseroussi.com/bandcamp-album-covers-by-genre.png" />
<meta name="twitter:title" content="Learning about deep learning through album cover classification"/>
<meta name="twitter:description" content="Progress on my album cover classification project, highlighting lessons that would be useful to others who are getting started with deep learning."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yanirseroussi.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Learning about deep learning through album cover classification",
      "item": "https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Learning about deep learning through album cover classification",
  "name": "Learning about deep learning through album cover classification",
  "description": "Progress on my album cover classification project, highlighting lessons that would be useful to others who are getting started with deep learning.",
  "keywords": [
    "data science", "deep learning", "machine learning", "predictive modelling"
  ],
  "articleBody": "In the past month, I’ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.\nInitial steps summary The following points were discussed in detail in the previous post on this project.\nThe problem I chose to work on is classifying Bandcamp album covers by genre, using a balanced dataset of 10,000 images from 10 different genres. The experimental code is based on Lasagne, and is available on GitHub. Having set up the environment for running experiments on a GPU, the plan was to get Lasagne’s examples working on my dataset, and then iteratively read tutorials/papers/books, implement ideas, play with parameters, and visualise parts of the network until I’m satisfied with the results. Preliminary experiments and learning resources I hit several issues when adapting Lasagne’s example code to my dataset. The key issue is that the example code is based on the MNIST digits dataset. That dataset’s images are 28×28 grayscale, and my dataset’s images are 350×350 RGB. This difference led to the training loss quickly diverging when running the example code without any changes. It turns out that simply lowering the learning rate resolves this issue, though the initial results I got were still not much better than random. In general, it appears that everything works on the MNIST digits dataset, so choosing to work on my own dataset made things more challenging (which is a good thing).\nThe main learning resource I used is the excellent notes for the Stanford course Convolutional Neural Networks for Visual Recognition. The notes are very clear, contain up-to-date information from recent publications, and include many practical tips for successful training of convolutional networks (convnets). In addition, I read some other tutorials and a few papers. These are summarised in a separate page.\nThe first step after getting the MNIST examples working on my dataset was to extend the code to enable more flexible architectures. My main focus was on vanilla convnets, i.e., networks with several convolutional layers, where each convolutional layer is optionally followed by a max-pooling layer, and the convolutional layers are followed by multiple dense/fully-connected layers and dropout layers. To allow for easy experimentation, the specification of the network can be done from the command line. For example, to train an AlexNet architecture:\n$ python manage.py run_experiment \\ --dataset-path /path/to/dataset \\ --model-architecture ConvNet \\ --model-params num_conv_layers=5:num_dense_layers=2:lc0_num_filters=48:lc0_filter_size=11:lc0_stride=4:lc0_mp=True:lm0_pool_size=3:lm0_stride=2:lc1_num_filters=128:lc1_filter_size=5:lc1_mp=True:lm1_pool_size=3:lm1_stride=2:lc2_num_filters=192:lc2_filter_size=3:lc3_num_filters=192:lc3_filter_size=3:lc4_num_filters=128:lc4_filter_size=3:lc4_mp=True:lm4_pool_size=3:lm4_stride=2:ld0_num_units=2048:ld1_num_units=2048 This can obviously be a bit of a mouthful, so common architectures are also defined in the code with parameters that can be overridden. For instance, to train an AlexNet with 64 filters in the first layer instead of 48:\n$ python manage.py run_experiment \\ --dataset-path /path/to/dataset \\ --model-architecture AlexNet \\ --model-params lc0_num_filters=64 There are many more command line flags (possibly too many), which make it easy to both tinker with various settings, and also run more rigorous experiments. My initial tinkering with convnets didn’t yield impressive results in terms of predictive accuracy on my dataset. It turned out that this was partly due to the lack of preprocessing – the less exciting but crucial part of any predictive modelling work.\nThe importance of preprocessing My initial focus was on getting things to work on the dataset without worrying too much about preprocessing. I haven’t done any image classification work in the past, so I had to learn about the right type of preprocessing to use. I kept it pretty simple and applied the following transformations:\nDownsampling: all images were scaled down to 256×256. I played briefly with other sizes, but decided on this size to make it easy to use models pretrained on ImageNet. Cropping \u0026 mirroring: during training time, each image was cropped to random 224×224 slices. Deterministic slices were used in test time. In addition, each crop was mirrored horizontally. In most cases I used ten overall crops. Again, these numbers were chosen for comparability with ImageNet-trained models. Mean subtraction: the training mean of each pixel was subtracted from each instance. Shuffling: probably the most important preprocessing step. Initially I had the instances sorted by their class, as an artifact of the way the dataset was constructed. Due to the relatively small number of instances the network sees in each batch, this meant that in each epoch, the network first fitted on all the instances from class 1, then all the instances from class 2, etc. This led to very poor performance, which was fixed by shuffling the data once at the start of the training procedure (shuffling every epoch could potentially make things even better). Baselines After building the experimental environment and a fair bit of tinkering, I decided it was time for some more serious experiments. The results of my initial games were rather disappointing – slightly better than a random baseline, which yields an accuracy score of 10%. Therefore, I ran some baselines to get an idea of what’s possible on this dataset.\nThe first baseline I tried was a random forest with 1,000 trees, which yielded 15.25% accuracy. This baseline was trained directly on the pixel values without any preprocessing other than downsampling. It’s worth noting that the downsampling size didn’t make much of a difference to this baseline (I tried a few values in the range 50×50-350×350). This baseline was also not particularly sensitive to whether RGB or grayscale values were used to represent the images.\nThe next experiments were with baselines that utilised pretrained Caffe models. Training a random forest with 1,000 trees on features extracted from the highest fully-connected layer (fc7) in the CaffeNet and VGGNet-19 models yielded accuracies of 16.72% and 16.40% respectively. This was pretty disappointing, as I expected these features to perform much better. The reason may be that album covers are very different from ImageNet images, and the representations in fc7 are too specific to ImageNet. Indeed, when fine-tuning the CaffeNet model (following the procedure outlined here), I got the best accuracy on the dataset: 22.60%. Using Caffe to train the same network from scratch didn’t even get close to this accuracy. However, I didn’t try to tune Caffe’s learning parameters. Instead, I went back to running experiments with my code.\nIt’s worth noting that the classes identified by the CaffeNet model often have little to do with the actual content of the image. Better baseline results may be obtained by using models that were pretrained on a richer dataset than ImageNet. The following table presents three example covers together with the top-five classes identified by the CaffeNet model for each image. The tags assigned by Clarifai’s API are also presented for comparison. From this example, it looks like Clarifai’s model is more successful at identifying the correct elements than the CaffeNet model, indicating that a baseline that uses the Clarifai tags may yield competitive performance.\nAlbum CaffeNet Clarifai October by Wille P\nhiphop_rap digital clock, spotlight, jack-o’-lantern, volcano, traffic light tree, landscape, sunset, desert, sun, sunrise, nature, evening, sky, travel Demo by Blackrat\nmetal spider web, barn spider, chain, bubble, fountain skull, bone, nobody, death, vector, help, horror, medicine, black and white, tattoo The Kool-Aid Album by Mr. Merge\nsoul dishrag, paper towel, honeycomb, envelope, chain mail symbol, nobody, sign, illustration, color, flag, text, stripes, business, character Training from scratch My initial experiments were with various convnet architectures, where I manually varied the filter sizes and number of layers to have a reasonable number of parameters and ensure that the model is trainable on a GPU with 4GB of memory. As mentioned, this approach yielded unimpressive results. Following the relative success of the fine-tuned CaffeNet baseline, I decided to run more rigorous experiments on variants of AlexNet (which is very similar to CaffeNet).\nGiven the large number of hyperparameters that need to be set when training deep convnets, I realised that setting values manually or via grid search is unlikely to yield the best results. To address this, I used hyperopt to search for the best configuration of values. The hyperparameters that were included in the search were the learning method (Nesterov momentum versus Adam with their respective parameters), the learning rate, whether crops are mirrored or not, the number of crops to use (1 or 5), dropout probabilities, the number of hidden units in the fully-connected layers, and the number of filters in each convolutional layer.\nEach configuration suggested by hyperopt was trained for 10 epochs, and the promising setups were trained until results stopped improving. The results of the search were rather disappointing, with the best accuracy being 17.19%. However, I learned a lot by finding hyperparameters in this manner – in the past I’ve only used a combination of manual settings with grid search.\nThere are many possible reasons for why the results are so poor. It could be that there’s just too little data to train a good classifier, which is supported by the inability to beat the fine-tuned results. This is in line with the results obtained by Zeiler and Fergus (2013), who found that convnets pretrained on ImageNet performed much better on the Caltech-101 and Caltech-256 datasets than the same networks trained from scratch. However, it could also be that I just didn’t run enough experiments – I definitely feel like I haven’t explored everything as well as I’d like. In addition, I’m still building my intuition for what works and why. I should work more on visualising the way the network learns to uncover more hidden gotchas in addition to those I’ve already found. Finally, it could be that it’s just too hard to distinguish between covers from the genres I chose for the study.\nIdeas for future work There are many avenues for improving on the work I’ve done so far. The code could definitely be made more robust and better tested, optimised and parallelised. It would be worth investing more in hyperparameter and architecture search, including incorporation of ideas from non-vanilla convnets (e.g., GoogLeNet). This search should be guided by visualisation and a deeper understanding of the trained networks, which may also come from analysing class-level accuracy (certain genres seem to be easier to distinguish than others). In addition, more sophisticated preprocessing may yield improved results.\nIf the goal were to get the best possible performance on my dataset, I’d invest in establishing the human performance baseline on the dataset by running some tests with Mechanical Turk. My guess is that humans would perform better than the algorithms tested so far due to access to external knowledge. Therefore, incorporating external knowledge in the form of manual features or additional data sources may yield the most substantial performance boosts. For example, text on an album cover may contain important clues about its genre, and models pretrained on style datasets may be more suitable than ImageNet models. In addition, it may be beneficial to use a model to detect multiple elements in images where the universe is not restricted to ImageNet classes. This approach was taken by Alexandre Passant, who used Clarifai’s API to tag and classify doom metal and K-pop album covers. Finally, using several different models in an ensemble is likely to help squeeze a bit more accuracy out of the dataset.\nAnother direction that may be worth exploring is using image data for recommendation work. The reason I chose to work on this problem was my exposure to album covers through my work on Bandcamp Recommender – a music recommendation system. It is well-known that visual elements influence the way users interact with recommender systems. This is especially true in Bandcamp Recommender’s case, as users see the album covers before they choose to play them. This leads me to conjecture that considering features that describe the album covers when generating recommendations would increase user interaction with the system. However, it’s hard to tell whether it’d increase the overall relevance of the results. You can’t judge an album by its cover. Or can you…?\nConclusion While I’ve learned a lot from working on this project, there’s still much more to discover. It was especially great to learn some generally-applicable lessons about hyperparameter optimisation and improvements to vanilla gradient descent. Despite the many potential ways of improving performance on my dataset, my next steps in the field would probably include working on problems for which obtaining a good solution is feasible and useful. For example, I have some ideas for applications to marine creature identification.\nFeedback and suggestions are always welcome. Please feel free to contact me privately or via the comments section.\nAcknowledgement: Thanks to Brian Basham and Diogo Moitinho de Almeida for useful tips and discussions.\n",
  "wordCount" : "2117",
  "inLanguage": "en",
  "image":"https://yanirseroussi.com/bandcamp-album-covers-by-genre.png","datePublished": "2015-07-06T22:21:42Z",
  "dateModified": "2023-07-06T09:28:02+10:00",
  "author":{
    "@type": "Person",
    "name": "Yanir Seroussi"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Yanir Seroussi | Data science and beyond",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yanirseroussi.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yanirseroussi.com/" accesskey="h" title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yanirseroussi.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://yanirseroussi.com/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="https://yanirseroussi.com/faq/" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Learning about deep learning through album cover classification
    </h1>
    <div class="post-meta"><span title='2015-07-06 22:21:42 +0000 UTC'>July 6, 2015</span>&nbsp;|&nbsp;<a href="https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2015-07-06-learning-about-deep-learning-through-album-cover-classification/index.md" rel="noopener noreferrer" target="_blank">Suggest changes</a>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="lazy" srcset="https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_360x0_resize_box_3.png 360w ,https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_480x0_resize_box_3.png 480w ,https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_720x0_resize_box_3.png 720w ,https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre_hube220240ac6ea6d528d49262fd2fcb98_1398155_1080x0_resize_box_3.png 1080w ,https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre.png 1259w" 
            sizes="(min-width: 768px) 720px, 100vw" src="https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/bandcamp-album-covers-by-genre.png" alt="" 
            width="1259" height="649">
        
</figure>
  <div class="post-content"><p>In the past month, I&rsquo;ve spent some time on <a href="https://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/">my album cover classification project</a>. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.</p>
<h3 id="initial-steps-summary">Initial steps summary<a hidden class="anchor" aria-hidden="true" href="#initial-steps-summary">#</a></h3>
<p>The following points were discussed in detail in the <a href="https://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/">previous post on this project</a>.</p>
<ul>
<li>The problem I chose to work on is classifying Bandcamp album covers by genre, using a balanced dataset of 10,000 images from 10 different genres.</li>
<li>The experimental code is based on <a href="http://lasagne.readthedocs.org/en/latest/" target="_blank" rel="noopener">Lasagne</a>, and is <a href="https://github.com/yanirs/bandcamp-deep-learning/" target="_blank" rel="noopener">available on GitHub</a>.</li>
<li>Having set up the environment for running experiments on a GPU, the plan was to get Lasagne&rsquo;s examples working on my dataset, and then iteratively read tutorials/papers/books, implement ideas, play with parameters, and visualise parts of the network until I&rsquo;m satisfied with the results.</li>
</ul>
<h3 id="preliminary-experiments-and-learning-resources">Preliminary experiments and learning resources<a hidden class="anchor" aria-hidden="true" href="#preliminary-experiments-and-learning-resources">#</a></h3>
<p>I hit several issues when adapting Lasagne&rsquo;s example code to my dataset. The key issue is that the example code is based on the MNIST digits dataset. That dataset&rsquo;s images are 28×28 grayscale, and my dataset&rsquo;s images are 350×350 RGB. This difference led to the training loss quickly diverging when running the example code without any changes. It turns out that simply lowering the learning rate resolves this issue, though the initial results I got were still not much better than random. In general, it appears that everything works on the MNIST digits dataset, so choosing to work on my own dataset made things more challenging (which is a good thing).</p>
<p>The main learning resource I used is the excellent notes for the Stanford course <a href="http://cs231n.github.io/" target="_blank" rel="noopener">Convolutional Neural Networks for Visual Recognition</a>. The notes are very clear, contain up-to-date information from recent publications, and include many practical tips for successful training of convolutional networks (convnets). In addition, I read some other tutorials and a few papers. These are summarised in <a href="https://yanirseroussi.com/deep-learning-resources/">a separate page</a>.</p>
<p>The first step after getting the MNIST examples working on my dataset was to extend the code to enable more flexible architectures. My main focus was on vanilla convnets, i.e., networks with several convolutional layers, where each convolutional layer is optionally followed by a max-pooling layer, and the convolutional layers are followed by multiple dense/fully-connected layers and dropout layers. To allow for easy experimentation, the specification of the network can be done from the command line. For example, to train an <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="noopener">AlexNet</a> architecture:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ python manage.py run_experiment <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --dataset-path /path/to/dataset <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --model-architecture ConvNet <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --model-params num_conv_layers<span style="color:#f92672">=</span>5:num_dense_layers<span style="color:#f92672">=</span>2:lc0_num_filters<span style="color:#f92672">=</span>48:lc0_filter_size<span style="color:#f92672">=</span>11:lc0_stride<span style="color:#f92672">=</span>4:lc0_mp<span style="color:#f92672">=</span>True:lm0_pool_size<span style="color:#f92672">=</span>3:lm0_stride<span style="color:#f92672">=</span>2:lc1_num_filters<span style="color:#f92672">=</span>128:lc1_filter_size<span style="color:#f92672">=</span>5:lc1_mp<span style="color:#f92672">=</span>True:lm1_pool_size<span style="color:#f92672">=</span>3:lm1_stride<span style="color:#f92672">=</span>2:lc2_num_filters<span style="color:#f92672">=</span>192:lc2_filter_size<span style="color:#f92672">=</span>3:lc3_num_filters<span style="color:#f92672">=</span>192:lc3_filter_size<span style="color:#f92672">=</span>3:lc4_num_filters<span style="color:#f92672">=</span>128:lc4_filter_size<span style="color:#f92672">=</span>3:lc4_mp<span style="color:#f92672">=</span>True:lm4_pool_size<span style="color:#f92672">=</span>3:lm4_stride<span style="color:#f92672">=</span>2:ld0_num_units<span style="color:#f92672">=</span>2048:ld1_num_units<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>
</span></span></code></pre></div><p>This can obviously be a bit of a mouthful, so common architectures are also defined in the code with parameters that can be overridden. For instance, to train an AlexNet with 64 filters in the first layer instead of 48:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ python manage.py run_experiment <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dataset-path /path/to/dataset <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model-architecture AlexNet <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model-params lc0_num_filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>
</span></span></code></pre></div><p>There are many more command line flags (possibly too many), which make it easy to both tinker with various settings, and also run more rigorous experiments. My initial tinkering with convnets didn&rsquo;t yield impressive results in terms of predictive accuracy on my dataset. It turned out that this was partly due to the lack of preprocessing – the less exciting but crucial part of any predictive modelling work.</p>
<h3 id="the-importance-of-preprocessing">The importance of preprocessing<a hidden class="anchor" aria-hidden="true" href="#the-importance-of-preprocessing">#</a></h3>
<p>My initial focus was on getting things to work on the dataset without worrying too much about preprocessing. I haven&rsquo;t done any image classification work in the past, so I had to learn about the right type of preprocessing to use. I kept it pretty simple and applied the following transformations:</p>
<ul>
<li>Downsampling: all images were scaled down to 256×256. I played briefly with other sizes, but decided on this size to make it easy to use models pretrained on ImageNet.</li>
<li>Cropping &amp; mirroring: during training time, each image was cropped to random 224×224 slices. Deterministic slices were used in test time. In addition, each crop was mirrored horizontally. In most cases I used ten overall crops. Again, these numbers were chosen for comparability with ImageNet-trained models.</li>
<li>Mean subtraction: the training mean of each pixel was subtracted from each instance.</li>
<li>Shuffling: probably the most important preprocessing step. Initially I had the instances sorted by their class, as an artifact of the way the dataset was constructed. Due to the relatively small number of instances the network sees in each batch, this meant that in each epoch, the network first fitted on all the instances from class 1, then all the instances from class 2, etc. This led to very poor performance, which was fixed by shuffling the data once at the start of the training procedure (shuffling every epoch could potentially make things even better).</li>
</ul>
<h3 id="baselines">Baselines<a hidden class="anchor" aria-hidden="true" href="#baselines">#</a></h3>
<p>After building the experimental environment and a fair bit of tinkering, I decided it was time for some more serious experiments. The results of my initial games were rather disappointing – slightly better than a random baseline, which yields an accuracy score of 10%. Therefore, I ran some baselines to get an idea of what&rsquo;s possible on this dataset.</p>
<p>The first baseline I tried was a random forest with 1,000 trees, which yielded 15.25% accuracy. This baseline was trained directly on the pixel values without any preprocessing other than downsampling. It&rsquo;s worth noting that the downsampling size didn&rsquo;t make much of a difference to this baseline (I tried a few values in the range 50×50-350×350). This baseline was also not particularly sensitive to whether RGB or grayscale values were used to represent the images.</p>
<p>The next experiments were with baselines that utilised pretrained <a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe</a> models. Training a random forest with 1,000 trees on features extracted from the highest fully-connected layer (fc7) in the <a href="http://caffe.berkeleyvision.org/model_zoo.html" target="_blank" rel="noopener">CaffeNet</a> and <a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md" target="_blank" rel="noopener">VGGNet-19</a> models yielded accuracies of 16.72% and 16.40% respectively. This was pretty disappointing, as I expected these features to perform much better. The reason may be that album covers are very different from ImageNet images, and the representations in fc7 are too specific to ImageNet. Indeed, when fine-tuning the CaffeNet model (following the procedure outlined <a href="http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html" target="_blank" rel="noopener">here</a>), I got the best accuracy on the dataset: 22.60%. Using Caffe to train the same network from scratch didn&rsquo;t even get close to this accuracy. However, I didn&rsquo;t try to tune Caffe&rsquo;s learning parameters. Instead, I went back to running experiments with my code.</p>
<p>It&rsquo;s worth noting that the classes identified by the CaffeNet model often have little to do with the actual content of the image. Better baseline results may be obtained by using models that were pretrained on a richer dataset than ImageNet. The following table presents three example covers together with the top-five classes identified by the CaffeNet model for each image. The tags assigned by <a href="http://clarifai.com" target="_blank" rel="noopener">Clarifai&rsquo;s API</a> are also presented for comparison. From this example, it looks like Clarifai&rsquo;s model is more successful at identifying the correct elements than the CaffeNet model, indicating that a baseline that uses the Clarifai tags may yield competitive performance.</p>
<table>
<thead>
<tr>
<th>Album</th>
<th>CaffeNet</th>
<th>Clarifai</th>
</tr>
</thead>
<tbody>
<tr>
<td>












  
    
      
    
  
    
      
    
  
    
  
    
  
    
  
  
    
  
  
  



<figure>
  <a href="october-by-wille-p.jpg" target="_blank" rel="noopener">
    <img
      
        sizes="
          (min-width: 768px) 700px,
          100vw
        "
        srcset="
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p_huec3f3336b90e00edb3fbaf0f6322101b_268530_360x0_resize_q75_box.jpg 360w,
            
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p_huec3f3336b90e00edb3fbaf0f6322101b_268530_480x0_resize_q75_box.jpg 480w,
            
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p.jpg 700w,
            
          
        "
        
        
        
          src="https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/october-by-wille-p.jpg"
        
      
        alt="October by Wille P
hiphop_rap"loading="lazy"
    />
  </a><figcaption>
        <p><a href="https://emigrant.bandcamp.com/album/october" target="_blank" rel="noopener">October by Wille P</a><br><strong>hiphop_rap</strong>
          </p>
      </figcaption>
</figure>
</td>
<td>digital clock, spotlight, jack-o&rsquo;-lantern, volcano, traffic light</td>
<td>tree, landscape, sunset, desert, sun, sunrise, nature, evening, sky, travel</td>
</tr>
<tr>
<td>












  
    
      
    
  
    
      
    
  
    
  
    
  
    
  
  
    
  
  
  



<figure>
  <a href="demo-by-blackrat.jpg" target="_blank" rel="noopener">
    <img
      
        sizes="
          (min-width: 768px) 700px,
          100vw
        "
        srcset="
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat_hud07da48a6ae0f5535eeb701dd098ca1f_138990_360x0_resize_q75_box.jpg 360w,
            
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat_hud07da48a6ae0f5535eeb701dd098ca1f_138990_480x0_resize_q75_box.jpg 480w,
            
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat.jpg 700w,
            
          
        "
        
        
        
          src="https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/demo-by-blackrat.jpg"
        
      
        alt="Demo by Blackrat
metal"loading="lazy"
    />
  </a><figcaption>
        <p><a href="https://blackrat.bandcamp.com/album/demo" target="_blank" rel="noopener">Demo by Blackrat</a><br><strong>metal</strong>
          </p>
      </figcaption>
</figure>
</td>
<td>spider web, barn spider, chain, bubble, fountain</td>
<td>skull, bone, nobody, death, vector, help, horror, medicine, black and white, tattoo</td>
</tr>
<tr>
<td>












  
    
      
    
  
    
      
    
  
    
  
    
  
    
  
  
    
  
  
  



<figure>
  <a href="the-kool-aid-album-by-mr-merge.jpg" target="_blank" rel="noopener">
    <img
      
        sizes="
          (min-width: 768px) 700px,
          100vw
        "
        srcset="
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge_hu440cc385fb81a579248ed53d13e0d97e_182230_360x0_resize_q75_box.jpg 360w,
            
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge_hu440cc385fb81a579248ed53d13e0d97e_182230_480x0_resize_q75_box.jpg 480w,
            
          
            
              https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge.jpg 700w,
            
          
        "
        
        
        
          src="https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/the-kool-aid-album-by-mr-merge.jpg"
        
      
        alt="The Kool-Aid Album by Mr. Merge
soul"loading="lazy"
    />
  </a><figcaption>
        <p><a href="https://redesignyourmindmuzik.bandcamp.com/album/the-kool-aid-album" target="_blank" rel="noopener">The Kool-Aid Album by Mr. Merge</a><br><strong>soul</strong>
          </p>
      </figcaption>
</figure>
</td>
<td>dishrag, paper towel, honeycomb, envelope, chain mail</td>
<td>symbol, nobody, sign, illustration, color, flag, text, stripes, business, character</td>
</tr>
</tbody>
</table>
<h3 id="training-from-scratch">Training from scratch<a hidden class="anchor" aria-hidden="true" href="#training-from-scratch">#</a></h3>
<p>My initial experiments were with various convnet architectures, where I manually varied the filter sizes and number of layers to have a reasonable number of parameters and ensure that the model is trainable on a GPU with 4GB of memory. As mentioned, this approach yielded unimpressive results. Following the relative success of the fine-tuned CaffeNet baseline, I decided to run more rigorous experiments on variants of AlexNet (which is very similar to CaffeNet).</p>
<p>Given the large number of hyperparameters that need to be set when training deep convnets, I realised that setting values manually or via grid search is unlikely to yield the best results. To address this, I used <a href="https://github.com/hyperopt/hyperopt" target="_blank" rel="noopener">hyperopt</a> to search for the best configuration of values. The hyperparameters that were included in the search were the learning method (Nesterov momentum versus Adam with their respective parameters), the learning rate, whether crops are mirrored or not, the number of crops to use (1 or 5), dropout probabilities, the number of hidden units in the fully-connected layers, and the number of filters in each convolutional layer.</p>
<p>Each configuration suggested by hyperopt was trained for 10 epochs, and the promising setups were trained until results stopped improving. The results of the search were rather disappointing, with the best accuracy being 17.19%. However, I learned a lot by finding hyperparameters in this manner – in the past I&rsquo;ve only used a combination of manual settings with grid search.</p>
<p>There are many possible reasons for why the results are so poor. It could be that there&rsquo;s just too little data to train a good classifier, which is supported by the inability to beat the fine-tuned results. This is in line with the results obtained by <a href="http://arxiv.org/pdf/1311.2901v3.pdf" target="_blank" rel="noopener">Zeiler and Fergus (2013)</a>, who found that convnets pretrained on ImageNet performed much better on the Caltech-101 and Caltech-256 datasets than the same networks trained from scratch. However, it could also be that I just didn&rsquo;t run enough experiments – I definitely feel like I haven&rsquo;t explored everything as well as I&rsquo;d like. In addition, I&rsquo;m still building my intuition for what works and why. I should work more on visualising the way the network learns to uncover more hidden gotchas in addition to those I&rsquo;ve already found. Finally, it could be that it&rsquo;s just too hard to distinguish between covers from the genres I chose for the study.</p>
<h3 id="ideas-for-future-work">Ideas for future work<a hidden class="anchor" aria-hidden="true" href="#ideas-for-future-work">#</a></h3>
<p>There are many avenues for improving on the work I&rsquo;ve done so far. The code could definitely be made more robust and better tested, optimised and parallelised. It would be worth investing more in hyperparameter and architecture search, including incorporation of ideas from non-vanilla convnets (e.g., <a href="http://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">GoogLeNet</a>). This search should be guided by visualisation and a deeper understanding of the trained networks, which may also come from analysing class-level accuracy (certain genres seem to be easier to distinguish than others). In addition, more sophisticated preprocessing may yield improved results.</p>
<p>If the goal were to get the best possible performance on my dataset, I&rsquo;d invest in establishing the human performance baseline on the dataset by running some tests with Mechanical Turk. My guess is that humans would perform better than the algorithms tested so far due to access to external knowledge. Therefore, incorporating external knowledge in the form of manual features or additional data sources may yield the most substantial performance boosts. For example, text on an album cover may contain important clues about its genre, and models pretrained on style datasets may be more suitable than ImageNet models. In addition, it may be beneficial to use a model to detect multiple elements in images where the universe is not restricted to ImageNet classes. This approach was taken by <a href="http://apassant.net/2015/05/14/album-covers-music-deep-learning/" target="_blank" rel="noopener">Alexandre Passant, who used Clarifai&rsquo;s API to tag and classify doom metal and K-pop album covers</a>. Finally, using several different models in an ensemble is likely to help squeeze a bit more accuracy out of the dataset.</p>
<p>Another direction that may be worth exploring is using image data for recommendation work. The reason I chose to work on this problem was my exposure to album covers through my work on <a href="http://www.bcrecommender.com" target="_blank" rel="noopener">Bandcamp Recommender – a music recommendation system</a>. It is well-known that visual elements influence the way users interact with recommender systems. This is especially true in Bandcamp Recommender&rsquo;s case, as users see the album covers before they choose to play them. This leads me to conjecture that considering features that describe the album covers when generating recommendations would increase user interaction with the system. However, it&rsquo;s hard to tell whether it&rsquo;d increase the overall relevance of the results. You can&rsquo;t judge an album by its cover. Or can you&hellip;?</p>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>While I&rsquo;ve learned a lot from working on this project, there&rsquo;s still much more to discover. It was especially great to learn some generally-applicable lessons about hyperparameter optimisation and improvements to vanilla gradient descent. Despite the many potential ways of improving performance on my dataset, my next steps in the field would probably include working on problems for which obtaining a good solution is feasible and useful. For example, I have some ideas for applications to marine creature identification.</p>
<p>Feedback and suggestions are always welcome. Please feel free to <a href="https://yanirseroussi.com/about/">contact me privately</a> or via the comments section.</p>
<p><small><strong>Acknowledgement:</strong> Thanks to Brian Basham and Diogo Moitinho de Almeida for useful tips and discussions.</small></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://yanirseroussi.com/tags/data-science/">data science</a></li>
      <li><a href="https://yanirseroussi.com/tags/deep-learning/">deep learning</a></li>
      <li><a href="https://yanirseroussi.com/tags/machine-learning/">machine learning</a></li>
      <li><a href="https://yanirseroussi.com/tags/predictive-modelling/">predictive modelling</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Learning about deep learning through album cover classification on twitter"
        href="https://twitter.com/intent/tweet/?text=Learning%20about%20deep%20learning%20through%20album%20cover%20classification&amp;url=https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f&amp;hashtags=datascience%2cdeeplearning%2cmachinelearning%2cpredictivemodelling">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Learning about deep learning through album cover classification on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f&amp;title=Learning%20about%20deep%20learning%20through%20album%20cover%20classification&amp;summary=Learning%20about%20deep%20learning%20through%20album%20cover%20classification&amp;source=https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Learning about deep learning through album cover classification on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f&title=Learning%20about%20deep%20learning%20through%20album%20cover%20classification">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Learning about deep learning through album cover classification on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Learning about deep learning through album cover classification on whatsapp"
        href="https://api.whatsapp.com/send?text=Learning%20about%20deep%20learning%20through%20album%20cover%20classification%20-%20https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Learning about deep learning through album cover classification on telegram"
        href="https://telegram.me/share/url?text=Learning%20about%20deep%20learning%20through%20album%20cover%20classification&amp;url=https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer><section class="comment-section">
  

  <strong>No comments</strong>
  <a
    class="comment-button"
    href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirseroussi.com%2f2015%2f07%2f06%2flearning-about-deep-learning-through-album-cover-classification%2f&body=<!-- Post your comment here and it may get added to the site -->"
    rel="noopener noreferrer"
    target="_blank"
  >
    Comment via GitHub issue
  </a>

  
  
  
  

  
</section>



</article>
    </main>
    
<footer class="footer">
    <span>Text and figures licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="noopener">CC BY-NC-ND 4.0</a> by <a href="https://yanirseroussi.com/about/">Yanir Seroussi</a>, except where noted otherwise  |</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer><div class="mailing-list-container">
  <form
      class="mailing-list"
      action="https://yanirseroussi.us17.list-manage.com/subscribe/post?u=3c08aa3ff27dd92978019febd&amp;id=bc3ab705af"
      method="post"
      target="_blank"
      novalidate
  >
    <label for="mailing-list-email">Get new post notifications</label>
    <input type="text" name="EMAIL" id="mailing-list-email" placeholder="Email address" />
    <div style="position: absolute; left: -5000px;" aria-hidden="true">
      <input type="text" name="b_3c08aa3ff27dd92978019febd_bc3ab705af" tabindex="-1" value="" />
    </div>
    <input type="submit" value="Subscribe" />
  </form>

  <div class="footer">
    Alternatively, <a href="https://github.com/yanirs/yanirseroussi.com" rel="noopener" target="_blank">watch on GitHub</a>
    or <a href="https://yanirseroussi.com/index.xml">subscribe to RSS feed</a>.
  </div>
</div>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
