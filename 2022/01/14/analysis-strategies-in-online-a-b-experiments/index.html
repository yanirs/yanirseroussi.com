<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials | Yanir Seroussi | Data & AI for Startup Impact</title><meta name=keywords content="causal inference,data science,marketing,split testing,statistics"><meta name=description content="Epidemiologists analyse clinical trials to estimate the intention-to-treat and per-protocol effects. This post applies their strategies to online experiments."><meta name=author content="Yanir Seroussi"><link rel=canonical href=https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/><meta name=google-site-verification content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU"><link crossorigin=anonymous href=/assets/css/stylesheet.d9b9873b5f5f328a87bad89f89301b8212fc8c8f1d4421d468cc7148aa2fcc2f.css integrity="sha256-2bmHO19fMoqHutifiTAbghL8jI8dRCHUaMxxSKovzC8=" rel="preload stylesheet" as=style><link rel=icon href=https://yanirseroussi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yanirseroussi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yanirseroussi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yanirseroussi.com/apple-touch-icon.png><link rel=mask-icon href=https://yanirseroussi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/"><meta property="og:site_name" content="Yanir Seroussi | Data & AI for Startup Impact"><meta property="og:title" content="Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials"><meta property="og:description" content="Epidemiologists analyse clinical trials to estimate the intention-to-treat and per-protocol effects. This post applies their strategies to online experiments."><meta property="og:locale" content="en-au"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-14T00:05:40+00:00"><meta property="article:modified_time" content="2024-02-21T11:52:55+10:00"><meta property="article:tag" content="Causal Inference"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="Marketing"><meta property="article:tag" content="Split Testing"><meta property="article:tag" content="Statistics"><meta property="og:image" content="https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment.jpg"><meta name=twitter:title content="Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials"><meta name=twitter:description content="Epidemiologists analyse clinical trials to estimate the intention-to-treat and per-protocol effects. This post applies their strategies to online experiments."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Browse Posts","item":"https://yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials","item":"https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials","name":"Analysis strategies in online A\/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials","description":"Epidemiologists analyse clinical trials to estimate the intention-to-treat and per-protocol effects. This post applies their strategies to online experiments.","keywords":["causal inference","data science","marketing","split testing","statistics"],"articleBody":" In theory, there is no difference between theory and practice. In practice, there is.\nBenjamin Brewster Many discussions of online A/B experiments deal with the sunny day scenario: You randomly assign users to groups A and B, expose group A to the control variant and group B to the treatment variant, run statistical tests on your chosen metrics, and assume that metric differences between the groups that aren’t explained by randomness are due to exposure to the treatment.\nHowever, it’s not always a sunny day for the online experimenter. Challenges include dealing with bot traffic and malicious users, and implementation realities that may make users experience both variants or neither of them. While many of these problems have parallels in clinical trials, I haven’t found many resources that explore these parallels. In this post, I share some lessons I learned from the rich clinical trial literature while building Automattic’s experimentation platform, focusing on analysis strategies that deal with deviations from the ideal experiment scenario.\nReminder: Why we run A/B experiments Uncontrolled versus controlled experiment While the practice of running online A/B experiments is now commonplace, it’s worth reflecting on why such experiments work. Why can’t we just roll out any treatments we think of, measure the metric changes, and assume that differences beyond what we expect from random variation are due to the genius (or folly) of our implemented treatments?\nWell, it’s not that simple because the world isn’t static. Even if we don’t make any changes, we’re likely to see different outcomes from month to month and day to day, as the world and our user population change. This is represented by the top part of the diagram above: While we’re interested in the causal impact of the Treatment on the Outcome, many Unknowns may affect both. That is, without an A/B experiment, the Unknowns act as confounders that make it impossible to estimate the causal effect without further assumptions.\nWith an ideal A/B experiment, we make exposure to the Treatment depend only on our randomisation mechanism – the Assigner on the bottom part of the diagram. Assuming everything goes to plan, we end up with two distinct groups for which exposure to the Treatment is only due to our randomisation mechanism. This allows us to conclude that any differences in the Outcome across the groups beyond what’s expected from randomness are due to the Treatment.\nHowever, reality is often different from this ideal scenario.\nRunning example To make things more concrete, let’s take a simple example: You run a crypto exchange, and you want to maximise signups from one of your landing pages. The current call-to-action text is “sign up”. You’re wondering whether changing it to “sign up today!” would instill a sense of urgency and increase the signup conversion rate (signups divided by unique visitors).\nsign up OR sign up today! A simplified mockup of the variants. Which one would you choose?\nPlacing this scenario into the above diagram, if we were to simply change the text, i.e., apply the Treatment to everyone, we wouldn’t be able to confidently tell whether the text change was the cause of any observed difference in the conversion rate. For example, if our release coincided with a surge of interest in cryptocurrency, this surge may be one of the Unknowns that would cause more motivated users to come to our exchange and sign up. That is, the surge would affect both exposure to the Treatment and the Outcome.\nWhen we run an ideal A/B experiment, we don’t have this problem. Factors like a surge of interest in crypto don’t affect the assignment of users to the control group A (“sign up”) and the treatment group B (“sign up today!”). We can compare the conversion rates across the groups, estimate random variability with our favourite A/B testing calculator, and rejoice. Right?\nWell, not so fast…\nProblems, problems… In the ideal scenario, all the users that were assigned to one of the experiment groups experience their assigned variant and produce a measurable outcome. In our running example, the groups are A: control and B: treatment with a simple exposure of seeing “sign up” for the former and “sign up today!” for the latter. The outcome is a successful signup or an absence of a signup. To make the outcome well-defined, it’s often a good idea to limit outcome measurement to events that happen (or don’t happen) within a reasonable attribution window from exposure or assignment. In our example, a reasonable attribution window is probably on the order of hours, as we don’t expect the call-to-action text to have long-lasting effects.\nPotential deviations from the ideal scenario include:\nAssignment of ineligible users. In our running example, these may be bots or users that already have an account. If we include many ineligible users in our analysis, we may underestimate the effect size even if their distribution across groups is uniform. Crossovers. These are users that manage to experience both variants. For example, they may come across our site on mobile with the “sign up today!” text, and then switch to desktop and see the “sign up” message. Depending on the instrumentation we have in place, we may not be able to detect such users, or we may only detect them if they sign up on one device and then log in on the other device. Assignment without exposure. Due to implementation constraints, we may not be guaranteed that assigned users are actually exposed to the treatment and control. In our running example, it may be that the assignment is done on the backend while exposure happens conditionally and asynchronously on the frontend – some users may bounce in the gap between assignment and exposure, and never see the call-to-action text. Multiple exposures. Once a user has been assigned, they may get exposed to the treatment and control multiple times (without crossing over). In our example, they may visit the landing page repeatedly and see the “sign up” or “sign up today!” text multiple times before deciding to sign up. Epidemiologist jargon and analysis strategies While clinical trials are more tightly controlled than online A/B experiments, they are also susceptible to problems like assignment of ineligible patients and non-adherence to treatment (e.g., crossover, non-exposure, and multiple exposures). Hence, much has been written on addressing these problems at the analysis stage. However, when researching the topic, overcoming the domain-specific language barrier was a bit of a challenge, as the terminology used by online experimenters is different from the terminology used by epidemiologists. Fortunately, I came across the term intention-to-treat at some point, which opened the door to decades of research on the topic.\nTwo papers I found useful are Intention-to-treat concept: A review (Gupta, 2011) and Guidelines for estimating causal effects in pragmatic randomized trials (Murray, Swanson, and Hernán, 2019). Seeing Miguel Hernán on the author list was an especially positive signal for me, as he is responsible for some of my favourite resources on causal inference, including the most practical book I’ve read on the topic.\nThe definitions and guidelines from these two papers provide a solid foundation for thinking about problems of ineligibility and non-adherence. Specifically, Gupta defines intention-to-treat as an analysis strategy “that includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol.”\nThere are often good reasons to exclude some randomised participants from analysis. Depending on the exclusions, this may or may not bias the results. The use of conservative exclusions can be described as modified intention-to-treat, which according to Gupta “allows the exclusion of some randomized subjects in a justified way (such as patients who were deemed ineligible after randomization or certain patients who never started treatment). However, the definition given to the modified ITT (mITT) in randomized controlled trials has been found to be irregular and arbitrary because there is a lack of consistent guidelines for its application. The mITT analysis allows a subjective approach in entry criteria, which may lead to confusion, inaccurate results and bias.”\nExclusions and further adjustments are usually an attempt to estimate the per-protocol effect, which is defined by Murray, Swanson, and Hernán as “the effect of receiving the assigned treatment strategies throughout the follow-up as specified in the study protocol.” Unfortunately, obtaining a valid estimate of the per-protocol effect isn’t trivial: “To validly estimate the per-protocol effect, baseline variables which predict adherence and are prognostic for the outcome need to be accounted for, either through direct adjustment or via an instrumental variable analysis. Yet two commonly used analytic approaches do not incorporate any such adjustment: (1) Naïve per-protocol analysis, that is, restricting the analytic subset to adherent individuals; and (2) As-treated analysis, that is, comparing individuals based on the treatment they choose.” In other words, if we’re not careful, the per-protocol analysis may become analogous to an uncontrolled experiment, as depicted at the top of the diagram above.\nWhat should be done in practice? From my reading of the clinical trial literature, the tendency is to use multiple analysis strategies. For example, the first guideline noted by Murray, Swanson, and Hernán is: “To adequately guide decision making by all stakeholders, report estimates of both the intention-to-treat effect and the per-protocol effect, as well as methods and key conditions underlying the estimation procedures.” This echoes the 1988 US FDA guidelines that require applicants to provide an intention-to-treat analysis in addition to the applicant’s preferred per-protocol analyses. Similarly, the 1998 European Medicines Agency guidelines provide more details on the intention-to-treat, modified intention-to-treat, and per-protocol strategies, stating that: “In general, it is advantageous to demonstrate a lack of sensitivity of the principal trial results to alternative choices of the set of subjects analysed. […] When the full analysis set and the per protocol set lead to essentially the same conclusions, confidence in the trial results is increased, bearing in mind, however, that the need to exclude a substantial proportion of subjects from the per protocol analysis throws some doubt on the overall validity of the trial.”\nWhile the stakes in online experiments are typically much lower than in human drug approval, I believe that applying multiple analysis strategies is still a great idea. We did that for Automattic’s experimentation platform, where we flagged discrepancies between the strategies if they led to conflicting conclusions. One downside of this approach is that it complicates the presentation of results in comparison to using a single strategy. If you face the same challenge, you may draw inspiration from seeing how it’s addressed by the open source frontend of Automattic’s experimentation platform.\nGoing back to our running example, we can perform the following analyses to deal with the deviations noted above:\nIntention-to-treat. Includes all users based on their initial group assignment, regardless of what variant they were exposed to. Modified intention-to-treat: No ineligible users. This applies to cases where we detect the ineligibility after assignment, but the eligibility criteria are based on factors that could have been known before the experiment. Hence, it should be safe to exclude the ineligible users after the fact. In our example, excluding bots and existing users should increase the observed effect size, but not change the preferred variant. Modified intention-to-treat: No crossovers. If we have a mechanism to detect some crossovers, excluding them and comparing the results to the intention-to-treat analysis may uncover implementation bugs. It’s worth noting that crossovers shouldn’t occur in cases where we can uniquely identify users at all stages of the experiment – it is a problem that is more likely to occur when dealing with anonymous users, as in our landing page example. As such, and given the inability to detect all crossovers, A/B experiments should be avoided when users are highly motivated to cross over. For example, displaying different price levels based on anonymous and transient identifiers like cookies is often a bad idea. Naive per-protocol: Exposed users. For this analysis, we’d only include users that were exposed to the control and treatment texts. As noted by Murray, Swanson, and Hernán, this is naive because we should adjust our estimates based on variables that predict exposure. However, if missing exposures are only due to the inherent limitations of online experiments, this falls more under the modified intention-to-treat criterion noted by Gupta, of excluding “patients who never started treatment”. Things get more complicated if we wish to use each exposure as a distinct starting point for measuring multiple assignment windows (the multiple exposures scenario above), which is akin to patients choosing their own dosage – far from a controlled experiment. For automated analysis, it’s better to use the first exposure as the attribution window start, as it should be unaffected by the experiment variants. For all analysis approaches, it’s critical to verify that there is no sample ratio mismatch in the analysed population, i.e., that the distribution of users across variants matches what we expect from a random assignment. If this isn’t the case, manual analysis by a qualified data scientist is needed. The result of this manual analysis may be that the results should be discarded, as sample ratio mismatches are a common indicator of implementation bugs. This is discussed in detail in the book Trustworthy Online Controlled Experiments, which also includes a chapter on exposure-based analysis (called triggering in the book). Among other recommendations, the authors suggest analysing the unexposed users. If everything goes as expected, metrics for the assigned-but-unexposed populations would behave like A/A experiment metrics, i.e., any differences between the groups should be due to random variability.\nHaving rigorous consistency checks in place and falling back to manual analysis when any discrepancies are detected should help avoid the pitfalls of unsafe user exclusions that’d bias the results. Given the need for careful adjustments to get a valid per-protocol estimate in case anything goes wrong, it is often best to fix any underlying issues and rerun the experiment. Usually, this is much cheaper to do in an online setting than in clinical trials.\nClosing thoughts and further reading Once you move from the theory of experimentation to the practice of running experiments in the real world, you discover the many complexities involved in doing it well. This applies whether you’re an epidemiologist or an online experimenter. As noted in the preface to the trustworthy experiments book: “Getting numbers is easy; getting numbers you can trust is hard!”\nThis post only scratched the surface of one area of experimentation: Deciding what population to analyse once the experiment was run. There is, of course, a lot more to online experimentation and causal inference than what I could cover here. But I hope that this message is clear: Approach experimentation with humility, and aim to learn from a broad set of teachers rather than limit yourself to the relatively-recent developments in online experiments.\nAs mentioned above, some resources that are worth reading to learn more include my favourite causal inference book, the trustworthy experiments book, and the guidelines for pragmatic trials. There are also a bunch of resources on my causal inference list, and my post on Bayesian A/B testing should be of interest if you made it to this point. Finally, I’m always happy to discuss these topics, so feel free to contact me or leave a comment with your thoughts.\nCover image by Tumisu from Pixabay\n","wordCount":"2567","inLanguage":"en","image":"https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment.jpg","datePublished":"2022-01-14T00:05:40Z","dateModified":"2024-02-21T11:52:55+10:00","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data \u0026 AI for Startup Impact","logo":{"@type":"ImageObject","url":"https://yanirseroussi.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data & AI for Startup Impact (Alt + H)">Yanir Seroussi | Data & AI for Startup Impact</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button">
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://yanirseroussi.com/about/ title=About><span>About</span></a></li><li><a href=https://yanirseroussi.com/posts/ title=Writing><span>Writing</span></a></li><li><a href=https://yanirseroussi.com/talks/ title=Speaking><span>Speaking</span></a></li><li><a href=https://yanirseroussi.com/consult/ title=Consulting><span>Consulting</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials</h1><div class=post-meta><span title='2022-01-14 00:05:40 +0000 UTC'>January 14, 2022</span></div></header><figure class=entry-cover><img loading=eager srcset='https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment_hu_345fd768e5aaa94.jpg 360w,https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment_hu_b55dd870c9bb7d55.jpg 480w,https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment_hu_532fa28fbd17f867.jpg 720w,https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment_hu_df84c26918ab9bf3.jpg 1080w,https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment_hu_c520517be2cf2a66.jpg 1500w,https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment.jpg 1920w' src=https://yanirseroussi.com/2022/01/14/analysis-strategies-in-online-a-b-experiments/online-drug-experiment.jpg sizes="(min-width: 768px) 720px, 100vw" width=1920 height=1152 alt></figure><div class=post-content><blockquote><p>In theory, there is no difference between theory and practice. In practice, there is.</p><footer><strong>Benjamin Brewster</strong></footer></blockquote><p>Many discussions of online A/B experiments deal with the sunny day scenario: You randomly assign users to groups A and B, expose group A to the control variant and group B to the treatment variant, run statistical tests on your chosen metrics, and assume that metric differences between the groups that aren&rsquo;t explained by randomness are due to exposure to the treatment.</p><p>However, it&rsquo;s not always a sunny day for the online experimenter. Challenges include dealing with bot traffic and malicious users, and implementation realities that may make users experience both variants or neither of them. While many of these problems have parallels in clinical trials, I haven&rsquo;t found many resources that explore these parallels. In this post, I share some lessons I learned from the rich clinical trial literature while building <a href=https://data.blog/category/experimentation-platform/ target=_blank rel=noopener>Automattic&rsquo;s experimentation platform</a>, focusing on analysis strategies that deal with deviations from the ideal experiment scenario.</p><h2 id=reminder-why-we-run-ab-experiments>Reminder: Why we run A/B experiments<a hidden class=anchor aria-hidden=true href=#reminder-why-we-run-ab-experiments>#</a></h2><figure><a href=uncontrolled-versus-controlled-experiment.svg target=_blank rel=noopener><img src=uncontrolled-versus-controlled-experiment.svg alt="Uncontrolled versus controlled experiment" loading=lazy></a><figcaption><p>Uncontrolled versus controlled experiment</p></figcaption></figure><p>While the practice of running online A/B experiments is now commonplace, it&rsquo;s worth reflecting on why such experiments work. Why can&rsquo;t we just roll out any treatments we think of, measure the metric changes, and assume that differences beyond what we expect from random variation are due to the genius (or folly) of our implemented treatments?</p><p>Well, it&rsquo;s not that simple because the world isn&rsquo;t static. Even if we don&rsquo;t make any changes, <a href=https://www.linkedin.com/pulse/how-identify-your-marketing-lies-start-telling-truth-tiberio-caetano/ target=_blank rel=noopener>we&rsquo;re likely to see different outcomes from month to month and day to day</a>, as the world and our user population change. This is represented by the top part of the diagram above: While we&rsquo;re interested in the causal impact of the <code>Treatment</code> on the <code>Outcome</code>, many <code>Unknowns</code> may affect both. That is, without an A/B experiment, the <code>Unknowns</code> act as confounders that make it impossible to estimate the causal effect without <a href=https://yanirseroussi.com/2016/05/15/diving-deeper-into-causality-pearl-kleinberg-hill-and-untested-assumptions/>further assumptions</a>.</p><p>With an ideal A/B experiment, we make exposure to the <code>Treatment</code> depend only on our randomisation mechanism – the <code>Assigner</code> on the bottom part of the diagram. Assuming everything goes to plan, we end up with two distinct groups for which exposure to the <code>Treatment</code> is only due to our randomisation mechanism. This allows us to conclude that any differences in the <code>Outcome</code> across the groups beyond what&rsquo;s expected from randomness are due to the <code>Treatment</code>.</p><p>However, reality is often different from this ideal scenario.</p><h2 id=running-example>Running example<a hidden class=anchor aria-hidden=true href=#running-example>#</a></h2><p>To make things more concrete, let&rsquo;s take a simple example: You run a crypto exchange, and you want to maximise signups from one of your landing pages. The current call-to-action text is <em>&ldquo;sign up&rdquo;</em>. You&rsquo;re wondering whether changing it to <em>&ldquo;sign up today!&rdquo;</em> would instill a sense of urgency and increase the signup conversion rate (signups divided by unique visitors).</p><figure><a class=comment-button href=# onclick='alert("This is variant A: control")' style=float:unset>sign up</a>
<small>OR</small>
<a class=comment-button href=# onclick='alert("This is variant B: treatment")' style=float:unset>sign up today!</a><figcaption><p>A simplified mockup of the variants. Which one would you choose?</p></figcaption></figure><p>Placing this scenario into the above diagram, if we were to simply change the text, i.e., apply the <code>Treatment</code> to everyone, we wouldn&rsquo;t be able to confidently tell whether the text change was the <em>cause</em> of any observed difference in the conversion rate. For example, if our release coincided with a surge of interest in cryptocurrency, this surge may be one of the <code>Unknowns</code> that would cause more motivated users to come to our exchange and sign up. That is, the surge would affect both exposure to the <code>Treatment</code> and the <code>Outcome</code>.</p><p>When we run an ideal A/B experiment, we don&rsquo;t have this problem. Factors like a surge of interest in crypto don&rsquo;t affect the assignment of users to the control group A (<em>&ldquo;sign up&rdquo;</em>) and the treatment group B (<em>&ldquo;sign up today!&rdquo;</em>). We can compare the conversion rates across the groups, estimate random variability with <a href=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/>our favourite A/B testing calculator</a>, and rejoice. Right?</p><p>Well, not so fast&mldr;</p><h2 id=problems-problems>Problems, problems&mldr;<a hidden class=anchor aria-hidden=true href=#problems-problems>#</a></h2><p>In the ideal scenario, all the users that were assigned to one of the experiment groups experience their assigned variant and produce a measurable outcome. In our running example, the groups are <code>A: control</code> and <code>B: treatment</code> with a simple exposure of seeing <em>&ldquo;sign up&rdquo;</em> for the former and <em>&ldquo;sign up today!&rdquo;</em> for the latter. The outcome is a successful signup or an absence of a signup. To make the outcome well-defined, it&rsquo;s often a good idea to limit outcome measurement to events that happen (or don&rsquo;t happen) within a reasonable <em>attribution window</em> from exposure or assignment. In our example, a reasonable attribution window is probably on the order of hours, as we don&rsquo;t expect the call-to-action text to have long-lasting effects.</p><p>Potential deviations from the ideal scenario include:</p><ul><li><strong>Assignment of ineligible users.</strong> In our running example, these may be bots or users that already have an account. If we include many ineligible users in our analysis, we may underestimate the effect size even if their distribution across groups is uniform.</li><li><strong>Crossovers.</strong> These are users that manage to experience both variants. For example, they may come across our site on mobile with the <em>&ldquo;sign up today!&rdquo;</em> text, and then switch to desktop and see the <em>&ldquo;sign up&rdquo;</em> message. Depending on the instrumentation we have in place, we may not be able to detect such users, or we may only detect them if they sign up on one device and then log in on the other device.</li><li><strong>Assignment without exposure.</strong> Due to implementation constraints, we may not be guaranteed that assigned users are actually exposed to the treatment and control. In our running example, it may be that the assignment is done on the backend while exposure happens conditionally and asynchronously on the frontend – some users may bounce in the gap between assignment and exposure, and never see the call-to-action text.</li><li><strong>Multiple exposures.</strong> Once a user has been assigned, they may get exposed to the treatment and control multiple times (without crossing over). In our example, they may visit the landing page repeatedly and see the <em>&ldquo;sign up&rdquo;</em> or <em>&ldquo;sign up today!&rdquo;</em> text multiple times before deciding to sign up.</li></ul><h2 id=epidemiologist-jargon-and-analysis-strategies>Epidemiologist jargon and analysis strategies<a hidden class=anchor aria-hidden=true href=#epidemiologist-jargon-and-analysis-strategies>#</a></h2><p>While clinical trials are more tightly controlled than online A/B experiments, they are also susceptible to problems like assignment of ineligible patients and non-adherence to treatment (e.g., crossover, non-exposure, and multiple exposures). Hence, much has been written on addressing these problems at the analysis stage. However, when researching the topic, overcoming the domain-specific language barrier was a bit of a challenge, as the terminology used by online experimenters is different from the terminology used by epidemiologists. Fortunately, I came across the term <em>intention-to-treat</em> at some point, which opened the door to decades of research on the topic.</p><p>Two papers I found useful are <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3159210/ target=_blank rel=noopener><em>Intention-to-treat concept: A review</em></a> (Gupta, 2011) and <a href=https://arxiv.org/abs/1911.06030 target=_blank rel=noopener><em>Guidelines for estimating causal effects in pragmatic randomized trials</em></a> (Murray, Swanson, and Hernán, 2019). Seeing <a href=https://www.hsph.harvard.edu/miguel-hernan/ target=_blank rel=noopener>Miguel Hernán</a> on the author list was an especially positive signal for me, as he is responsible for <a href=https://yanirseroussi.com/causal-inference-resources/>some of my favourite resources on causal inference</a>, including <a href=https://yanirseroussi.com/2018/12/24/the-most-practical-causal-inference-book-ive-read-is-still-a-draft/>the most practical book I&rsquo;ve read on the topic</a>.</p><p>The definitions and guidelines from these two papers provide a solid foundation for thinking about problems of ineligibility and non-adherence. Specifically, Gupta defines intention-to-treat as an analysis strategy <em>&ldquo;that includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol.&rdquo;</em></p><p>There are often good reasons to exclude some randomised participants from analysis. Depending on the exclusions, this may or may not bias the results. The use of conservative exclusions can be described as modified intention-to-treat, which according to Gupta <em>&ldquo;allows the exclusion of some randomized subjects in a justified way (such as patients who were deemed ineligible after randomization or certain patients who never started treatment). However, the definition given to the modified ITT (mITT) in randomized controlled trials has been found to be irregular and arbitrary because there is a lack of consistent guidelines for its application. The mITT analysis allows a subjective approach in entry criteria, which may lead to confusion, inaccurate results and bias.&rdquo;</em></p><p>Exclusions and further adjustments are usually an attempt to estimate the per-protocol effect, which is defined by Murray, Swanson, and Hernán as <em>&ldquo;the effect of receiving the assigned treatment strategies throughout the follow-up as specified in the study protocol.&rdquo;</em> Unfortunately, obtaining a valid estimate of the per-protocol effect isn&rsquo;t trivial: <em>&ldquo;To validly estimate the per-protocol effect, baseline variables which predict adherence and are prognostic for the outcome need to be accounted for, either through direct adjustment or via an instrumental variable analysis. Yet two commonly used analytic approaches do not incorporate any such adjustment: (1) Naïve per-protocol analysis, that is, restricting the analytic subset to adherent individuals; and (2) As-treated analysis, that is, comparing individuals based on the treatment they choose.&rdquo;</em> In other words, if we&rsquo;re not careful, the per-protocol analysis may become analogous to an uncontrolled experiment, as depicted at the top of the diagram above.</p><h2 id=what-should-be-done-in-practice>What should be done in practice?<a hidden class=anchor aria-hidden=true href=#what-should-be-done-in-practice>#</a></h2><p>From my reading of the clinical trial literature, the tendency is to use multiple analysis strategies. For example, the first guideline noted by Murray, Swanson, and Hernán is: <em>&ldquo;To adequately guide decision making by all stakeholders, report estimates of both the intention-to-treat effect and the per-protocol effect, as well as methods and key conditions underlying the estimation procedures.&rdquo;</em> This echoes <a href=https://www.fda.gov/regulatory-information/search-fda-guidance-documents/format-and-content-clinical-and-statistical-sections-application target=_blank rel=noopener>the 1988 US FDA guidelines</a> that require applicants to provide an intention-to-treat analysis in addition to the applicant&rsquo;s preferred per-protocol analyses. Similarly, <a href=https://www.ema.europa.eu/en/documents/scientific-guideline/ich-e-9-statistical-principles-clinical-trials-step-5_en.pdf target=_blank rel=noopener>the 1998 European Medicines Agency guidelines</a> provide more details on the intention-to-treat, modified intention-to-treat, and per-protocol strategies, stating that: <em>&ldquo;In general, it is advantageous to demonstrate a lack of sensitivity of the principal trial results to alternative choices of the set of subjects analysed. [&mldr;] When the full analysis set and the per protocol set lead to essentially the same conclusions, confidence in the trial results is increased, bearing in mind, however, that the need to exclude a substantial proportion of subjects from the per protocol analysis throws some doubt on the overall validity of the trial.&rdquo;</em></p><p>While the stakes in online experiments are typically much lower than in human drug approval, I believe that applying multiple analysis strategies is still a great idea. We did that for Automattic&rsquo;s experimentation platform, where we flagged discrepancies between the strategies if they led to conflicting conclusions. One downside of this approach is that it complicates the presentation of results in comparison to using a single strategy. If you face the same challenge, you may draw inspiration from seeing how it&rsquo;s addressed by the <a href=https://github.com/Automattic/abacus target=_blank rel=noopener>open source frontend of Automattic&rsquo;s experimentation platform</a>.</p><p>Going back to our running example, we can perform the following analyses to deal with the deviations noted above:</p><ul><li><strong>Intention-to-treat.</strong> Includes all users based on their initial group assignment, regardless of what variant they were exposed to.</li><li><strong>Modified intention-to-treat: No ineligible users.</strong> This applies to cases where we detect the ineligibility after assignment, but the eligibility criteria are based on factors that could have been known before the experiment. Hence, it <em>should</em> be safe to exclude the ineligible users after the fact. In our example, excluding bots and existing users should increase the observed effect size, but not change the preferred variant.</li><li><strong>Modified intention-to-treat: No crossovers.</strong> If we have a mechanism to detect <em>some</em> crossovers, excluding them and comparing the results to the intention-to-treat analysis may uncover implementation bugs. It&rsquo;s worth noting that crossovers shouldn&rsquo;t occur in cases where we can uniquely identify users at all stages of the experiment – it is a problem that is more likely to occur when dealing with anonymous users, as in our landing page example. As such, and given the inability to detect all crossovers, A/B experiments should be avoided when users are highly motivated to cross over. For example, displaying different price levels based on anonymous and transient identifiers like cookies is often a bad idea.</li><li><strong>Naive per-protocol: Exposed users.</strong> For this analysis, we&rsquo;d only include users that were exposed to the control and treatment texts. As noted by Murray, Swanson, and Hernán, this is naive because we <em>should</em> adjust our estimates based on variables that predict exposure. However, if missing exposures are only due to the inherent limitations of online experiments, this falls more under the modified intention-to-treat criterion noted by Gupta, of excluding <em>&ldquo;patients who never started treatment&rdquo;</em>. Things get more complicated if we wish to use each exposure as a distinct starting point for measuring multiple assignment windows (the <em>multiple exposures</em> scenario above), which is akin to patients choosing their own dosage – far from a controlled experiment. For automated analysis, it&rsquo;s better to use the first exposure as the attribution window start, as it should be unaffected by the experiment variants.</li></ul><p>For all analysis approaches, it&rsquo;s critical to verify that there is no <em>sample ratio mismatch</em> in the analysed population, i.e., that the distribution of users across variants matches what we expect from a random assignment. If this isn&rsquo;t the case, manual analysis by a qualified data scientist is needed. The result of this manual analysis may be that the results should be discarded, as sample ratio mismatches are a common indicator of implementation bugs. This is discussed in detail in the book <a href=https://experimentguide.com/ target=_blank rel=noopener><em>Trustworthy Online Controlled Experiments</em></a>, which also includes a chapter on exposure-based analysis (called <em>triggering</em> in the book). Among other recommendations, the authors suggest analysing the <em>unexposed</em> users. If everything goes as expected, metrics for the assigned-but-unexposed populations would behave like A/A experiment metrics, i.e., any differences between the groups should be due to random variability.</p><p>Having rigorous consistency checks in place and falling back to manual analysis when any discrepancies are detected should help avoid the pitfalls of unsafe user exclusions that&rsquo;d bias the results. Given the need for careful adjustments to get a valid per-protocol estimate in case anything goes wrong, it is often best to fix any underlying issues and rerun the experiment. Usually, this is much cheaper to do in an online setting than in clinical trials.</p><h2 id=closing-thoughts-and-further-reading>Closing thoughts and further reading<a hidden class=anchor aria-hidden=true href=#closing-thoughts-and-further-reading>#</a></h2><p>Once you move from the theory of experimentation to the practice of running experiments in the real world, you discover the many complexities involved in doing it well. This applies whether you&rsquo;re an epidemiologist or an online experimenter. As noted in the preface to the trustworthy experiments book: <em>&ldquo;Getting numbers is easy; getting numbers you can trust is hard!&rdquo;</em></p><p>This post only scratched the surface of one area of experimentation: Deciding what population to analyse once the experiment was run. There is, of course, a lot more to online experimentation and causal inference than what I could cover here. But I hope that this message is clear: <strong>Approach experimentation with humility, and aim to learn from a broad set of teachers rather than limit yourself to the relatively-recent developments in online experiments.</strong></p><p>As mentioned above, some resources that are worth reading to learn more include <a href=https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/ target=_blank rel=noopener>my favourite causal inference book</a>, <a href=https://experimentguide.com/ target=_blank rel=noopener>the trustworthy experiments book</a>, and <a href=https://arxiv.org/abs/1911.06030 target=_blank rel=noopener>the guidelines for pragmatic trials</a>. There are also a bunch of resources on <a href=https://yanirseroussi.com/causal-inference-resources/>my causal inference list</a>, and <a href=https://yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/>my post on Bayesian A/B testing</a> should be of interest if you made it to this point. Finally, I&rsquo;m always happy to discuss these topics, so feel free to <a href=https://yanirseroussi.com/about/>contact me</a> or leave a comment with your thoughts.</p><hr><p><small>Cover image by <a href=https://pixabay.com/photos/online-pharmacy-pills-click-3962209/ target=_blank rel=noopener>Tumisu from Pixabay</a></small></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yanirseroussi.com/tags/causal-inference/>Causal Inference</a></li><li><a href=https://yanirseroussi.com/tags/data-science/>Data Science</a></li><li><a href=https://yanirseroussi.com/tags/marketing/>Marketing</a></li><li><a href=https://yanirseroussi.com/tags/split-testing/>Split Testing</a></li><li><a href=https://yanirseroussi.com/tags/statistics/>Statistics</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials on x" href="https://x.com/intent/tweet/?text=Analysis%20strategies%20in%20online%20A%2fB%20experiments%3a%20Intention-to-treat%2c%20per-protocol%2c%20and%20other%20lessons%20from%20clinical%20trials&amp;url=https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f&amp;hashtags=causalinference%2cdatascience%2cmarketing%2csplittesting%2cstatistics"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f&amp;title=Analysis%20strategies%20in%20online%20A%2fB%20experiments%3a%20Intention-to-treat%2c%20per-protocol%2c%20and%20other%20lessons%20from%20clinical%20trials&amp;summary=Analysis%20strategies%20in%20online%20A%2fB%20experiments%3a%20Intention-to-treat%2c%20per-protocol%2c%20and%20other%20lessons%20from%20clinical%20trials&amp;source=https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f&title=Analysis%20strategies%20in%20online%20A%2fB%20experiments%3a%20Intention-to-treat%2c%20per-protocol%2c%20and%20other%20lessons%20from%20clinical%20trials"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials on whatsapp" href="https://api.whatsapp.com/send?text=Analysis%20strategies%20in%20online%20A%2fB%20experiments%3a%20Intention-to-treat%2c%20per-protocol%2c%20and%20other%20lessons%20from%20clinical%20trials%20-%20https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials on telegram" href="https://telegram.me/share/url?text=Analysis%20strategies%20in%20online%20A%2fB%20experiments%3a%20Intention-to-treat%2c%20per-protocol%2c%20and%20other%20lessons%20from%20clinical%20trials&amp;url=https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Analysis strategies in online A/B experiments: Intention-to-treat, per-protocol, and other lessons from clinical trials on ycombinator" href="https://news.ycombinator.com/submitlink?t=Analysis%20strategies%20in%20online%20A%2fB%20experiments%3a%20Intention-to-treat%2c%20per-protocol%2c%20and%20other%20lessons%20from%20clinical%20trials&u=https%3a%2f%2fyanirseroussi.com%2f2022%2f01%2f14%2fanalysis-strategies-in-online-a-b-experiments%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><a href=/contact/#mailing-list-email target=_blank aria-label="subscribe to mailing list" class=mailing-list-link id=mailing-list-link>Subscribe
</a><script>const mailingListButton=document.getElementById("mailing-list-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mailingListButton.style.visibility="visible",mailingListButton.style.opacity="1"):(mailingListButton.style.visibility="hidden",mailingListButton.style.opacity="0")}</script><div class=mailing-list-container><script src=https://f.convertkit.com/ckjs/ck.5.js></script><form class="mailing-list seva-form formkit-form" action=https://app.convertkit.com/forms/6549537/subscriptions method=post data-sv-form=6549537 data-uid=9157759fce data-format=inline data-version=5 data-options='{"settings":{"after_subscribe":{"action":"message","redirect_url":"","success_message":"Success! Now check your email to confirm your subscription."},"recaptcha":{"enabled":false},"return_visitor":{"action":"show","custom_content":""}},"version":"5"}'><div data-style=clean><ul class="formkit-alert formkit-alert-error" data-element=errors data-group=alert></ul><div data-element=fields data-stacked=false><label for=mailing-list-email>Get new posts in your mailbox</label>
<input id=mailing-list-email name=email_address aria-label="Email address" placeholder="Email address" required type=email>
<button data-element=submit>Subscribe</button></div></div></form><div class=footer>Join hundreds of subscribers. No spam or AI-generated slop. Unsubscribe any time.</div></div><section class=comment-section><p class="post-content contact-cta">Public comments are closed, but I love hearing from readers. Feel free to
<a href=/contact/ target=_blank>contact me</a> with your thoughts.</p></section><p class="post-content data-webring">This site is a part of the <a href=https://randyau.github.io/datawebring/index.html target=_blank rel=noopener>Data People Writing Stuff</a> webring.<br><a class=data-webring-previous-link target=_blank rel=noopener>← previous site</a>
&nbsp; | &nbsp;
<a class=data-webring-next-link target=_blank rel=noopener>next site →</a></p><script>function populateDataWebringLinks(){const e=["https://www.randyau.com/","https://vickiboykis.com/","https://www.counting-stuff.com/","https://gecky.me/","https://qethanm.cc/datawebring/","https://mlops.systems/","https://e2eml.school/","https://blog.harterrt.com/","https://www.jessemostipak.com/","https://elliotgunn.github.io/","https://radbrt.com","https://simon.podhajsky.net/blog/","https://www.heltweg.org/","https://emilyriederer.com/","https://kylestratis.com","https://www.eamoncaddigan.net/","https://karnwong.me/","https://aino-spring.com/"];function t(e){let n,s,t;for(t=e.length-1;t>0;t--)n=Math.floor(Math.random()*(t+1)),s=e[t],e[t]=e[n],e[n]=s}t(e),document.querySelector(".data-webring-previous-link").href=e[0],document.querySelector(".data-webring-next-link").href=e[1]}populateDataWebringLinks()</script></article></main><div class=global-footer><div class=footer><span>Text and figures licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank rel=noopener>CC BY-NC-ND 4.0</a> by <a href=https://yanirseroussi.com/about/>Yanir Seroussi</a>, except where noted otherwise&nbsp;&nbsp;|</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></div></div><script>const menuTrigger=document.querySelector("#menu-trigger"),menuElem=document.querySelector(".menu");menuTrigger.addEventListener("click",function(){menuElem.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){menuTrigger.contains(e.target)||menuElem.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>