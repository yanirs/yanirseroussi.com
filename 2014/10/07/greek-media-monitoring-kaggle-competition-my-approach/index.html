<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Greek Media Monitoring Kaggle competition: My approach | Yanir Seroussi | Data science and beyond</title>
<meta name=keywords content="data science,kaggle,kaggle competition,multi-label classification,predictive modelling">
<meta name=description content="A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.
Data & evaluation The data consists of articles scanned from Greek print media in May-September 2013.">
<meta name=author content="Yanir Seroussi">
<link rel=canonical href=https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/>
<link crossorigin=anonymous href=/yanirseroussi.com/assets/css/stylesheet.min.7603165cb47dcda1f46a839ca379731b2f33098c043e75f680940e69a5d546a8.css integrity="sha256-dgMWXLR9zaH0aoOco3lzGy8zCYwEPnX2gJQOaaXVRqg=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/yanirseroussi.com/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirs.github.io/yanirseroussi.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://yanirs.github.io/yanirseroussi.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://yanirs.github.io/yanirseroussi.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://yanirs.github.io/yanirseroussi.com/apple-touch-icon.png>
<link rel=mask-icon href=https://yanirs.github.io/yanirseroussi.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.89.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Greek Media Monitoring Kaggle competition: My approach">
<meta property="og:description" content="A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.
Data & evaluation The data consists of articles scanned from Greek print media in May-September 2013.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/">
<meta property="og:image" content="https://yanirs.github.io/yanirseroussi.com/wise2014-connected-components.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2014-10-07T03:21:35+00:00">
<meta property="article:modified_time" content="2014-10-07T03:21:35+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://yanirs.github.io/yanirseroussi.com/wise2014-connected-components.png">
<meta name=twitter:title content="Greek Media Monitoring Kaggle competition: My approach">
<meta name=twitter:description content="A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.
Data & evaluation The data consists of articles scanned from Greek print media in May-September 2013.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirs.github.io/yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Greek Media Monitoring Kaggle competition: My approach","item":"https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Greek Media Monitoring Kaggle competition: My approach","name":"Greek Media Monitoring Kaggle competition: My approach","description":"A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.\nData \u0026amp; evaluation The data consists of articles scanned from Greek print media in May-September 2013.","keywords":["data science","kaggle","kaggle competition","multi-label classification","predictive modelling"],"articleBody":"A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.\nData \u0026 evaluation The data consists of articles scanned from Greek print media in May-September 2013. Due to copyright issues, the organisers didn‚Äôt make the original articles available ‚Äì competitors only had access to normalised tf-idf representations of the texts. This limited the options for doing feature engineering and made it impossible to consider things like word order, but it made things somewhat simpler as the focus was on modelling due to inability to extract interesting features.\nOverall, there are about 65K texts in the training set and 35K in the test set, where the split is based on chronological ordering (i.e., the training articles were published before the test articles). Each article was manually labelled with one or more labels out of a set of 203 labels. For each test article, the goal is to infer its set of labels. Submissions were ranked using the mean F1 score.\nDespite being manually annotated, the data isn‚Äôt very clean. Issues include identical texts that have different labels, empty articles, and articles with very few words. For example, the training set includes ten ‚Äúarticles‚Äù with a single word. Five of these articles have the word 68839, but each of these five was given a different label. Such issues are not unusual in Kaggle competitions or in real life, but they do limit the general usefulness of the results since any model built on this data would fit some noise.\nLocal validation setup As mentioned in previous posts (How to (almost) win Kaggle competitions and Kaggle beginner tips) having a solid local validation setup is very important. It ensures you don‚Äôt waste time on weak submissions, increases confidence in the models, and avoids leaking information about how well you‚Äôre doing.\nI used the first 35K training texts for local training and the following 30K texts for validation. While the article publication dates weren‚Äôt provided, I hoped that this would mimic the competition setup, where the test dataset consists of articles that were published after the articles in the training dataset. This seemed to work, as my local results were consistent with the leaderboard results. I‚Äôm pleased to report that this setup allowed me to have the lowest number of submissions of all the top-10 teams üôÇ\nThings that worked I originally wanted to use this competition to play with deep learning through Python packages such as Theano and PyLearn2. However, as this was the first time I worked on a multilabel classification problem, I got sucked into reading a lot of papers on the topic and never got around to doing deep learning. Maybe next time‚Ä¶\nOne of my key discoveries was that there if you define a graph where the vertices are labels and there‚Äôs an edge between two labels if they appear together in a document‚Äôs label set, then there are two main connected components of labels and several small ones with single labels (see figure below). It is possible to train a linear classifier that distinguishes between the components with very high accuracy (over 99%). This allowed me to improve performance by training different classifiers on each connected component.\n   My best submission ended up being a simple weighted linear combination of three models. All these models are hierarchical ensembles, where a linear classifier distinguishes between connected components, and the base models are trained on texts from a single connected component. These base models are:\n Ensemble of classifier chains (ECC) with linear classifiers (SGDClassifier from scikit-learn) trained for each label, using hinge loss and L1 penalty Same as 1, but with modified Huber loss A linear classifier with modified Huber loss and L1 penalty that predicts single label probabilities  For each test document, each one of these base models yields a score for each label. These scores are weighted and thresholded to yield the final predictions.\nIt was interesting to learn that a relatively-simple model like ECC yields competitive results. The basic idea behind ECC is to combine different classifier chains. Each classifier chain is also an ensemble where each base classifier is trained to predict a single label. The input for each classifier in the chain depends on the output of preceding classifiers, so it encodes dependencies between labels. For example, if label 2 always appears with label 1 and the label 1 classifier precedes the label 2 classifier in the chain, the label 2 classifier is able to use this dependency information directly, which should increase its accuracy (though it is affected by misclassifications by the label 1 classifier). See Read et al.‚Äôs paper for a more in-depth explanation.\nAnother notable observation is that L1 penalty worked well, which is not too surprising when considering the fact that the dataset has 300K features and many of them are probably irrelevant to prediction (L1 penalty yields sparse models where many features get zero weight).\nThings that didn‚Äôt work As I was travelling, I didn‚Äôt have much time to work on this competition over its two final weeks (though this was a good way of passing the time on long flights). One thing that I tried was understanding some of the probabilistic classifier chain (PCC) code out there by porting it to Python, but the results were very disappointing, probably due to bugs in my code. I expected PCC to work well, especially with the extension for optimising the F-measure. Figuring out how to run the Java code would have probably been a better use of my time than porting the code to Python.\nI also played with reverse-engineering the features back to counts, but it was problematic since the feature values are normalised. It was disappointing that we weren‚Äôt at least given the bag of words representations. I also attempted to reduce the feature representation with latent Dirichlet allocation, but it didn‚Äôt perform well ‚Äì possibly because I couldn‚Äôt get the correct word counts.\nConclusion Overall, this was a fun competition. Despite minor issues with the data and not having enough time to do everything I wanted to do, it was a great learning experience. From reading the summaries by the other teams, it appears that other competitors enjoyed it too. As always, I highly recommend Kaggle competitions to beginners who are trying to learn more about the field of data science and predictive modelling, and to more experienced data scientists who want to improve their skills.\n","wordCount":"1114","inLanguage":"en","image":"https://yanirs.github.io/yanirseroussi.com/wise2014-connected-components.png","datePublished":"2014-10-07T03:21:35Z","dateModified":"2014-10-07T03:21:35Z","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirs.github.io/yanirseroussi.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://yanirs.github.io/yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Greek Media Monitoring Kaggle competition: My approach
</h1>
<div class=post-meta>October 7, 2014&nbsp;¬∑&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2014-10-07-greek-media-monitoring-kaggle-competition-my-approach/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a>
</div>
</header>
<figure class=entry-cover>
<img loading=lazy srcset="https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_360x0_resize_box_3.png 360w ,https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_480x0_resize_box_3.png 480w ,https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_720x0_resize_box_3.png 720w ,https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png 769w" sizes="(min-width: 768px) 720px, 100vw" src=https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png alt width=769 height=527>
</figure>
<div class=post-content><p>A few months ago I participated in the <a href=http://www.kaggle.com/c/wise-2014 target=_blank rel=noopener>Kaggle Greek Media Monitoring competition</a>. The goal of the competition was doing <a href=https://en.wikipedia.org/wiki/Multi-label_classification target=_blank rel=noopener>multilabel classification</a> of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.</p>
<h3 id=data--evaluation>Data & evaluation<a hidden class=anchor aria-hidden=true href=#data--evaluation>#</a></h3>
<p>The data consists of articles scanned from Greek print media in May-September 2013. Due to copyright issues, the organisers didn&rsquo;t make the original articles available ‚Äì competitors only had access to normalised <a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf target=_blank rel=noopener>tf-idf</a> representations of the texts. This limited the options for doing feature engineering and made it impossible to consider things like word order, but it made things somewhat simpler as the focus was on modelling due to inability to extract interesting features.</p>
<p>Overall, there are about 65K texts in the training set and 35K in the test set, where the split is based on chronological ordering (i.e., the training articles were published before the test articles). Each article was manually labelled with one or more labels out of a set of 203 labels. For each test article, the goal is to infer its set of labels. Submissions were ranked using the <a href=http://www.kaggle.com/c/wise-2014/details/evaluation target=_blank rel=noopener>mean F1 score</a>.</p>
<p>Despite being manually annotated, the data isn&rsquo;t very clean. Issues include identical texts that have different labels, empty articles, and articles with very few words. For example, the training set includes ten &ldquo;articles&rdquo; with a single word. Five of these articles have the word 68839, but each of these five was given a different label. Such issues are not unusual in Kaggle competitions or in real life, but they do limit the general usefulness of the results since any model built on this data would fit some noise.</p>
<h3 id=local-validation-setup>Local validation setup<a hidden class=anchor aria-hidden=true href=#local-validation-setup>#</a></h3>
<p>As mentioned in previous posts (<a href=http://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/>How to (almost) win Kaggle competitions</a> and <a href=http://yanirseroussi.com/2014/01/19/kaggle-beginner-tips/>Kaggle beginner tips</a>) having a solid local validation setup is very important. It ensures you don&rsquo;t waste time on weak submissions, increases confidence in the models, and avoids leaking information about how well you&rsquo;re doing.</p>
<p>I used the first 35K training texts for local training and the following 30K texts for validation. While the article publication dates weren&rsquo;t provided, I hoped that this would mimic the competition setup, where the test dataset consists of articles that were published after the articles in the training dataset. This seemed to work, as my local results were consistent with the leaderboard results. I&rsquo;m pleased to report that this setup allowed me to have the lowest number of submissions of all the top-10 teams üôÇ</p>
<h3 id=things-that-worked>Things that worked<a hidden class=anchor aria-hidden=true href=#things-that-worked>#</a></h3>
<p>I originally wanted to use this competition to play with deep learning through Python packages such as <a href=http://deeplearning.net/software/theano/ target=_blank rel=noopener>Theano</a> and <a href=http://deeplearning.net/software/pylearn2/ target=_blank rel=noopener>PyLearn2</a>. However, as this was the first time I worked on a multilabel classification problem, I got sucked into reading a lot of papers on the topic and never got around to doing deep learning. Maybe next time&mldr;</p>
<p>One of my key discoveries was that there if you define a graph where the vertices are labels and there&rsquo;s an edge between two labels if they appear together in a document&rsquo;s label set, then there are two main connected components of labels and several small ones with single labels (see figure below). It is possible to train a linear classifier that distinguishes between the components with very high accuracy (over 99%). This allowed me to improve performance by training different classifiers on each connected component.</p>
<figure>
<a href=wise2014-connected-components.png target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 720px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_360x0_resize_box_3.png 360w,
https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_480x0_resize_box_3.png 480w,
https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_720x0_resize_box_3.png 720w,
https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png 769w," src=https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png alt="wise2014 connected components" loading=lazy>
</a>
</figure>
<p>My best submission ended up being a simple weighted linear combination of three models. All these models are hierarchical ensembles, where a linear classifier distinguishes between connected components, and the base models are trained on texts from a single connected component. These base models are:</p>
<ol>
<li><a href=http://www.cms.waikato.ac.nz/~ml/publications/2009/chains.pdf target=_blank rel=noopener>Ensemble of classifier chains</a> (ECC) with linear classifiers (<a href=http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html target=_blank rel=noopener>SGDClassifier from scikit-learn</a>) trained for each label, using hinge loss and L1 penalty</li>
<li>Same as 1, but with modified Huber loss</li>
<li>A linear classifier with modified Huber loss and L1 penalty that predicts single label probabilities</li>
</ol>
<p>For each test document, each one of these base models yields a score for each label. These scores are weighted and thresholded to yield the final predictions.</p>
<p>It was interesting to learn that a relatively-simple model like ECC yields competitive results. The basic idea behind ECC is to combine different <em>classifier chains</em>. Each classifier chain is also an ensemble where each base classifier is trained to predict a single label. The input for each classifier in the chain depends on the output of preceding classifiers, so it encodes dependencies between labels. For example, if label 2 always appears with label 1 and the label 1 classifier precedes the label 2 classifier in the chain, the label 2 classifier is able to use this dependency information directly, which should increase its accuracy (though it is affected by misclassifications by the label 1 classifier). See <a href=http://www.cms.waikato.ac.nz/~ml/publications/2009/chains.pdf target=_blank rel=noopener>Read et al.&rsquo;s paper</a> for a more in-depth explanation.</p>
<p>Another notable observation is that L1 penalty worked well, which is not too surprising when considering the fact that the dataset has 300K features and many of them are probably irrelevant to prediction (L1 penalty yields sparse models where many features get zero weight).</p>
<h3 id=things-that-didnt-work>Things that didn&rsquo;t work<a hidden class=anchor aria-hidden=true href=#things-that-didnt-work>#</a></h3>
<p>As I was travelling, I didn&rsquo;t have much time to work on this competition over its two final weeks (though this was a good way of passing the time on long flights). One thing that I tried was understanding some of the probabilistic classifier chain (PCC) code out there by porting it to Python, but the results were very disappointing, probably due to bugs in my code. I expected PCC to work well, especially with <a href=http://papers.nips.cc/paper/4389-an-exact-algorithm-for-f-measure-maximization target=_blank rel=noopener>the extension for optimising the F-measure</a>. Figuring out how to run the Java code would have probably been a better use of my time than porting the code to Python.</p>
<p>I also played with reverse-engineering the features back to counts, but it was problematic since the feature values are normalised. It was disappointing that we weren&rsquo;t at least given the bag of words representations. I also attempted to reduce the feature representation with <a href=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation target=_blank rel=noopener>latent Dirichlet allocation</a>, but it didn&rsquo;t perform well ‚Äì possibly because I couldn&rsquo;t get the correct word counts.</p>
<h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3>
<p>Overall, this was a fun competition. Despite minor issues with the data and not having enough time to do everything I wanted to do, it was a great learning experience. From reading <a href=http://www.kaggle.com/c/wise-2014/forums/t/9773/our-approach-5th-place/50995 target=_blank rel=noopener>the summaries by the other teams</a>, it appears that other competitors enjoyed it too. As always, I highly recommend Kaggle competitions to beginners who are trying to learn more about the field of data science and predictive modelling, and to more experienced data scientists who want to improve their skills.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/data-science/>data science</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/kaggle/>kaggle</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/kaggle-competition/>kaggle competition</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/multi-label-classification/>multi-label classification</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/predictive-modelling/>predictive modelling</a></li>
</ul>
</footer><section class=comment-section>
<strong>No comments</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirs.github.io%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>
Comment via GitHub issue
</a>
</section>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://yanirs.github.io/yanirseroussi.com/>Yanir Seroussi | Data science and beyond</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><div class=mailing-list-container>
<form class=mailing-list action=https://tinyletter.com/yanir method=post target=popupwindow onsubmit="return window.open('https://tinyletter.com/yanir','popupwindow','scrollbars=yes,width=800,height=600'),!0">
<label for=mailing-list-email>Get new post notifications</label>
<input type=text name=email id=mailing-list-email placeholder="Email address">
<input type=hidden value=1 name=embed>
<input type=submit value=Subscribe>
</form>
</div>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>