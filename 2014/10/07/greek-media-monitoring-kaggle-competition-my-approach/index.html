<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Greek Media Monitoring Kaggle competition: My approach | Yanir Seroussi | Data science and beyond</title><meta name=keywords content="data science,Kaggle,Kaggle competition,multi-label classification,predictive modelling"><meta name=description content="Summary of my approach to the Greek Media Monitoring Kaggle competition, where I finished 6th out of 120 teams."><meta name=author content="Yanir Seroussi"><link rel=canonical href=https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/><meta name=google-site-verification content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU"><link crossorigin=anonymous href=/assets/css/stylesheet.14c2944979911d0cdd8e64a58dfa90394ec943228a4c13c39d6ea94250330089.css integrity="sha256-FMKUSXmRHQzdjmSljfqQOU7JQyKKTBPDnW6pQlAzAIk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirseroussi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yanirseroussi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yanirseroussi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yanirseroussi.com/apple-touch-icon.png><link rel=mask-icon href=https://yanirseroussi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Greek Media Monitoring Kaggle competition: My approach"><meta property="og:description" content="Summary of my approach to the Greek Media Monitoring Kaggle competition, where I finished 6th out of 120 teams."><meta property="og:type" content="article"><meta property="og:url" content="https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/"><meta property="og:image" content="https://yanirseroussi.com/wise2014-connected-components.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2014-10-07T03:21:35+00:00"><meta property="article:modified_time" content="2023-07-06T09:28:02+10:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yanirseroussi.com/wise2014-connected-components.png"><meta name=twitter:title content="Greek Media Monitoring Kaggle competition: My approach"><meta name=twitter:description content="Summary of my approach to the Greek Media Monitoring Kaggle competition, where I finished 6th out of 120 teams."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Greek Media Monitoring Kaggle competition: My approach","item":"https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Greek Media Monitoring Kaggle competition: My approach","name":"Greek Media Monitoring Kaggle competition: My approach","description":"Summary of my approach to the Greek Media Monitoring Kaggle competition, where I finished 6th out of 120 teams.","keywords":["data science","Kaggle","Kaggle competition","multi-label classification","predictive modelling"],"articleBody":"A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.\nData \u0026 evaluation The data consists of articles scanned from Greek print media in May-September 2013. Due to copyright issues, the organisers didn‚Äôt make the original articles available ‚Äì competitors only had access to normalised tf-idf representations of the texts. This limited the options for doing feature engineering and made it impossible to consider things like word order, but it made things somewhat simpler as the focus was on modelling due to inability to extract interesting features.\nOverall, there are about 65K texts in the training set and 35K in the test set, where the split is based on chronological ordering (i.e., the training articles were published before the test articles). Each article was manually labelled with one or more labels out of a set of 203 labels. For each test article, the goal is to infer its set of labels. Submissions were ranked using the mean F1 score.\nDespite being manually annotated, the data isn‚Äôt very clean. Issues include identical texts that have different labels, empty articles, and articles with very few words. For example, the training set includes ten ‚Äúarticles‚Äù with a single word. Five of these articles have the word 68839, but each of these five was given a different label. Such issues are not unusual in Kaggle competitions or in real life, but they do limit the general usefulness of the results since any model built on this data would fit some noise.\nLocal validation setup As mentioned in previous posts (How to (almost) win Kaggle competitions and Kaggle beginner tips) having a solid local validation setup is very important. It ensures you don‚Äôt waste time on weak submissions, increases confidence in the models, and avoids leaking information about how well you‚Äôre doing.\nI used the first 35K training texts for local training and the following 30K texts for validation. While the article publication dates weren‚Äôt provided, I hoped that this would mimic the competition setup, where the test dataset consists of articles that were published after the articles in the training dataset. This seemed to work, as my local results were consistent with the leaderboard results. I‚Äôm pleased to report that this setup allowed me to have the lowest number of submissions of all the top-10 teams üôÇ\nThings that worked I originally wanted to use this competition to play with deep learning through Python packages such as Theano and PyLearn2. However, as this was the first time I worked on a multilabel classification problem, I got sucked into reading a lot of papers on the topic and never got around to doing deep learning. Maybe next time‚Ä¶\nOne of my key discoveries was that there if you define a graph where the vertices are labels and there‚Äôs an edge between two labels if they appear together in a document‚Äôs label set, then there are two main connected components of labels and several small ones with single labels (see figure below). It is possible to train a linear classifier that distinguishes between the components with very high accuracy (over 99%). This allowed me to improve performance by training different classifiers on each connected component.\nMy best submission ended up being a simple weighted linear combination of three models. All these models are hierarchical ensembles, where a linear classifier distinguishes between connected components, and the base models are trained on texts from a single connected component. These base models are:\nEnsemble of classifier chains (ECC) with linear classifiers (SGDClassifier from scikit-learn) trained for each label, using hinge loss and L1 penalty Same as 1, but with modified Huber loss A linear classifier with modified Huber loss and L1 penalty that predicts single label probabilities For each test document, each one of these base models yields a score for each label. These scores are weighted and thresholded to yield the final predictions.\nIt was interesting to learn that a relatively-simple model like ECC yields competitive results. The basic idea behind ECC is to combine different classifier chains. Each classifier chain is also an ensemble where each base classifier is trained to predict a single label. The input for each classifier in the chain depends on the output of preceding classifiers, so it encodes dependencies between labels. For example, if label 2 always appears with label 1 and the label 1 classifier precedes the label 2 classifier in the chain, the label 2 classifier is able to use this dependency information directly, which should increase its accuracy (though it is affected by misclassifications by the label 1 classifier). See Read et al.‚Äôs paper for a more in-depth explanation.\nAnother notable observation is that L1 penalty worked well, which is not too surprising when considering the fact that the dataset has 300K features and many of them are probably irrelevant to prediction (L1 penalty yields sparse models where many features get zero weight).\nThings that didn‚Äôt work As I was travelling, I didn‚Äôt have much time to work on this competition over its two final weeks (though this was a good way of passing the time on long flights). One thing that I tried was understanding some of the probabilistic classifier chain (PCC) code out there by porting it to Python, but the results were very disappointing, probably due to bugs in my code. I expected PCC to work well, especially with the extension for optimising the F-measure. Figuring out how to run the Java code would have probably been a better use of my time than porting the code to Python.\nI also played with reverse-engineering the features back to counts, but it was problematic since the feature values are normalised. It was disappointing that we weren‚Äôt at least given the bag of words representations. I also attempted to reduce the feature representation with latent Dirichlet allocation, but it didn‚Äôt perform well ‚Äì possibly because I couldn‚Äôt get the correct word counts.\nConclusion Overall, this was a fun competition. Despite minor issues with the data and not having enough time to do everything I wanted to do, it was a great learning experience. From reading the summaries by the other teams, it appears that other competitors enjoyed it too. As always, I highly recommend Kaggle competitions to beginners who are trying to learn more about the field of data science and predictive modelling, and to more experienced data scientists who want to improve their skills.\n","wordCount":"1114","inLanguage":"en","image":"https://yanirseroussi.com/wise2014-connected-components.png","datePublished":"2014-10-07T03:21:35Z","dateModified":"2023-07-06T09:28:02+10:00","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirseroussi.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yanirseroussi.com/about/ title=About><span>About</span></a></li><li><a href=https://yanirseroussi.com/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://yanirseroussi.com/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Greek Media Monitoring Kaggle competition: My approach</h1><div class=post-meta><span title='2014-10-07 03:21:35 +0000 UTC'>October 7, 2014</span>&nbsp;¬∑&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2014-10-07-greek-media-monitoring-kaggle-competition-my-approach/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a></div></header><figure class=entry-cover><img loading=lazy srcset="https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_360x0_resize_box_3.png 360w ,https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_480x0_resize_box_3.png 480w ,https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_720x0_resize_box_3.png 720w ,https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png 769w" sizes="(min-width: 768px) 720px, 100vw" src=https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png alt width=769 height=527></figure><div class=post-content><p>A few months ago I participated in the <a href=http://www.kaggle.com/c/wise-2014 target=_blank rel=noopener>Kaggle Greek Media Monitoring competition</a>. The goal of the competition was doing <a href=https://en.wikipedia.org/wiki/Multi-label_classification target=_blank rel=noopener>multilabel classification</a> of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.</p><h3 id=data--evaluation>Data & evaluation<a hidden class=anchor aria-hidden=true href=#data--evaluation>#</a></h3><p>The data consists of articles scanned from Greek print media in May-September 2013. Due to copyright issues, the organisers didn&rsquo;t make the original articles available ‚Äì competitors only had access to normalised <a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf target=_blank rel=noopener>tf-idf</a> representations of the texts. This limited the options for doing feature engineering and made it impossible to consider things like word order, but it made things somewhat simpler as the focus was on modelling due to inability to extract interesting features.</p><p>Overall, there are about 65K texts in the training set and 35K in the test set, where the split is based on chronological ordering (i.e., the training articles were published before the test articles). Each article was manually labelled with one or more labels out of a set of 203 labels. For each test article, the goal is to infer its set of labels. Submissions were ranked using the <a href=http://www.kaggle.com/c/wise-2014/details/evaluation target=_blank rel=noopener>mean F1 score</a>.</p><p>Despite being manually annotated, the data isn&rsquo;t very clean. Issues include identical texts that have different labels, empty articles, and articles with very few words. For example, the training set includes ten &ldquo;articles&rdquo; with a single word. Five of these articles have the word 68839, but each of these five was given a different label. Such issues are not unusual in Kaggle competitions or in real life, but they do limit the general usefulness of the results since any model built on this data would fit some noise.</p><h3 id=local-validation-setup>Local validation setup<a hidden class=anchor aria-hidden=true href=#local-validation-setup>#</a></h3><p>As mentioned in previous posts (<a href=https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/>How to (almost) win Kaggle competitions</a> and <a href=https://yanirseroussi.com/2014/01/19/kaggle-beginner-tips/>Kaggle beginner tips</a>) having a solid local validation setup is very important. It ensures you don&rsquo;t waste time on weak submissions, increases confidence in the models, and avoids leaking information about how well you&rsquo;re doing.</p><p>I used the first 35K training texts for local training and the following 30K texts for validation. While the article publication dates weren&rsquo;t provided, I hoped that this would mimic the competition setup, where the test dataset consists of articles that were published after the articles in the training dataset. This seemed to work, as my local results were consistent with the leaderboard results. I&rsquo;m pleased to report that this setup allowed me to have the lowest number of submissions of all the top-10 teams üôÇ</p><h3 id=things-that-worked>Things that worked<a hidden class=anchor aria-hidden=true href=#things-that-worked>#</a></h3><p>I originally wanted to use this competition to play with deep learning through Python packages such as <a href=http://deeplearning.net/software/theano/ target=_blank rel=noopener>Theano</a> and <a href=http://deeplearning.net/software/pylearn2/ target=_blank rel=noopener>PyLearn2</a>. However, as this was the first time I worked on a multilabel classification problem, I got sucked into reading a lot of papers on the topic and never got around to doing deep learning. Maybe next time&mldr;</p><p>One of my key discoveries was that there if you define a graph where the vertices are labels and there&rsquo;s an edge between two labels if they appear together in a document&rsquo;s label set, then there are two main connected components of labels and several small ones with single labels (see figure below). It is possible to train a linear classifier that distinguishes between the components with very high accuracy (over 99%). This allowed me to improve performance by training different classifiers on each connected component.</p><figure><a href=wise2014-connected-components.png target=_blank rel=noopener><img sizes="
          (min-width: 768px) 720px,
          100vw
        " srcset="https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_360x0_resize_box_3.png 360w,
https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_480x0_resize_box_3.png 480w,
https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components_hu4bfbbe3f9a9448d9a431640c78e486b4_93326_720x0_resize_box_3.png 720w,
https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png 769w," src=https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/wise2014-connected-components.png alt="wise2014 connected components" loading=lazy></a></figure><p>My best submission ended up being a simple weighted linear combination of three models. All these models are hierarchical ensembles, where a linear classifier distinguishes between connected components, and the base models are trained on texts from a single connected component. These base models are:</p><ol><li><a href=http://www.cms.waikato.ac.nz/~ml/publications/2009/chains.pdf target=_blank rel=noopener>Ensemble of classifier chains</a> (ECC) with linear classifiers (<a href=http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html target=_blank rel=noopener>SGDClassifier from scikit-learn</a>) trained for each label, using hinge loss and L1 penalty</li><li>Same as 1, but with modified Huber loss</li><li>A linear classifier with modified Huber loss and L1 penalty that predicts single label probabilities</li></ol><p>For each test document, each one of these base models yields a score for each label. These scores are weighted and thresholded to yield the final predictions.</p><p>It was interesting to learn that a relatively-simple model like ECC yields competitive results. The basic idea behind ECC is to combine different <em>classifier chains</em>. Each classifier chain is also an ensemble where each base classifier is trained to predict a single label. The input for each classifier in the chain depends on the output of preceding classifiers, so it encodes dependencies between labels. For example, if label 2 always appears with label 1 and the label 1 classifier precedes the label 2 classifier in the chain, the label 2 classifier is able to use this dependency information directly, which should increase its accuracy (though it is affected by misclassifications by the label 1 classifier). See <a href=http://www.cms.waikato.ac.nz/~ml/publications/2009/chains.pdf target=_blank rel=noopener>Read et al.&rsquo;s paper</a> for a more in-depth explanation.</p><p>Another notable observation is that L1 penalty worked well, which is not too surprising when considering the fact that the dataset has 300K features and many of them are probably irrelevant to prediction (L1 penalty yields sparse models where many features get zero weight).</p><h3 id=things-that-didnt-work>Things that didn&rsquo;t work<a hidden class=anchor aria-hidden=true href=#things-that-didnt-work>#</a></h3><p>As I was travelling, I didn&rsquo;t have much time to work on this competition over its two final weeks (though this was a good way of passing the time on long flights). One thing that I tried was understanding some of the probabilistic classifier chain (PCC) code out there by porting it to Python, but the results were very disappointing, probably due to bugs in my code. I expected PCC to work well, especially with <a href=http://papers.nips.cc/paper/4389-an-exact-algorithm-for-f-measure-maximization target=_blank rel=noopener>the extension for optimising the F-measure</a>. Figuring out how to run the Java code would have probably been a better use of my time than porting the code to Python.</p><p>I also played with reverse-engineering the features back to counts, but it was problematic since the feature values are normalised. It was disappointing that we weren&rsquo;t at least given the bag of words representations. I also attempted to reduce the feature representation with <a href=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation target=_blank rel=noopener>latent Dirichlet allocation</a>, but it didn&rsquo;t perform well ‚Äì possibly because I couldn&rsquo;t get the correct word counts.</p><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>Overall, this was a fun competition. Despite minor issues with the data and not having enough time to do everything I wanted to do, it was a great learning experience. From reading <a href=http://www.kaggle.com/c/wise-2014/forums/t/9773/our-approach-5th-place/50995 target=_blank rel=noopener>the summaries by the other teams</a>, it appears that other competitors enjoyed it too. As always, I highly recommend Kaggle competitions to beginners who are trying to learn more about the field of data science and predictive modelling, and to more experienced data scientists who want to improve their skills.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yanirseroussi.com/tags/data-science/>data science</a></li><li><a href=https://yanirseroussi.com/tags/kaggle/>Kaggle</a></li><li><a href=https://yanirseroussi.com/tags/kaggle-competition/>Kaggle competition</a></li><li><a href=https://yanirseroussi.com/tags/multi-label-classification/>multi-label classification</a></li><li><a href=https://yanirseroussi.com/tags/predictive-modelling/>predictive modelling</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Greek Media Monitoring Kaggle competition: My approach on twitter" href="https://twitter.com/intent/tweet/?text=Greek%20Media%20Monitoring%20Kaggle%20competition%3a%20My%20approach&amp;url=https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f&amp;hashtags=datascience%2cKaggle%2cKagglecompetition%2cmulti-labelclassification%2cpredictivemodelling"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Greek Media Monitoring Kaggle competition: My approach on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f&amp;title=Greek%20Media%20Monitoring%20Kaggle%20competition%3a%20My%20approach&amp;summary=Greek%20Media%20Monitoring%20Kaggle%20competition%3a%20My%20approach&amp;source=https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Greek Media Monitoring Kaggle competition: My approach on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f&title=Greek%20Media%20Monitoring%20Kaggle%20competition%3a%20My%20approach"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Greek Media Monitoring Kaggle competition: My approach on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Greek Media Monitoring Kaggle competition: My approach on whatsapp" href="https://api.whatsapp.com/send?text=Greek%20Media%20Monitoring%20Kaggle%20competition%3a%20My%20approach%20-%20https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Greek Media Monitoring Kaggle competition: My approach on telegram" href="https://telegram.me/share/url?text=Greek%20Media%20Monitoring%20Kaggle%20competition%3a%20My%20approach&amp;url=https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><section class=comment-section><strong>No comments</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirseroussi.com%2f2014%2f10%2f07%2fgreek-media-monitoring-kaggle-competition-my-approach%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>Comment via GitHub issue</a></section></article></main><footer class=footer><span>Text and figures licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank rel=noopener>CC BY-NC-ND 4.0</a> by <a href=https://yanirseroussi.com/about/>Yanir Seroussi</a>, except where noted otherwise¬†¬†|</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><div class=mailing-list-container><form class=mailing-list action="https://yanirseroussi.us17.list-manage.com/subscribe/post?u=3c08aa3ff27dd92978019febd&amp;id=bc3ab705af" method=post target=_blank novalidate><label for=mailing-list-email>Get new post notifications</label>
<input type=text name=EMAIL id=mailing-list-email placeholder="Email address"><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_3c08aa3ff27dd92978019febd_bc3ab705af tabindex=-1></div><input type=submit value=Subscribe></form><div class=footer>Alternatively, <a href=https://github.com/yanirs/yanirseroussi.com rel=noopener target=_blank>watch on GitHub</a>
or <a href=https://yanirseroussi.com/index.xml>subscribe to RSS feed</a>.</div></div><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>