<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>How to (almost) win Kaggle competitions | Yanir Seroussi | Data science and beyond</title>
<meta name=keywords content="data science,kaggle,kaggle beginners,kaggle competition,predictive modelling">
<meta name=description content="Last week, I gave a talk at the Data Science Sydney Meetup group about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are here). Some of these tips were covered in my beginner tips post from a few months ago. Similar advice was also recently published on the Kaggle blog – it&rsquo;s great to see that my tips are in line with the thoughts of other prolific kagglers.">
<meta name=author content="Yanir Seroussi">
<link rel=canonical href=https://yanirs.github.io/yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/>
<link crossorigin=anonymous href=/yanirseroussi.com/assets/css/stylesheet.min.51e68192da3381c9040a063242bafad56d6d28666fff1f9e523f9eaad0207a83.css integrity="sha256-UeaBktozgckECgYyQrr61W1tKGZv/x+eUj+eqtAgeoM=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/yanirseroussi.com/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirs.github.io/yanirseroussi.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://yanirs.github.io/yanirseroussi.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://yanirs.github.io/yanirseroussi.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://yanirs.github.io/yanirseroussi.com/apple-touch-icon.png>
<link rel=mask-icon href=https://yanirs.github.io/yanirseroussi.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="How to (almost) win Kaggle competitions">
<meta property="og:description" content="Last week, I gave a talk at the Data Science Sydney Meetup group about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are here). Some of these tips were covered in my beginner tips post from a few months ago. Similar advice was also recently published on the Kaggle blog – it&rsquo;s great to see that my tips are in line with the thoughts of other prolific kagglers.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yanirs.github.io/yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2014-08-24T12:40:53+00:00">
<meta property="article:modified_time" content="2014-08-24T12:40:53+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="How to (almost) win Kaggle competitions">
<meta name=twitter:description content="Last week, I gave a talk at the Data Science Sydney Meetup group about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are here). Some of these tips were covered in my beginner tips post from a few months ago. Similar advice was also recently published on the Kaggle blog – it&rsquo;s great to see that my tips are in line with the thoughts of other prolific kagglers.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirs.github.io/yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"How to (almost) win Kaggle competitions","item":"https://yanirs.github.io/yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How to (almost) win Kaggle competitions","name":"How to (almost) win Kaggle competitions","description":"Last week, I gave a talk at the Data Science Sydney Meetup group about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are here). Some of these tips were covered in my beginner tips post from a few months ago. Similar advice was also recently published on the Kaggle blog – it\u0026rsquo;s great to see that my tips are in line with the thoughts of other prolific kagglers.","keywords":["data science","kaggle","kaggle beginners","kaggle competition","predictive modelling"],"articleBody":"Last week, I gave a talk at the Data Science Sydney Meetup group about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are here). Some of these tips were covered in my beginner tips post from a few months ago. Similar advice was also recently published on the Kaggle blog – it’s great to see that my tips are in line with the thoughts of other prolific kagglers.\nTip 1: RTFM It’s surprising to see how many people miss out on important details, such as remembering the final date to make the first submission. Before jumping into building models, it’s important to understand the competition timeline, be able to reproduce benchmarks, generate the correct submission format, etc.\nTip 2: Know your measure A key part of doing well in a competition is understanding how the measure works. It’s often easy to obtain significant improvements in your score by using an optimisation approach that is suitable to the measure. A classic example is optimising the mean absolute error (MAE) versus the mean square error (MSE). It’s easy to show that given no other data for a set of numbers, the predictor that minimises the MAE is the median, while the predictor that minimises the MSE is the mean. Indeed, in the EMC Data Science Hackathon we fell back to the median rather than the mean when there wasn’t enough data, and that ended up working pretty well.\nTip 3: Know your data In Kaggle competitions, overspecialisation (without overfitting) is a good thing. This is unlike academic machine learning papers, where researchers often test their proposed method on many different datasets. This is also unlike more applied work, where you may care about data drifting and whether what you predict actually makes sense. Examples include the Hackathon, where the measures of pollutants in the air were repeated for consecutive hours (i.e., they weren’t really measured); the multi-label Greek article competition, where I found connected components of labels (doesn’t generalise well to other datasets); and the Arabic writers competition, where I used histogram kernels to deal with the features that we were given. The general lesson is that custom solutions win, and that’s why the world needs data scientists (at least until we are replaced by robots).\nTip 4: What before how It’s important to know what you want to model before figuring out how to model it. It seems like many beginners tend to worry too much about which tool to use (Python or R? Logistic regression or SVMs?), when they should be worrying about understanding the data and what useful patterns they want to capture. For example, when we worked on the Yandex search personalisation competition, we spent a lot of time looking at the data and thinking what makes sense for users to be doing. In that case it was easy to come up with ideas, because we all use search engines. But the main message is that to be effective, you have to become one with the data.\nTip 5: Do local validation This is a point I covered in my Kaggle beginner tips post. Having a local validation environment allows you to move faster and produce more reliable results than when relying on the leaderboard. The main scenarios when you should skip local validation is when the data is too small (a problem I had in the Arabic writers competition), or when you run out of time (towards the end of the competition).\nTip 6: Make fewer submissions In addition to making you look good, making few submissions reduces the likelihood of overfitting the leaderboard, which is a real problem. If your local validation is set up well and is consistent with the leaderboard (which you need to test by making one or two submissions), there’s really no need to make many submissions. Further, if you’re doing well, making submissions erodes your competitive advantage by showing your competitors what scores are obtainable and motivating them to work harder. Just resist the urge to submit, unless you have a really good reason.\nTip 7: Do your research For any given problem, it’s likely that there are people dedicating their lives to its solution. These people (often academics) have probably published papers, benchmarks and code, which you can learn from. Unlike actually winning, which is not only dependent on you, gaining deeper knowledge and understanding is the only sure reward of a competition. This has worked well for me, as I’ve learned something new and applied it successfully in nearly every competition I’ve worked on.\nTip 8: Apply the basics rigorously While playing with obscure methods can be a lot of fun, it’s often the case that the basics will get you very far. Common algorithms have good implementations in most major languages, so there’s really no reason not to try them. However, note that when you do try any methods, you must do some minimal tuning of the main parameters (e.g., number of trees in a random forest or the regularisation of a linear model). Running a method without minimal tuning is worse than not running it at all, because you may get a false negative – giving up on a method that actually works very well.\nAn example of applying the basics rigorously is in the classic paper In defense of one-vs-all classification, where the authors showed that the simple one-vs-all (OVA) approach to multiclass classification is at least as good as approaches that are much more sophisticated. In their words: “What we find is that although a wide array of more sophisticated methods for multiclass classification exist, experimental evidence of the superiority of these methods over a simple OVA scheme is either lacking or improperly controlled or measured”. If such a failure to perform proper experiments can happen to serious machine learning researchers, it can definitely happen to the average kaggler. Don’t let it happen to you.\nTip 9: The forum is your friend It’s very important to subscribe to the forum to receive notifications on issues with the data or the competition. In addition, it’s worth trying to figure out what your competitors are doing. An extreme example is the recent trend of code sharing during the competition (which I don’t really like) – while it’s not a good idea to rely on such code, it’s important to be aware of its existence. Finally, reading the post-competition summaries on the forum is a valuable way of learning from the winners and improving over time.\nTip 10: Ensemble all the things Not to be confused with ensemble methods (which are also very important), the idea here is to combine models that were developed independently. In high-profile competitions, it is often the case that teams merge and gain a significant boost from combining their models. This is worth doing even when competing alone, because almost no competition is won by a single model.\n","wordCount":"1169","inLanguage":"en","datePublished":"2014-08-24T12:40:53Z","dateModified":"2014-08-24T12:40:53Z","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirs.github.io/yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirs.github.io/yanirseroussi.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://yanirs.github.io/yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
How to (almost) win Kaggle competitions
</h1>
<div class=post-meta>August 24, 2014&nbsp;·&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2014-08-24-how-to-almost-win-kaggle-competitions/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a>
</div>
</header>
<div class=post-content><p>Last week, I gave a talk at the <a href=http://www.meetup.com/Data-Science-Sydney/ target=_blank rel=noopener>Data Science Sydney Meetup group</a> about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are <a href=http://yanirs.github.io/talks/data-science-sydney-winning-kaggle/ target=_blank rel=noopener>here</a>). Some of these tips were covered in my <a href=http://yanirseroussi.com/2014/01/19/kaggle-beginner-tips/>beginner tips post</a> from a few months ago. Similar advice was also <a href=http://blog.kaggle.com/2014/08/01/learning-from-the-best/ target=_blank rel=noopener>recently published on the Kaggle blog</a> – it&rsquo;s great to see that my tips are in line with the thoughts of other prolific kagglers.</p>
<h3 id=tip-1-rtfm>Tip 1: RTFM<a hidden class=anchor aria-hidden=true href=#tip-1-rtfm>#</a></h3>
<p>It&rsquo;s surprising to see how many people miss out on important details, such as remembering the final date to make the first submission. Before jumping into building models, it&rsquo;s important to understand the competition timeline, be able to reproduce benchmarks, generate the correct submission format, etc.</p>
<h3 id=tip-2-know-your-measure>Tip 2: Know your measure<a hidden class=anchor aria-hidden=true href=#tip-2-know-your-measure>#</a></h3>
<p>A key part of doing well in a competition is understanding how the measure works. It&rsquo;s often easy to obtain significant improvements in your score by using an optimisation approach that is suitable to the measure. A classic example is optimising the mean absolute error (MAE) versus the mean square error (MSE). It&rsquo;s easy to show that given no other data for a set of numbers, the predictor that minimises the MAE is the median, while the predictor that minimises the MSE is the mean. Indeed, in the <a href=https://www.kaggle.com/c/dsg-hackathon/forums/t/1821/general-approaches-to-partitioning-the-models/10631#post10631 target=_blank rel=noopener>EMC Data Science Hackathon</a> we fell back to the median rather than the mean when there wasn&rsquo;t enough data, and that ended up working pretty well.</p>
<h3 id=tip-3-know-your-data>Tip 3: Know your data<a hidden class=anchor aria-hidden=true href=#tip-3-know-your-data>#</a></h3>
<p>In Kaggle competitions, overspecialisation (without overfitting) is a good thing. This is unlike academic machine learning papers, where researchers often test their proposed method on many different datasets. This is also unlike more applied work, where you may care about data drifting and whether what you predict actually makes sense. Examples include the <a href=https://www.kaggle.com/c/dsg-hackathon/forums/t/1821/general-approaches-to-partitioning-the-models/10631#post10631 target=_blank rel=noopener>Hackathon</a>, where the measures of pollutants in the air were repeated for consecutive hours (i.e., they weren&rsquo;t really measured); the <a title="Greek Media Monitoring Kaggle competition: My approach" href=http://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/ target=_blank rel=noopener>multi-label Greek article competition</a>, where I found connected components of labels (doesn&rsquo;t generalise well to other datasets); and the <a href=http://blog.kaggle.com/2012/04/29/on-diffusion-kernels-histograms-and-arabic-writer-identification/ target=_blank rel=noopener>Arabic writers competition</a>, where I used histogram kernels to deal with the features that we were given. The general lesson is that custom solutions win, and that&rsquo;s why the world needs data scientists (at least <a href=http://www.datarobot.com/ target=_blank rel=noopener>until we are replaced by robots</a>).</p>
<h3 id=tip-4-what-before-how>Tip 4: What before how<a hidden class=anchor aria-hidden=true href=#tip-4-what-before-how>#</a></h3>
<p>It&rsquo;s important to know <em>what</em> you want to model before figuring out <em>how</em> to model it. It seems like many beginners tend to worry too much about which tool to use (Python or R? Logistic regression or SVMs?), when they should be worrying about understanding the data and what useful patterns they want to capture. For example, when we worked on the <a href=https://www.kaggle.com/c/yandex-personalized-web-search-challenge/forums/t/6811/share-your-approach/37306#post37306>Yandex search personalisation competition</a>, we spent a lot of time looking at the data and thinking what makes sense for users to be doing. In that case it was easy to come up with ideas, because we all use search engines. But the main message is that to be effective, you have to become one with the data.</p>
<h3 id=tip-5-do-local-validation>Tip 5: Do local validation<a hidden class=anchor aria-hidden=true href=#tip-5-do-local-validation>#</a></h3>
<p>This is a point I covered in my <a href=http://yanirseroussi.com/2014/01/19/kaggle-beginner-tips/#validation>Kaggle beginner tips post</a>. Having a local validation environment allows you to move faster and produce more reliable results than when relying on the leaderboard. The main scenarios when you should skip local validation is when the data is too small (a problem I had in the <a href=http://blog.kaggle.com/2012/04/29/on-diffusion-kernels-histograms-and-arabic-writer-identification/ target=_blank rel=noopener>Arabic writers competition</a>), or when you run out of time (towards the end of the competition).</p>
<h3 id=tip-6-make-fewer-submissions>Tip 6: Make fewer submissions<a hidden class=anchor aria-hidden=true href=#tip-6-make-fewer-submissions>#</a></h3>
<p>In addition to making you look good, making few submissions reduces the likelihood of overfitting the leaderboard, which is a real problem. If your local validation is set up well and is consistent with the leaderboard (which you need to test by making one or two submissions), there&rsquo;s really no need to make many submissions. Further, if you&rsquo;re doing well, making submissions erodes your competitive advantage by showing your competitors what scores are obtainable and motivating them to work harder. Just resist the urge to submit, unless you have a really good reason.</p>
<h3 id=tip-7-do-your-research>Tip 7: Do your research<a hidden class=anchor aria-hidden=true href=#tip-7-do-your-research>#</a></h3>
<p>For any given problem, it&rsquo;s likely that there are people dedicating their lives to its solution. These people (often academics) have probably published papers, benchmarks and code, which you can learn from. Unlike actually winning, which is not only dependent on you, gaining deeper knowledge and understanding is the only sure reward of a competition. This has worked well for me, as I&rsquo;ve learned something new and applied it successfully in <a href=http://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/>nearly every competition I&rsquo;ve worked on</a>.</p>
<h3 id=tip-8-apply-the-basics-rigorously>Tip 8: Apply the basics rigorously<a hidden class=anchor aria-hidden=true href=#tip-8-apply-the-basics-rigorously>#</a></h3>
<p>While playing with obscure methods can be a lot of fun, it&rsquo;s often the case that the basics will get you very far. Common algorithms have good implementations in most major languages, so there&rsquo;s really no reason not to try them. However, note that when you do try any methods, you <em>must</em> do some minimal tuning of the main parameters (e.g., number of trees in a random forest or the regularisation of a linear model). <strong>Running a method without minimal tuning is worse than not running it at all</strong>, because you may get a false negative – giving up on a method that actually works very well.</p>
<p>An example of applying the basics rigorously is in the classic paper <a href=http://jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf target=_blank rel=noopener>In defense of one-vs-all classification</a>, where the authors showed that the simple one-vs-all (OVA) approach to multiclass classification is at least as good as approaches that are much more sophisticated. In their words: &ldquo;What we find is that although a wide array of more sophisticated methods for multiclass classification exist, experimental evidence of the superiority of these methods over a simple OVA scheme is either lacking or improperly controlled or measured&rdquo;. If such a failure to perform proper experiments can happen to serious machine learning researchers, it can definitely happen to the average kaggler. Don&rsquo;t let it happen to you.</p>
<h3 id=tip-9-the-forum-is-your-friend>Tip 9: The forum is your friend<a hidden class=anchor aria-hidden=true href=#tip-9-the-forum-is-your-friend>#</a></h3>
<p>It&rsquo;s very important to subscribe to the forum to receive notifications on issues with the data or the competition. In addition, it&rsquo;s worth trying to figure out what your competitors are doing. An extreme example is the recent trend of code sharing during the competition (<a href=http://www.kaggle.com/forums/t/5681/fed-up-with-beating-benchmark-code/30787#post30787 target=_blank rel=noopener>which I don&rsquo;t really like</a>) – while it&rsquo;s not a good idea to rely on such code, it&rsquo;s important to be aware of its existence. Finally, reading the post-competition summaries on the forum is a valuable way of learning from the winners and improving over time.</p>
<h3 id=tip-10-ensemble-all-the-things>Tip 10: Ensemble all the things<a hidden class=anchor aria-hidden=true href=#tip-10-ensemble-all-the-things>#</a></h3>
<p>Not to be confused with ensemble methods (which are also very important), the idea here is to combine models that were developed independently. In high-profile competitions, it is often the case that teams merge and gain a significant boost from combining their models. This is worth doing even when competing alone, because almost no competition is won by a single model.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/data-science/>data science</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/kaggle/>kaggle</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/kaggle-beginners/>kaggle beginners</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/kaggle-competition/>kaggle competition</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/predictive-modelling/>predictive modelling</a></li>
</ul>
</footer><section class=comment-section>
<strong>No comments</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirs.github.io%2fyanirseroussi.com%2f2014%2f08%2f24%2fhow-to-almost-win-kaggle-competitions%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>
Comment via GitHub issue
</a>
</section>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://yanirs.github.io/yanirseroussi.com/>Yanir Seroussi | Data science and beyond</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>