<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Stochastic Gradient Boosting: Choosing the Best Number of Iterations | Yanir Seroussi | Data science and beyond</title>
<meta name=keywords content="data science,gradient boosting,machine learning,predictive modelling,scikit-learn">
<meta name=description content="In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).">
<meta name=author content="Yanir Seroussi">
<link rel=canonical href=https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/>
<link crossorigin=anonymous href=/yanirseroussi.com/assets/css/stylesheet.min.7603165cb47dcda1f46a839ca379731b2f33098c043e75f680940e69a5d546a8.css integrity="sha256-dgMWXLR9zaH0aoOco3lzGy8zCYwEPnX2gJQOaaXVRqg=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/yanirseroussi.com/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yanirs.github.io/yanirseroussi.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://yanirs.github.io/yanirseroussi.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://yanirs.github.io/yanirseroussi.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://yanirs.github.io/yanirseroussi.com/apple-touch-icon.png>
<link rel=mask-icon href=https://yanirs.github.io/yanirseroussi.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.89.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Stochastic Gradient Boosting: Choosing the Best Number of Iterations">
<meta property="og:description" content="In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2014-12-29T02:30:06+00:00">
<meta property="article:modified_time" content="2014-12-29T02:30:06+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Stochastic Gradient Boosting: Choosing the Best Number of Iterations">
<meta name=twitter:description content="In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yanirs.github.io/yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Stochastic Gradient Boosting: Choosing the Best Number of Iterations","item":"https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Stochastic Gradient Boosting: Choosing the Best Number of Iterations","name":"Stochastic Gradient Boosting: Choosing the Best Number of Iterations","description":"In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).","keywords":["data science","gradient boosting","machine learning","predictive modelling","scikit-learn"],"articleBody":"In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).\nThe bug in scikit-learn was fixed by porting the solution used in R’s GBM package, where the number of iterations is estimated by minimising the improvement on the OOB samples in each boosting iteration. This approach is known to underestimate the number of required iterations, which means that it’s not very useful in practice. This underestimation may be due to the fact that the GBM method is partly estimated on in-bag samples, as the OOB samples for the Nth iteration are likely to have been in-bag in previous iterations.\nI was curious about how my approach compares to the GBM method. Preliminary results on the toy dataset from scikit-learn’s documentation looked promising:\n   My approach (TSO) beat both 5-fold cross-validation (CV) and the GBM/scikit-learn method (SKO), as TSO obtains its minimum at the closest number of iterations to the test set’s (T) optimal value.\nThe next step in testing TSO’s viability was to rerun Ridgeway’s experiments from Section 3.3 of the GBM documentation (R code here). I used the same 12 UCI datasets that Ridgeway used, running 5×2 cross-validation on each one. For each dataset, the score was obtained by dividing the mean loss of the best method on the dataset by the loss of each method. Hence, all scores are between 0.0 and 1.0, with the best score being 1.0. The following figure summarises the results on the 12 datasets.\n   The following table shows the raw data that was used to produce the figure.\n   Dataset CV SKO TSO     creditrating 0.9962 0.9771 1   breastcancer 1 0.6675 0.4869   mushrooms 0.9588 0.9963 1   abalone 1 0.9754 0.9963   ionosphere 0.9919 1 0.8129   diabetes 1 0.9869 0.9985   autoprices 1 0.9565 0.5839   autompg 1 0.8753 0.9948   bostonhousing 1 0.8299 0.5412   haberman 1 0.9793 0.9266   cpuperformance 0.9934 0.9160 1   adult 1 0.9824 0.9991    The main finding is that CV remains the most reliable approach. Even when CV is not the best-performing method, it’s not much worse than the best method (this is in line with Ridgeway’s findings). TSO yielded the best results on 3/12 of the datasets, and beat SKO 7/12 times. However, TSO’s results are the most variant of the three methods: when it fails, it often yields very poor results.\nIn conclusion, stick to cross-validation for the best results. It’s more computationally intensive than SKO and TSO, but can be parallelised. I still think that there may be a way to avoid cross-validation, perhaps by extending SKO/TSO in more intelligent ways (see some interesting ideas by Eugene Dubossarsky here and here). Any comments/ideas are very welcome.\n","wordCount":"507","inLanguage":"en","datePublished":"2014-12-29T02:30:06Z","dateModified":"2014-12-29T02:30:06Z","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi | Data science and beyond","logo":{"@type":"ImageObject","url":"https://yanirs.github.io/yanirseroussi.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://yanirs.github.io/yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Stochastic Gradient Boosting: Choosing the Best Number of Iterations
</h1>
<div class=post-meta>December 29, 2014&nbsp;·&nbsp;Yanir Seroussi&nbsp;|&nbsp;<a href=https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2014-12-29-stochastic-gradient-boosting-choosing-the-best-number-of-iterations/index.md rel="noopener noreferrer" target=_blank>Suggest changes</a>
</div>
</header>
<div class=post-content><p>In my <a href=http://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/ title="Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)">summary of the Kaggle bulldozer price forecasting competition</a>, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated <a href=https://github.com/scikit-learn/scikit-learn/issues/1802 target=_blank rel=noopener>by a bug in scikit-learn</a>, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).</p>
<p>The bug in scikit-learn was <a href=https://github.com/scikit-learn/scikit-learn/pull/2188 target=_blank rel=noopener>fixed</a> by porting the solution used in <a href=http://cran.r-project.org/web/packages/gbm/index.html target=_blank rel=noopener>R&rsquo;s GBM package</a>, where the number of iterations is estimated by minimising the improvement on the OOB samples in each boosting iteration. This approach is known to <a href=http://cran.open-source-solution.org/web/packages/gbm/vignettes/gbm.pdf target=_blank rel=noopener>underestimate the number of required iterations</a>, which means that it&rsquo;s not very useful in practice. This underestimation may be due to the fact that the GBM method is partly estimated on in-bag samples, as the OOB samples for the Nth iteration are likely to have been in-bag in previous iterations.</p>
<p>I was curious about how my approach compares to the GBM method. Preliminary results on the <a href=http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html target=_blank rel=noopener>toy dataset from scikit-learn&rsquo;s documentation</a> looked promising:</p>
<figure>
<a href=gradient-boosting-out-of-bag-experiment-toy-dataset.png target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 720px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_360x0_resize_box_3.png 360w,
https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_480x0_resize_box_3.png 480w,
https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_720x0_resize_box_3.png 720w,
https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiment-toy-dataset.png 858w," src=https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_800x0_resize_box_3.png alt="Gradient Boosting out of bag experiment (toy dataset)" loading=lazy>
</a>
</figure>
<p>My approach (TSO) beat both 5-fold cross-validation (CV) and the GBM/scikit-learn method (SKO), as TSO obtains its minimum at the closest number of iterations to the test set&rsquo;s (T) optimal value.</p>
<p>The next step in testing TSO&rsquo;s viability was to rerun <a href=http://cran.open-source-solution.org/web/packages/gbm/vignettes/gbm.pdf target=_blank rel=noopener>Ridgeway&rsquo;s experiments from Section 3.3 of the GBM documentation</a> (<a href=https://github.com/harrysouthworth/gbm/blob/master/demo/OOB-reps.R target=_blank rel=noopener>R code here</a>). I used the same 12 UCI datasets that Ridgeway used, running 5×2 cross-validation on each one. For each dataset, the score was obtained by dividing the mean loss of the best method on the dataset by the loss of each method. Hence, all scores are between 0.0 and 1.0, with the best score being 1.0. The following figure summarises the results on the 12 datasets.</p>
<figure>
<a href=gradient-boosting-out-of-bag-experiments-uci-datasets target=_blank rel=noopener>
<img sizes="
          (min-width: 768px) 591px,
          100vw
        " srcset="https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiments-uci-datasets_hub650f2df69f9df4831910f3fa535d462_3872_360x0_resize_box_3.png 360w,
https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiments-uci-datasets_hub650f2df69f9df4831910f3fa535d462_3872_480x0_resize_box_3.png 480w,
https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiments-uci-datasets.png 591w," src=https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/gradient-boosting-out-of-bag-experiments-uci-datasets.png alt="Gradient Boosting out of bag experiment (UCI datasets)" loading=lazy>
</a>
</figure>
<p>The following table shows the raw data that was used to produce the figure.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>CV</th>
<th>SKO</th>
<th>TSO</th>
</tr>
</thead>
<tbody>
<tr>
<td>creditrating</td>
<td>0.9962</td>
<td>0.9771</td>
<td>1</td>
</tr>
<tr>
<td>breastcancer</td>
<td>1</td>
<td>0.6675</td>
<td>0.4869</td>
</tr>
<tr>
<td>mushrooms</td>
<td>0.9588</td>
<td>0.9963</td>
<td>1</td>
</tr>
<tr>
<td>abalone</td>
<td>1</td>
<td>0.9754</td>
<td>0.9963</td>
</tr>
<tr>
<td>ionosphere</td>
<td>0.9919</td>
<td>1</td>
<td>0.8129</td>
</tr>
<tr>
<td>diabetes</td>
<td>1</td>
<td>0.9869</td>
<td>0.9985</td>
</tr>
<tr>
<td>autoprices</td>
<td>1</td>
<td>0.9565</td>
<td>0.5839</td>
</tr>
<tr>
<td>autompg</td>
<td>1</td>
<td>0.8753</td>
<td>0.9948</td>
</tr>
<tr>
<td>bostonhousing</td>
<td>1</td>
<td>0.8299</td>
<td>0.5412</td>
</tr>
<tr>
<td>haberman</td>
<td>1</td>
<td>0.9793</td>
<td>0.9266</td>
</tr>
<tr>
<td>cpuperformance</td>
<td>0.9934</td>
<td>0.9160</td>
<td>1</td>
</tr>
<tr>
<td>adult</td>
<td>1</td>
<td>0.9824</td>
<td>0.9991</td>
</tr>
</tbody>
</table>
<p>The main finding is that CV remains the most reliable approach. Even when CV is not the best-performing method, it&rsquo;s not much worse than the best method (this is in line with Ridgeway&rsquo;s findings). TSO yielded the best results on 3/12 of the datasets, and beat SKO 7/12 times. However, TSO&rsquo;s results are the most variant of the three methods: when it fails, it often yields very poor results.</p>
<p>In conclusion, stick to cross-validation for the best results. It&rsquo;s more computationally intensive than SKO and TSO, but can be parallelised. I still think that there may be a way to avoid cross-validation, perhaps by extending SKO/TSO in more intelligent ways (see some interesting ideas by Eugene Dubossarsky <a href=http://cavemoosum.blogspot.com.au/2014/02/cross-validation-is-over-long-live.html target=_blank rel=noopener>here</a> and <a href=http://cavemoosum.blogspot.com.au/2014/03/cross-validation-is-not-quite-kaput-but.html target=_blank rel=noopener>here</a>). Any comments/ideas are very welcome.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/data-science/>data science</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/gradient-boosting/>gradient boosting</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/machine-learning/>machine learning</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/predictive-modelling/>predictive modelling</a></li>
<li><a href=https://yanirs.github.io/yanirseroussi.com/tags/scikit-learn/>scikit-learn</a></li>
</ul>
</footer><section class=comment-section>
<strong>No comments</strong>
<a class=comment-button href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirs.github.io%2fyanirseroussi.com%2f2014%2f12%2f29%2fstochastic-gradient-boosting-choosing-the-best-number-of-iterations%2f&body=<!-- Post your comment here and it may get added to the site -->" rel="noopener noreferrer" target=_blank>
Comment via GitHub issue
</a>
</section>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://yanirs.github.io/yanirseroussi.com/>Yanir Seroussi | Data science and beyond</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><div class=mailing-list-container>
<form class=mailing-list action=https://tinyletter.com/yanir method=post target=popupwindow onsubmit="return window.open('https://tinyletter.com/yanir','popupwindow','scrollbars=yes,width=800,height=600'),!0">
<label for=mailing-list-email>Get new post notifications</label>
<input type=text name=email id=mailing-list-email placeholder="Email address">
<input type=hidden value=1 name=embed>
<input type=submit value=Subscribe>
</form>
</div>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>