<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary) | Yanir Seroussi | Data science and beyond</title>
<meta name="keywords" content="data science, gradient boosting, Kaggle, Kaggle competition, predictive modelling, price forecasting, scikit-learn">
<meta name="description" content="Summary of a Kaggle competition to forecast bulldozer sale price, where I finished 9th out of 476 teams.">
<meta name="author" content="Yanir Seroussi">
<link rel="canonical" href="https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/">
<meta name="google-site-verification" content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU">
<link crossorigin="anonymous" href="/assets/css/stylesheet.532aaac5e1381d5800c7cbf01ec4a3ee56d87b989b9ddfa0a189e8c8e78dd55b.css" integrity="sha256-UyqqxeE4HVgAx8vwHsSj7lbYe5ibnd&#43;goYnoyOeN1Vs=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://yanirseroussi.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yanirseroussi.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yanirseroussi.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yanirseroussi.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://yanirseroussi.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)" />
<meta property="og:description" content="Summary of a Kaggle competition to forecast bulldozer sale price, where I finished 9th out of 476 teams." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/" />
<meta property="og:image" content="https://yanirseroussi.com/noisy-bulldozers.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2014-11-19T09:17:34+00:00" />
<meta property="article:modified_time" content="2023-07-06T09:28:02+10:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://yanirseroussi.com/noisy-bulldozers.jpg" />
<meta name="twitter:title" content="Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)"/>
<meta name="twitter:description" content="Summary of a Kaggle competition to forecast bulldozer sale price, where I finished 9th out of 476 teams."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yanirseroussi.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)",
      "item": "https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)",
  "name": "Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)",
  "description": "Summary of a Kaggle competition to forecast bulldozer sale price, where I finished 9th out of 476 teams.",
  "keywords": [
    "data science", "gradient boosting", "Kaggle", "Kaggle competition", "predictive modelling", "price forecasting", "scikit-learn"
  ],
  "articleBody": "Messy data, buggy software, but all in all a good learning experience...\nEarly last year, I had some free time on my hands, so I decided to participate in yet another Kaggle competition. Having never done any price forecasting work before, I thought it would be interesting to work on the Blue Book for Bulldozers competition, where the goal was to predict the sale price of auctioned bulldozers. I’ve done alright, finishing 9th out of 476 teams. And the experience did turn out to be interesting, but not for the reasons I expected.\nData and evaluation The competition dataset consists of about 425K historical records of bulldozer sales. The training subset consists of sales from the 1990s through to the end of 2011, with the validation and testing periods being January-April 2012 and May-November 2012 respectively. The goal is to predict the sale price of each bulldozer, given the sale date and venue, and the bulldozer’s features (e.g., model ID, mechanical specifications, and machine-specific data such as machine ID and manufacturing year). Submissions were scored using the RMSLE measure.\nEarly in the competition (before I joined), there were many posts in the forum regarding issues with the data. The organisers responded by posting an appendix to the data, which included the “correct” information. From people’s posts after the competition ended, it seems like using the “correct” data consistently made the results worse. Luckily, I discovered this about a week before the competition ended. Reducing my reliance on the appendix made a huge difference in the performance of my models. This discovery was thanks to a forum post, which illustrates the general point on the importance of monitoring the forum in Kaggle competitions.\nMy approach: feature engineering, data splitting, and stochastic gradient boosting Having read the forum discussions on data quality, I assumed that spending time on data cleanup and feature engineering would give me an edge over competitors who focused only on data modelling. It’s well-known that simple models fitted on more/better data tend to yield better results than complex models fitted on less/messy data (aka GIGO – garbage in, garbage out). However, doing data cleaning and feature engineering is less glamorous than building sophisticated models, which is why many people avoid the former.\nSadly, the data was incredibly messy, so most of my cleanup efforts resulted in no improvements. Even intuitive modifications yielded poor results, like transforming each bulldozer’s manufacturing year into its age at the time of sale. Essentially, to do well in this competition, one had to fit the noise rather than remove it. This was rather disappointing, as one of the nice things about Kaggle competitions is being able to work on relatively clean data. Anomalies in data included bulldozers that have been running for hundreds of years and machines that got sold years before they were manufactured (impossible for second-hand bulldozers!). It is obvious that Fast Iron (the company who sponsored the competition) would have obtained more usable models from this competition if they had spent more time cleaning up the data themselves.\nThroughout the competition I went through several iterations of modelling and data cleaning. My final submission ended up being a linear combination of four models:\nGradient boosting machine (GBM) regression on the full dataset A linear model on the full dataset An ensemble of GBMs, one for each product group (rationale: different product groups represent different bulldozer classes, like track excavators and motor graders, so their prices are not really comparable) A similar ensemble, where each product group and sale year has a separate GBM, and earlier years get lower weight than more recent years I ended up discarding old training data (before 2000) and the machine IDs (another surprise: even though some machines were sold multiple times, this information was useless). For the GBMs, I treated categorical features as ordinal, which sort of makes sense for many of the features (e.g., model series values are ordered). For the linear model, I just coded them as binary indicators.\nThe most important discovery: stochastic gradient boosting bugs This was the first time I used gradient boosting. Since I was using so many different models, it was hard to reliably tune the number of trees, so I figured I’d use stochastic gradient boosting and rely on out-of-bag (OOB) samples to set the number of trees. This led to me finding a bug in scikit-learn: the OOB scores were actually calculated on in-bag samples.\nI reported the issue to the maintainers of scikit-learn and made an attempt at fixing it by skipping trees to obtain the OOB samples. This yielded better results than the buggy version, and in some cases I replaced a plain GBM with an ensemble of four stochastic GBMs with subsample ratio of 0.5 and a different random seed for each one (averaging their outputs).\nThis wasn’t enough to convince the maintainers of scikit-learn to accept the pull request with my fix, as they didn’t like my idea of skipping trees. This is for a good reason — obtaining better results on a single dataset should be insufficient to convince anyone. They ended up fixing the issue by copying the implementation from R’s GBM package, which is known to underestimate the number of required trees/boosting iterations (see Section 3.3 in the GBM guide).\nRecently, I had some time to test my tree skipping idea on the toy dataset used in the scikit-learn documentation. As the following figure shows, a smoothed variant of my tree skipping idea (TSO in the figure) yields superior results to the scikit-learn/R approach (SKO in the figure). The actual loss doesn’t matter — what matters is where it’s minimised. In this case TSO obtains the closest approximation of the number of iterations to the value that minimises the test error, which is a promising result.\nThese results are pretty cool, but this is still just a toy dataset (though repeating the experiment with 100 different random seeds to generate different toy datasets yields similar results). The next steps would be to repeat Ridgeway’s experiments from the GBM guide on multiple datasets to see whether the results generalise well, which will be the topic of a different post. Regardless of the final outcomes, this story illustrates the unexpected paths in which a Kaggle competition can take you. No matter what rank you end up obtaining and regardless of your skill level, there’s always something new to learn.\nUpdate: I ran Ridgway’s experiments. The results are discussed in Stochastic Gradient Boosting: Choosing the Best Number of Iterations.\n",
  "wordCount" : "1087",
  "inLanguage": "en",
  "image":"https://yanirseroussi.com/noisy-bulldozers.jpg","datePublished": "2014-11-19T09:17:34Z",
  "dateModified": "2023-07-06T09:28:02+10:00",
  "author":{
    "@type": "Person",
    "name": "Yanir Seroussi"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Yanir Seroussi | Data science and beyond",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yanirseroussi.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yanirseroussi.com/" accesskey="h" title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yanirseroussi.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://yanirseroussi.com/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="https://yanirseroussi.com/faq/" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)
    </h1>
    <div class="post-meta"><span title='2014-11-19 09:17:34 +0000 UTC'>November 19, 2014</span>&nbsp;|&nbsp;<a href="https://github.com/yanirs/yanirseroussi.com/blob/master/content/posts/2014-11-19-fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/index.md" rel="noopener noreferrer" target="_blank">Suggest changes</a>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="lazy" srcset="https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/noisy-bulldozers_hu766b19432f2e7b969d67fa48688a7a26_267258_360x0_resize_q75_box.jpg 360w ,https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/noisy-bulldozers_hu766b19432f2e7b969d67fa48688a7a26_267258_480x0_resize_q75_box.jpg 480w ,https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/noisy-bulldozers_hu766b19432f2e7b969d67fa48688a7a26_267258_720x0_resize_q75_box.jpg 720w ,https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/noisy-bulldozers.jpg 800w" 
            sizes="(min-width: 768px) 720px, 100vw" src="https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/noisy-bulldozers.jpg" alt="" 
            width="800" height="261">
        
</figure>
  <div class="post-content"><p class="intro-note">Messy data, buggy software, but all in all a good learning experience...</p>
<p>Early last year, I had some free time on my hands, so I decided to participate in yet another Kaggle competition. Having never done any price forecasting work before, I thought it would be interesting to work on the <a href="https://www.kaggle.com/c/bluebook-for-bulldozers" target="_blank" rel="noopener">Blue Book for Bulldozers competition</a>, where the goal was to predict the sale price of auctioned bulldozers. I&rsquo;ve done alright, finishing 9th out of 476 teams. And the experience did turn out to be interesting, but not for the reasons I expected.</p>
<h3 id="data-and-evaluation">Data and evaluation<a hidden class="anchor" aria-hidden="true" href="#data-and-evaluation">#</a></h3>
<p>The competition dataset consists of about 425K historical records of bulldozer sales. The training subset consists of sales from the 1990s through to the end of 2011, with the validation and testing periods being January-April 2012 and May-November 2012 respectively. The goal is to predict the sale price of each bulldozer, given the sale date and venue, and the bulldozer&rsquo;s features (e.g., model ID, mechanical specifications, and machine-specific data such as machine ID and manufacturing year). Submissions were scored using the <a href="http://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError" target="_blank" rel="noopener">RMSLE measure</a>.</p>
<p>Early in the competition (before I joined), there were many posts in the forum regarding issues with the data. The organisers responded by posting an appendix to the data, which included the &ldquo;correct&rdquo; information. From people&rsquo;s posts after the competition ended, it seems like using the &ldquo;correct&rdquo; data consistently made the results <strong>worse</strong>. Luckily, I discovered this about a week before the competition ended. Reducing my reliance on the appendix made a huge difference in the performance of my models. This discovery was thanks to a forum post, which illustrates the <a href="https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/" title="How to (almost) win Kaggle competitions - Tip 9">general point on the importance of monitoring the forum in Kaggle competitions</a>.</p>
<h3 id="my-approach-feature-engineering-data-splitting-and-stochastic-gradient-boosting">My approach: feature engineering, data splitting, and stochastic gradient boosting<a hidden class="anchor" aria-hidden="true" href="#my-approach-feature-engineering-data-splitting-and-stochastic-gradient-boosting">#</a></h3>
<p>Having read the forum discussions on data quality, I assumed that spending time on data cleanup and feature engineering would give me an edge over competitors who focused only on data modelling. It&rsquo;s well-known that simple models fitted on more/better data tend to yield better results than complex models fitted on less/messy data (aka GIGO – garbage in, garbage out). However, doing data cleaning and feature engineering is less glamorous than building sophisticated models, which is why many people avoid the former.</p>
<p>Sadly, the data was incredibly messy, so most of my cleanup efforts resulted in no improvements. Even intuitive modifications yielded poor results, like transforming each bulldozer&rsquo;s manufacturing year into its age at the time of sale. Essentially, to do well in this competition, one had to fit the noise rather than remove it. This was rather disappointing, as one of the nice things about Kaggle competitions is being able to work on relatively clean data. Anomalies in data included bulldozers that have been running for hundreds of years and machines that got sold years before they were manufactured (impossible for second-hand bulldozers!). It is obvious that Fast Iron (the company who sponsored the competition) would have obtained more usable models from this competition if they had spent more time cleaning up the data themselves.</p>
<p>Throughout the competition I went through several iterations of modelling and data cleaning. My final submission ended up being a linear combination of four models:</p>
<ul>
<li><a href="http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting" target="_blank" rel="noopener">Gradient boosting machine</a> (GBM) regression on the full dataset</li>
<li>A linear model on the full dataset</li>
<li>An ensemble of GBMs, one for each product group (rationale: different product groups represent different bulldozer classes, like track excavators and motor graders, so their prices are not really comparable)</li>
<li>A similar ensemble, where each product group and sale year has a separate GBM, and earlier years get lower weight than more recent years</li>
</ul>
<p>I ended up discarding old training data (before 2000) and the machine IDs (another surprise: even though some machines were sold multiple times, this information was useless). For the GBMs, I treated categorical features as ordinal, which sort of makes sense for many of the features (e.g., model series values are ordered). For the linear model, I just coded them as binary indicators.</p>
<h3 id="the-most-important-discovery-stochastic-gradient-boosting-bugs">The most important discovery: stochastic gradient boosting bugs<a hidden class="anchor" aria-hidden="true" href="#the-most-important-discovery-stochastic-gradient-boosting-bugs">#</a></h3>
<p>This was the first time I used gradient boosting. Since I was using so many different models, it was hard to reliably tune the number of trees, so I figured I&rsquo;d use stochastic gradient boosting and rely on out-of-bag (OOB) samples to set the number of trees. This led to me finding a bug in <a href="http://scikit-learn.org" target="_blank" rel="noopener">scikit-learn</a>: the OOB scores were actually calculated on in-bag samples.</p>
<p>I <a href="https://github.com/scikit-learn/scikit-learn/issues/1802" target="_blank" rel="noopener">reported the issue</a> to the maintainers of scikit-learn and made an attempt at fixing it by skipping trees to obtain the OOB samples. This yielded better results than the buggy version, and in some cases I replaced a plain GBM with an ensemble of four stochastic GBMs with subsample ratio of 0.5 and a different random seed for each one (averaging their outputs).</p>
<p>This wasn&rsquo;t enough to convince the maintainers of scikit-learn to accept <a href="https://github.com/scikit-learn/scikit-learn/pull/1806" target="_blank" rel="noopener">the pull request with my fix</a>, as they didn&rsquo;t like my idea of skipping trees. This is for a good reason — obtaining better results on a single dataset should be insufficient to convince anyone. They ended up fixing the issue by copying the implementation from R&rsquo;s GBM package, which is known to underestimate the number of required trees/boosting iterations (see <a href="http://cran.open-source-solution.org/web/packages/gbm/vignettes/gbm.pdf" target="_blank" rel="noopener">Section 3.3 in the GBM guide</a>).</p>
<p>Recently, I had some time to test my tree skipping idea on the toy dataset used in <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html" target="_blank" rel="noopener">the scikit-learn documentation</a>. As the following figure shows, a smoothed variant of my tree skipping idea (TSO in the figure) yields superior results to the scikit-learn/R approach (SKO in the figure). The actual loss doesn&rsquo;t matter — what matters is where it&rsquo;s minimised. In this case TSO obtains the closest approximation of the number of iterations to the value that minimises the test error, which is a promising result.</p>













  
    
      
    
  
    
      
    
  
    
      
    
  
    
  
    
  
  
    
  
  
  



<figure>
  <a href="gradient-boosting-out-of-bag-experiment-toy-dataset.png" target="_blank" rel="noopener">
    <img
      
        sizes="
          (min-width: 768px) 720px,
          100vw
        "
        srcset="
          
            
              https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_360x0_resize_box_3.png 360w,
            
          
            
              https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_480x0_resize_box_3.png 480w,
            
          
            
              https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_720x0_resize_box_3.png 720w,
            
          
            
              https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/gradient-boosting-out-of-bag-experiment-toy-dataset.png 858w,
            
          
        "
        
        
        
          src="https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/gradient-boosting-out-of-bag-experiment-toy-dataset_hu02dc1ebe47af12a7ec8f5877429b5dec_71277_800x0_resize_box_3.png"
        
      
        alt="Gradient Boosting out of bag experiment (toy dataset)"loading="lazy"
    />
  </a>
</figure>

<p>These results are pretty cool, but this is still just a toy dataset (though repeating the experiment with 100 different random seeds to generate different toy datasets yields similar results). The next steps would be to repeat Ridgeway&rsquo;s experiments from the GBM guide on multiple datasets to see whether the results generalise well, which will be the topic of a different post. Regardless of the final outcomes, this story illustrates the unexpected paths in which a Kaggle competition can take you. No matter what rank you end up obtaining and regardless of your skill level, there&rsquo;s always something new to learn.</p>
<p><strong>Update:</strong> I ran Ridgway&rsquo;s experiments. The results are discussed in <a href="https://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/" title="Stochastic Gradient Boosting: Choosing the Best Number of Iterations">Stochastic Gradient Boosting: Choosing the Best Number of Iterations</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://yanirseroussi.com/tags/data-science/">data science</a></li>
      <li><a href="https://yanirseroussi.com/tags/gradient-boosting/">gradient boosting</a></li>
      <li><a href="https://yanirseroussi.com/tags/kaggle/">Kaggle</a></li>
      <li><a href="https://yanirseroussi.com/tags/kaggle-competition/">Kaggle competition</a></li>
      <li><a href="https://yanirseroussi.com/tags/predictive-modelling/">predictive modelling</a></li>
      <li><a href="https://yanirseroussi.com/tags/price-forecasting/">price forecasting</a></li>
      <li><a href="https://yanirseroussi.com/tags/scikit-learn/">scikit-learn</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary) on twitter"
        href="https://twitter.com/intent/tweet/?text=Fitting%20noise%3a%20Forecasting%20the%20sale%20price%20of%20bulldozers%20%28Kaggle%20competition%20summary%29&amp;url=https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f&amp;hashtags=datascience%2cgradientboosting%2cKaggle%2cKagglecompetition%2cpredictivemodelling%2cpriceforecasting%2cscikit-learn">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary) on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f&amp;title=Fitting%20noise%3a%20Forecasting%20the%20sale%20price%20of%20bulldozers%20%28Kaggle%20competition%20summary%29&amp;summary=Fitting%20noise%3a%20Forecasting%20the%20sale%20price%20of%20bulldozers%20%28Kaggle%20competition%20summary%29&amp;source=https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary) on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f&title=Fitting%20noise%3a%20Forecasting%20the%20sale%20price%20of%20bulldozers%20%28Kaggle%20competition%20summary%29">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary) on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary) on whatsapp"
        href="https://api.whatsapp.com/send?text=Fitting%20noise%3a%20Forecasting%20the%20sale%20price%20of%20bulldozers%20%28Kaggle%20competition%20summary%29%20-%20https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary) on telegram"
        href="https://telegram.me/share/url?text=Fitting%20noise%3a%20Forecasting%20the%20sale%20price%20of%20bulldozers%20%28Kaggle%20competition%20summary%29&amp;url=https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer><section class="comment-section">
  

  <strong>No comments</strong>
  <a
    class="comment-button"
    href="https://github.com/yanirs/yanirseroussi.com/issues/new?title=New comment on https%3a%2f%2fyanirseroussi.com%2f2014%2f11%2f19%2ffitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary%2f&body=<!-- Post your comment here and it may get added to the site -->"
    rel="noopener noreferrer"
    target="_blank"
  >
    Comment via GitHub issue
  </a>

  
  
  
  

  
</section>



</article>
    </main>
    
<footer class="footer">
    <span>Text and figures licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="noopener">CC BY-NC-ND 4.0</a> by <a href="https://yanirseroussi.com/about/">Yanir Seroussi</a>, except where noted otherwise  |</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer><div class="mailing-list-container">
  <form
      class="mailing-list"
      action="https://yanirseroussi.us17.list-manage.com/subscribe/post?u=3c08aa3ff27dd92978019febd&amp;id=bc3ab705af"
      method="post"
      target="_blank"
      novalidate
  >
    <label for="mailing-list-email">Get new post notifications</label>
    <input type="text" name="EMAIL" id="mailing-list-email" placeholder="Email address" />
    <div style="position: absolute; left: -5000px;" aria-hidden="true">
      <input type="text" name="b_3c08aa3ff27dd92978019febd_bc3ab705af" tabindex="-1" value="" />
    </div>
    <input type="submit" value="Subscribe" />
  </form>

  <div class="footer">
    Alternatively, <a href="https://github.com/yanirs/yanirseroussi.com" rel="noopener" target="_blank">watch on GitHub</a>
    or <a href="https://yanirseroussi.com/index.xml">subscribe to RSS feed</a>.
  </div>
</div>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
