<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Beyond good vibes: Securing AI agents by design | Yanir Seroussi – AI/ML Engineering Consultant</title><meta name=keywords content="artificial intelligence,security,software engineering"><meta name=description content="Summary of my talk on the paper &ldquo;Design Patterns for Securing LLM Agents against Prompt Injections&rdquo;."><meta name=author content="Yanir Seroussi"><link rel=canonical href=https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/><meta name=google-site-verification content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU"><link crossorigin=anonymous href=/assets/css/stylesheet.d9b9873b5f5f328a87bad89f89301b8212fc8c8f1d4421d468cc7148aa2fcc2f.css integrity="sha256-2bmHO19fMoqHutifiTAbghL8jI8dRCHUaMxxSKovzC8=" rel="preload stylesheet" as=style><link rel=icon href=https://yanirseroussi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yanirseroussi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yanirseroussi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yanirseroussi.com/apple-touch-icon.png><link rel=mask-icon href=https://yanirseroussi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/"><meta property="og:site_name" content="Yanir Seroussi – AI/ML Engineering Consultant"><meta property="og:title" content="Beyond good vibes: Securing AI agents by design"><meta property="og:description" content="Summary of my talk on the paper “Design Patterns for Securing LLM Agents against Prompt Injections”."><meta property="og:locale" content="en-au"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-08T04:00:00+00:00"><meta property="article:modified_time" content="2025-08-08T14:17:43+10:00"><meta property="article:tag" content="Artificial Intelligence"><meta property="article:tag" content="Security"><meta property="article:tag" content="Software Engineering"><meta property="og:image" content="https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding.webp"><meta name=twitter:title content="Beyond good vibes: Securing AI agents by design"><meta name=twitter:description content="Summary of my talk on the paper &ldquo;Design Patterns for Securing LLM Agents against Prompt Injections&rdquo;."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Browse Posts","item":"https://yanirseroussi.com/posts/"},{"@type":"ListItem","position":2,"name":"Beyond good vibes: Securing AI agents by design","item":"https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Beyond good vibes: Securing AI agents by design","name":"Beyond good vibes: Securing AI agents by design","description":"Summary of my talk on the paper \u0026ldquo;Design Patterns for Securing LLM Agents against Prompt Injections\u0026rdquo;.","keywords":["artificial intelligence","security","software engineering"],"articleBody":"Amidst the buzz about new AI models and features, there’s also a constant stream of news about prompt injection attacks. And the victims of those attacks aren’t just scrappy new startups. Even established players like Microsoft and Atlassian have released vulnerable AI software in recent months.\nOn an individual level, even seasoned engineers such as Steve Yegge can fall victim to complacency when using AI, as shown by this story (emphasis mine):\nSo one of my favorite things to do is give my coding agents more and more permissions and freedom, just to see how far I can push their productivity without going too far off the rails. It’s a delicate balance. I haven’t given them direct access to my bank account yet. But I did give one access to my Google Cloud production instances and systems. And it promptly wiped a production database password and locked my network.\nPut together, this means that as of 2025, we can’t trust AI agents to act securely, no matter how strict our prompts are.\nTo dive deeper into this topic, I recently led a discussion at the Brisbane AI Paper Reading group around Design Patterns for Securing LLM Agents against Prompt Injections. My slides are here. This post follows the same flow as the slides, covering key concepts, the design patterns from the paper, and a recommended activity to go deep into several case studies.\nKey concepts When talking about AI agent security, we should start by defining AI agents. I like the definition from Anthropic: AI agents are models using tools in a loop. We are essentially dealing with tokens in / tokens out machines. Tokens are numbers that represent parts of words (or other numerical representations of inputs and outputs, such as images). Current models accept an input as a sequence of tokens and produce an output as a sequence of tokens, based on complex mathematical functions. And therein lies the problem: We can’t know for certain what tokens are going to come out of the model.\nThe paper defines prompt injections as attacks that “occur when adversaries introduce instructions into the content processed by an LLM, causing it to deviate from its intended behavior. Most of these attacks insert malicious instructions to an otherwise benign user prompt, analogously to an SQL injection (thus, the name).”\nSimilarly to SQL injections, prompt injections can be direct (i.e., done by a malicious or compromised user) or indirect (i.e., inserted into the prompt from documents planted by the attacker). However, there is a key difference between SQL and prompt injections: In SQL, escaping inputs works deterministically, so we can defend from injections by carefully manipulating inputs to the SQL engine. As current AI models accept token streams as inputs and output probabilistic token streams, we cannot defend from prompt injections only by manipulating the inputs. That is, effective defences against prompt injections have to be designed on a system level, outside the models.\nIn the early days of the post-ChatGPT AI wave, prompt injection risks were rather benign – typically limited to embarrassing outputs. These days, AI agents are given access to tools, which increases their utility – but also increases potential harm. A great way to conceptualise the potential harm is through what Simon Willison called the lethal trifecta for AI agents:\nAccess to your private data. Exposure to untrusted content. Ability to communicate externally. Many recent vulnerabilities are due to the combination of these three factors in a single agent. For example, you can build an agent that: (1) reads your emails (access to private data); and (2) sends responses (ability to communicate externally). But since your emails contain untrusted content (as anyone can email you), this opens the door to your private messages getting exfiltrated via prompt injections.\nThis leads to a simple rule of thumb that both users and AI engineers can apply: Avoid the lethal trifecta! Users should be extra careful with all the shiny new agent products that require broad access, while engineers should design systems that avoid mixing the three elements in a single agent (e.g., by following the design patterns described below).\nAvoiding the lethal trifecta is an application of the principle of least privilege: any entity must only be granted access to the resources it needs to complete its tasks. Related to this are the decisions we make on which entities we trust to perform specific tasks.\nTo drive this point home, ChatGPT and I designed the image in the following slide. Despite my repeated requests, it just wouldn’t spell “wouldn’t” correctly, which serves as a great example of why we should limit our trust of current AI models. While they exhibit greater intelligence than most humans on many fronts, they still make mistakes that intelligent humans wouldn’t make.\nThe last two key concepts are threat modelling and the balance of security versus utility.\nThe Threat Modelling Manifesto provides the four key questions to ask when modelling threats:\nWhat are we working on? What can go wrong? What are we going to do about it? Did we do a good enough job? When answering these questions, we aim to balance security and utility based on the analysed threats. For example, if we designed a house with thick concrete walls and no doors or windows, its security would be high, but its utility would be low. In most places, houses with lockable doors and windows represent a good balance between security and utility: It isn’t too cumbersome to lock all entry points and carry keys, and the level of security is reasonable for the likely threat of opportunistic break-ins. The same goes with AI agents: The level of acceptable risk depends on the threats and data we’re dealing with, e.g., when I share data with an online chatbot, I accept the risk that it may leak at some point.\nDesign patterns and best practices While the key concepts above come from a variety of sources, the best source for understanding the design patterns introduced in the paper is the paper itself. Therefore, I don’t go into too much detail here. Instead, this section serves as a quick reference to the six patterns.\nPattern 1: Action selector. The agent translates incoming requests to pre-defined actions, e.g., a web “chatbot” that responds with links, canned responses, and actual actions the user can already access. This means it can’t do anything more harmful than what the user can do, but it is essentially the same as a dropdown menu with a natural language interface. It’s effective against indirect prompt injection and potentially against direct injection, as there is nothing to gain from injection by a malicious user.\nPattern 2: Plan then execute. The agent defines a plan and can’t deviate from it based on the untrusted data, e.g., an input like “send today’s schedule to John” becomes calendar.read(today); email.write(..., \"john@company\"). This doesn’t prevent direct prompt injections, and doesn’t protect from indirect prompt injections that affect the contents of the email. Whether this pattern is good enough depends on the threat model.\nPattern 3: LLM map-reduce. Only isolated agents (the mappers) interact with untrusted data. Their outputs are restricted to prevent prompt injections (e.g., using regular expressions or strict typing). For example, when handling a request like “find all files that contain an invoice and email me the list of filenames”, the main agent doesn’t have access to the contents of the files. Instead, it invokes isolated mapper agents that read each file and return a boolean. The final reduce operation produces a safe list based on the trusted boolean inputs (assuming filenames are trusted).\nPattern 4: Dual LLM. This is a more general version of the map-reduce pattern, where the privileged LLM doesn’t see the untrusted data. The quarantined LLM sees the data, but only communicates about it using symbols. For example, when handling a request like “find email addresses in this (potentially untrusted) document”, the quarantined LLM sees the contents of the document and returns variables with the email addresses (which match strict patterns). The privileged LLM only sees the trusted variable names, so it’s not affected by the contents of the document.\nPattern 5: Code then execute. This is a generalisation of the previous three patterns, where the main agent writes a formal computer program that isolates untrusted input processing to unprivileged agents. The implementation isn’t trivial: this pattern comes from the 125-page paper that introduced the CaMeL interpreter (“a custom Python interpreter that operates over a restricted version of Python”). I haven’t read the full CaMeL paper, but my understanding is that it requires defining specific security policies to be fully effective. Therefore, practical implementations depend on the use case and threat model.\nPattern 6: Context minimization. Like Simon Willison, I found the description of this pattern to be a bit confusing. My understanding is that in this case the prompt may be malicious, so it’s removed from the tool response. However, the example given in the paper seems a bit convoluted, as it includes prompt-injecting a customer service agent to give the user a quote with a large discount. This assumes a company would go be willing to go with whatever discounts given by its bots. This is a scenario that should be handled via other means (like deterministic guardrails), as we’re not yet at the point where we can trust AI agents in customer-facing roles.\nGeneral best practices. The authors emphasise that while different design patterns apply based on the use case, the following best practices should apply to any agent to the extent possible.\nAction sandboxing: Applying the principle of least privilege in practice, by running specific agent actions in sandboxed environments with limited permissions. Strict data formatting: Using deterministic code to ensure that expected schema rules are followed, rather than allowing arbitrary text. User permissions: Limiting the agent’s permissions to those of the authenticated user, so it can only do what the user can do. This reduces the potential harm from prompt injection attacks. User confirmation: Asking the user before running actions that are potentially harmful. While this is useful in some scenarios, it increases friction and reduces automation. Further, excessive requests for confirmation increase the chance of users ignoring the content of the confirmation requests (as most of us do when asked to read the terms of service). Data and action attribution: Providing explanations to the user on the reasoning of the agent (though this may also lead to user fatigue). All the design pattern slides in one image. Case study activity Consuming information about design patterns only takes you so far. It’s already better than vibe coding and hoping for the best, but applying patterns in reality is the way to truly learn them.\nTo simulate this at the AI paper event, I split the attendees to groups. I then assigned each group one of the case studies discussed in the paper (§4.2 data analyst, §4.3 email and calendar assistant, §4.4 customer service representative, §4.5 booking agent, and §4.6 product recommender), and prompted each group with the following tasks.\nDefine the capabilities and scope of the agent: What can a human agent do? Identify the threats, focusing on direct and indirect prompt injections. Discuss mitigations using the patterns and best practices. Present the approach that best balances security and utility. I was pleased with the lively discussion that this activity sparked! 🎉\nYou can also try this at home: Discuss the above points with colleagues or with your favourite chatbot. Prompted well, they can be surprisingly effective at poking holes in your thinking. Just don’t trust them with unbounded access to your key accounts (yet).\n","wordCount":"1936","inLanguage":"en","image":"https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding.webp","datePublished":"2025-08-08T04:00:00Z","dateModified":"2025-08-08T14:17:43+10:00","author":{"@type":"Person","name":"Yanir Seroussi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/"},"publisher":{"@type":"Organization","name":"Yanir Seroussi – AI/ML Engineering Consultant","logo":{"@type":"ImageObject","url":"https://yanirseroussi.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yanirseroussi.com/ accesskey=h title="Yanir Seroussi – AI/ML Engineering Consultant (Alt + H)">Yanir Seroussi – AI/ML Engineering Consultant</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button">
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://yanirseroussi.com/about/ title=About><span>About</span></a></li><li><a href=https://yanirseroussi.com/posts/ title=Writing><span>Writing</span></a></li><li><a href=https://yanirseroussi.com/talks/ title=Speaking><span>Speaking</span></a></li><li><a href=https://yanirseroussi.com/consult/ title=Consulting><span>Consulting</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Beyond good vibes: Securing AI agents by design</h1><div class=post-meta><span title='2025-08-08 04:00:00 +0000 UTC'>August 8, 2025</span></div></header><figure class=entry-cover><img loading=eager srcset='https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding_hu_98fac2131e698a63.webp 360w,https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding_hu_31322218ef79ab4a.webp 480w,https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding_hu_6740c8822eac934c.webp 720w,https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding_hu_81c93bc49c08f030.webp 1080w,https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding.webp 1200w' src=https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-security-versus-vibe-coding.webp sizes="(min-width: 768px) 720px, 100vw" width=1200 height=630 alt="Gemini-generated illustration of serious AI security versus hacky vibe coding"></figure><div class=post-content><p>Amidst the buzz about new AI models and features, there&rsquo;s also a constant stream of news about prompt injection attacks.
And the victims of those attacks aren&rsquo;t just scrappy new startups.
Even established players like <a href=https://www.aim.security/lp/aim-labs-echoleak-blogpost target=_blank rel=noopener>Microsoft</a> and <a href=https://www.catonetworks.com/blog/cato-ctrl-poc-attack-targeting-atlassians-mcp/ target=_blank rel=noopener>Atlassian</a> have released vulnerable AI software in recent months.</p><p>On an individual level, even seasoned engineers such as <a href=https://en.wikipedia.org/wiki/Steve_Yegge target=_blank rel=noopener>Steve Yegge</a> can fall victim to complacency when using AI, as shown by <a href=https://x.com/steve_yegge/status/1946360175339974807 target=_blank rel=noopener>this story</a> (emphasis mine):</p><blockquote><p>So one of my favorite things to do is give my coding agents more and more permissions and freedom, just to see how far I can push their productivity without going too far off the rails. It&rsquo;s a delicate balance. I haven&rsquo;t given them direct access to my bank account yet. But I did give one access to my Google Cloud production instances and systems. <strong>And it promptly wiped a production database password and locked my network.</strong></p></blockquote><p>Put together, this means that as of 2025, <strong>we can&rsquo;t trust AI agents to act securely, no matter how strict our prompts are.</strong></p><p>To dive deeper into this topic, I recently led a discussion at <a href=https://events.humanitix.com/august-event-ai-paper-reading target=_blank rel=noopener>the Brisbane AI Paper Reading group</a> around <a href=https://arxiv.org/abs/2506.08837 target=_blank rel=noopener>Design Patterns for Securing LLM Agents against Prompt Injections</a>.
My slides are <a href=https://docs.google.com/presentation/d/1mrrcKgehO4HZD6YPrxOu7_keQf5Jm11NqQEZMzw-iwI/edit target=_blank rel=noopener>here</a>.
This post follows the same flow as the slides, covering key concepts, the design patterns from the paper, and a recommended activity to go deep into several case studies.</p><h2 id=key-concepts>Key concepts<a hidden class=anchor aria-hidden=true href=#key-concepts>#</a></h2><p>When talking about AI agent security, we should start by defining AI agents.
I like <a href=https://x.com/eugeneyan/status/1894569714070032559 target=_blank rel=noopener>the definition from Anthropic</a>: <strong>AI agents are models using tools in a loop.</strong>
We are essentially dealing with <strong>tokens in / tokens out machines</strong>.
Tokens are numbers that represent parts of words (or other numerical representations of inputs and outputs, such as images).
Current models accept an input as a sequence of tokens and produce an output as a sequence of tokens, based on complex mathematical functions.
And therein lies the problem: <strong>We can&rsquo;t know for certain what tokens are going to come out of the model.</strong></p><p><strong>The paper defines prompt injections</strong> as attacks that <em>&ldquo;occur when adversaries introduce instructions into the content processed by an LLM, causing it to deviate from its intended behavior. Most of these attacks insert malicious instructions to an otherwise benign user prompt, analogously to an SQL injection (thus, the name).&rdquo;</em></p><p>Similarly to SQL injections, prompt injections can be direct (i.e., done by a malicious or compromised user) or indirect (i.e., inserted into the prompt from documents planted by the attacker).
However, there is a key difference between SQL and prompt injections: In SQL, escaping inputs works deterministically, so we can defend from injections by carefully manipulating inputs to the SQL engine.
As current AI models accept token streams as inputs and output <em>probabilistic</em> token streams, <strong>we cannot defend from prompt injections only by manipulating the inputs.</strong>
That is, <strong>effective defences against prompt injections have to be designed on a system level, outside the models</strong>.</p><p>In the early days of the post-ChatGPT AI wave, prompt injection risks were rather benign – typically limited to embarrassing outputs.
These days, AI agents are given access to tools, which increases their utility – but also increases potential harm.
A great way to conceptualise the potential harm is through <a href=https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/ target=_blank rel=noopener>what Simon Willison called</a> <strong>the lethal trifecta for AI agents:</strong></p><ol><li>Access to your private data.</li><li>Exposure to untrusted content.</li><li>Ability to communicate externally.</li></ol><p>Many recent vulnerabilities are due to the combination of these three factors in a single agent.
For example, you can build an agent that: (1) reads your emails (access to private data); and (2) sends responses (ability to communicate externally).
But since your emails contain untrusted content (as anyone can email you), this opens the door to your private messages getting exfiltrated via prompt injections.</p><p>This leads to a simple rule of thumb that both users and AI engineers can apply: <strong>Avoid the lethal trifecta!</strong>
Users should be extra careful with all the shiny new agent products that require broad access, while engineers should design systems that avoid mixing the three elements in a single agent (e.g., by following the design patterns described below).</p><figure><a href=ai-agent-lethal-trifecta-venn-diagram.webp target=_blank rel=noopener><img sizes="(min-width: 768px) 720px,
100vw" srcset="https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-agent-lethal-trifecta-venn-diagram_hu_2f38b46f07935e3e.webp 360w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-agent-lethal-trifecta-venn-diagram_hu_579e6f8451fd693b.webp 480w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-agent-lethal-trifecta-venn-diagram_hu_a33cabf8503dbae7.webp 720w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-agent-lethal-trifecta-venn-diagram_hu_e77798cddd0e1a8a.webp 1080w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-agent-lethal-trifecta-venn-diagram.webp 1326w," src=https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/ai-agent-lethal-trifecta-venn-diagram_hu_75ad6222d952de.webp alt="Venn diagram of the AI agent lethal trifecta (described in the text)" loading=lazy></a></figure><p>Avoiding the lethal trifecta is an application of <strong><a href=https://en.wikipedia.org/wiki/Principle_of_least_privilege target=_blank rel=noopener>the principle of least privilege</a>:</strong> any entity must only be granted access to the resources it needs to complete its tasks.
Related to this are the decisions we make on which entities we trust to perform specific tasks.</p><p>To drive this point home, ChatGPT and I designed the image in the following slide.
Despite my repeated requests, it just wouldn&rsquo;t spell &ldquo;wouldn&rsquo;t&rdquo; correctly, which serves as a great example of why we should limit our trust of current AI models.
While they exhibit greater intelligence than most humans on many fronts, they still make mistakes that intelligent humans wouldn&rsquo;t make.</p><figure><a href=least-privilege-for-life.webp target=_blank rel=noopener><img sizes="(min-width: 768px) 720px,
100vw" srcset="https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/least-privilege-for-life_hu_b4aa0b4c7c872fd7.webp 360w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/least-privilege-for-life_hu_2ac9e36a53443e50.webp 480w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/least-privilege-for-life_hu_f4619999eeaa7378.webp 720w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/least-privilege-for-life_hu_75dc35d1fe716bcd.webp 1080w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/least-privilege-for-life_hu_9593b9a1ca890b1.webp 1500w," src=https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/least-privilege-for-life_hu_34f73f867fcfcaa0.webp alt="Least Privilege for Life slide (described in the text)" loading=lazy></a></figure><p>The last two key concepts are <strong>threat modelling</strong> and the balance of <strong>security versus utility</strong>.</p><p>The <a href=https://www.threatmodelingmanifesto.org/ target=_blank rel=noopener>Threat Modelling Manifesto</a> provides the four key questions to ask when modelling threats:</p><blockquote><ol><li>What are we working on?</li><li>What can go wrong?</li><li>What are we going to do about it?</li><li>Did we do a good enough job?</li></ol></blockquote><p>When answering these questions, we aim to balance security and utility based on the analysed threats.
For example, if we designed a house with thick concrete walls and no doors or windows, its security would be high, but its utility would be low.
In most places, houses with lockable doors and windows represent a good balance between security and utility: It isn&rsquo;t too cumbersome to lock all entry points and carry keys, and the level of security is reasonable for the likely threat of opportunistic break-ins.
The same goes with AI agents: The level of acceptable risk depends on the threats and data we&rsquo;re dealing with, e.g., when I share data with an online chatbot, I accept the risk that it may leak at some point.</p><h2 id=design-patterns-and-best-practices>Design patterns and best practices<a hidden class=anchor aria-hidden=true href=#design-patterns-and-best-practices>#</a></h2><p>While the key concepts above come from a variety of sources, the best source for understanding the design patterns introduced in the paper is <a href=https://arxiv.org/abs/2506.08837 target=_blank rel=noopener>the paper itself</a>.
Therefore, I don&rsquo;t go into too much detail here.
Instead, this section serves as a quick reference to the six patterns.</p><p><strong>Pattern 1: Action selector.</strong>
The agent translates incoming requests to pre-defined actions, e.g., a web &ldquo;chatbot&rdquo; that responds with links, canned responses, and actual actions the user can already access.
This means it can&rsquo;t do anything more harmful than what the user can do, but it is essentially the same as a dropdown menu with a natural language interface.
It&rsquo;s effective against indirect prompt injection and potentially against direct injection, as there is nothing to gain from injection by a malicious user.</p><p><strong>Pattern 2: Plan then execute.</strong>
The agent defines a plan and can&rsquo;t deviate from it based on the untrusted data, e.g., an input like &ldquo;send today&rsquo;s schedule to John&rdquo; becomes <code>calendar.read(today); email.write(..., "john@company")</code>.
This doesn&rsquo;t prevent direct prompt injections, and doesn&rsquo;t protect from indirect prompt injections that affect the contents of the email.
Whether this pattern is good enough depends on the threat model.</p><p><strong>Pattern 3: LLM map-reduce.</strong>
Only isolated agents (the mappers) interact with untrusted data.
Their outputs are restricted to prevent prompt injections (e.g., using regular expressions or strict typing).
For example, when handling a request like &ldquo;find all files that contain an invoice and email me the list of filenames&rdquo;, the main agent doesn&rsquo;t have access to the contents of the files.
Instead, it invokes isolated mapper agents that read each file and return a boolean.
The final reduce operation produces a safe list based on the trusted boolean inputs (assuming filenames are trusted).</p><p><strong>Pattern 4: Dual LLM.</strong>
This is a more general version of the map-reduce pattern, where the privileged LLM doesn&rsquo;t see the untrusted data.
The quarantined LLM sees the data, but only communicates about it using symbols.
For example, when handling a request like &ldquo;find email addresses in this (potentially untrusted) document&rdquo;, the quarantined LLM sees the contents of the document and returns variables with the email addresses (which match strict patterns).
The privileged LLM only sees the trusted variable names, so it&rsquo;s not affected by the contents of the document.</p><p><strong>Pattern 5: Code then execute.</strong>
This is a generalisation of the previous three patterns, where the main agent writes a formal computer program that isolates untrusted input processing to unprivileged agents.
The implementation isn&rsquo;t trivial: this pattern comes from <a href=https://arxiv.org/abs/2503.18813 target=_blank rel=noopener>the 125-page paper that introduced the CaMeL interpreter</a> (<em>&ldquo;a custom Python interpreter that operates over a restricted version of Python&rdquo;</em>).
I haven&rsquo;t read the full CaMeL paper, but my understanding is that it requires defining specific security policies to be fully effective.
Therefore, practical implementations depend on the use case and threat model.</p><p><strong>Pattern 6: Context minimization.</strong>
<a href=https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/#the-context-minimization-pattern target=_blank rel=noopener>Like Simon Willison</a>, I found the description of this pattern to be a bit confusing.
My understanding is that in this case the prompt may be malicious, so it&rsquo;s removed from the tool response.
However, the example given in the paper seems a bit convoluted, as it includes prompt-injecting a customer service agent to give the user a quote with a large discount.
This assumes a company would go be willing to go with whatever discounts given by its bots.
This is a scenario that should be handled via other means (like <a href=https://yanirseroussi.com/2025/06/20/posting-into-the-void-with-guardrails/>deterministic guardrails</a>), as we&rsquo;re not yet at the point where we can trust AI agents in customer-facing roles.</p><p><strong>General best practices.</strong>
The authors emphasise that while different design patterns apply based on the use case, the following best practices should apply to any agent to the extent possible.</p><ul><li><strong>Action sandboxing:</strong> Applying the principle of least privilege in practice, by running specific agent actions in sandboxed environments with limited permissions.</li><li><strong>Strict data formatting:</strong> Using deterministic code to ensure that expected schema rules are followed, rather than allowing arbitrary text.</li><li><strong>User permissions:</strong> Limiting the agent&rsquo;s permissions to those of the authenticated user, so it can only do what the user can do. This reduces the potential harm from prompt injection attacks.</li><li><strong>User confirmation:</strong> Asking the user before running actions that are potentially harmful. While this is useful in some scenarios, it increases friction and reduces automation. Further, excessive requests for confirmation increase the chance of users ignoring the content of the confirmation requests (as most of us do when asked to read the terms of service).</li><li><strong>Data and action attribution:</strong> Providing explanations to the user on the reasoning of the agent (though this may also lead to user fatigue).</li></ul><figure><a href=anti-prompt-injection-patterns-and-practices.webp target=_blank rel=noopener><img sizes="(min-width: 768px) 720px,
100vw" srcset="https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/anti-prompt-injection-patterns-and-practices_hu_5a1ac2cae811fed6.webp 360w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/anti-prompt-injection-patterns-and-practices_hu_9c7861a2e8f2beb9.webp 480w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/anti-prompt-injection-patterns-and-practices_hu_24d8c21f75d4b53f.webp 720w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/anti-prompt-injection-patterns-and-practices_hu_844e494c9c4c841d.webp 1080w,
https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/anti-prompt-injection-patterns-and-practices_hu_c77f4600ca01f661.webp 1500w," src=https://yanirseroussi.com/2025/08/08/beyond-good-vibes-securing-ai-agents-by-design/anti-prompt-injection-patterns-and-practices_hu_7f0f645801e542b7.webp alt="All the design pattern slides in one image." loading=lazy></a><figcaption><p>All the design pattern slides in one image.</p></figcaption></figure><h2 id=case-study-activity>Case study activity<a hidden class=anchor aria-hidden=true href=#case-study-activity>#</a></h2><p>Consuming information about design patterns only takes you so far.
It&rsquo;s already better than vibe coding and hoping for the best, but applying patterns in reality is the way to truly learn them.</p><p>To simulate this at the AI paper event, I split the attendees to groups. I then assigned each group one of the case studies discussed in the paper (§4.2 data analyst, §4.3 email and calendar assistant, §4.4 customer service representative, §4.5 booking agent, and §4.6 product recommender), and prompted each group with the following tasks.</p><ol><li>Define the capabilities and scope of the agent: What can a human agent do?</li><li>Identify the threats, focusing on direct and indirect prompt injections.</li><li>Discuss mitigations using the patterns and best practices.</li><li>Present the approach that best balances security and utility.</li></ol><p>I was pleased with the lively discussion that this activity sparked! 🎉</p><p>You can also try this at home: Discuss the above points with colleagues or with your favourite chatbot.
Prompted well, they can be surprisingly effective at poking holes in your thinking.
Just don&rsquo;t trust them with unbounded access to your key accounts (yet).</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yanirseroussi.com/tags/artificial-intelligence/>Artificial Intelligence</a></li><li><a href=https://yanirseroussi.com/tags/security/>Security</a></li><li><a href=https://yanirseroussi.com/tags/software-engineering/>Software Engineering</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Beyond good vibes: Securing AI agents by design on x" href="https://x.com/intent/tweet/?text=Beyond%20good%20vibes%3a%20Securing%20AI%20agents%20by%20design&amp;url=https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f&amp;hashtags=artificialintelligence%2csecurity%2csoftwareengineering"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Beyond good vibes: Securing AI agents by design on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f&amp;title=Beyond%20good%20vibes%3a%20Securing%20AI%20agents%20by%20design&amp;summary=Beyond%20good%20vibes%3a%20Securing%20AI%20agents%20by%20design&amp;source=https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Beyond good vibes: Securing AI agents by design on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f&title=Beyond%20good%20vibes%3a%20Securing%20AI%20agents%20by%20design"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Beyond good vibes: Securing AI agents by design on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Beyond good vibes: Securing AI agents by design on whatsapp" href="https://api.whatsapp.com/send?text=Beyond%20good%20vibes%3a%20Securing%20AI%20agents%20by%20design%20-%20https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Beyond good vibes: Securing AI agents by design on telegram" href="https://telegram.me/share/url?text=Beyond%20good%20vibes%3a%20Securing%20AI%20agents%20by%20design&amp;url=https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Beyond good vibes: Securing AI agents by design on ycombinator" href="https://news.ycombinator.com/submitlink?t=Beyond%20good%20vibes%3a%20Securing%20AI%20agents%20by%20design&u=https%3a%2f%2fyanirseroussi.com%2f2025%2f08%2f08%2fbeyond-good-vibes-securing-ai-agents-by-design%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><a href=/contact/#mailing-list-email target=_blank aria-label="subscribe to mailing list" class=mailing-list-link id=mailing-list-link>Subscribe
</a><script>const mailingListButton=document.getElementById("mailing-list-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mailingListButton.style.visibility="visible",mailingListButton.style.opacity="1"):(mailingListButton.style.visibility="hidden",mailingListButton.style.opacity="0")}</script><div class=mailing-list-container><script src=https://f.convertkit.com/ckjs/ck.5.js></script><form class="mailing-list seva-form formkit-form" action=https://app.convertkit.com/forms/6549537/subscriptions method=post data-sv-form=6549537 data-uid=9157759fce data-format=inline data-version=5 data-options='{"settings":{"after_subscribe":{"action":"message","redirect_url":"","success_message":"Success! Now check your email to confirm your subscription."},"recaptcha":{"enabled":false},"return_visitor":{"action":"show","custom_content":""}},"version":"5"}'><div data-style=clean><ul class="formkit-alert formkit-alert-error" data-element=errors data-group=alert></ul><div data-element=fields data-stacked=false><label for=mailing-list-email>Get new posts in your mailbox</label>
<input id=mailing-list-email name=email_address aria-label="Email address" placeholder="Email address" required type=email>
<button data-element=submit>Subscribe</button></div></div></form><div class=footer>Join hundreds of subscribers. No spam or AI-generated slop. Unsubscribe any time.</div></div><section class=comment-section><p class="post-content contact-cta">Public comments are closed, but I love hearing from readers. Feel free to
<a href=/contact/ target=_blank>contact me</a> with your thoughts.</p></section><p class="post-content data-webring">This site is a part of the <a href=https://randyau.github.io/datawebring/index.html target=_blank rel=noopener>Data People Writing Stuff</a> webring.<br><a class=data-webring-previous-link target=_blank rel=noopener>← previous site</a>
&nbsp; | &nbsp;
<a class=data-webring-next-link target=_blank rel=noopener>next site →</a></p><script>function populateDataWebringLinks(){const e=["https://www.randyau.com/","https://vickiboykis.com/","https://www.counting-stuff.com/","https://gecky.me/","https://qethanm.cc/datawebring/","https://mlops.systems/","https://e2eml.school/","https://blog.harterrt.com/","https://www.jessemostipak.com/","https://elliotgunn.github.io/","https://radbrt.com","https://simon.podhajsky.net/blog/","https://www.heltweg.org/","https://emilyriederer.com/","https://kylestratis.com","https://www.eamoncaddigan.net/","https://karnwong.me/","https://aino-spring.com/"];function t(e){let n,s,t;for(t=e.length-1;t>0;t--)n=Math.floor(Math.random()*(t+1)),s=e[t],e[t]=e[n],e[n]=s}t(e),document.querySelector(".data-webring-previous-link").href=e[0],document.querySelector(".data-webring-next-link").href=e[1]}populateDataWebringLinks()</script></article></main><div class=global-footer><div class=footer><span>Text and figures licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank rel=noopener>CC BY-NC-ND 4.0</a> by <a href=https://yanirseroussi.com/about/>Yanir Seroussi</a>, except where noted otherwise&nbsp;&nbsp;|</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></div></div><script>const menuTrigger=document.querySelector("#menu-trigger"),menuElem=document.querySelector(".menu");menuTrigger.addEventListener("click",function(){menuElem.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){menuTrigger.contains(e.target)||menuElem.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>