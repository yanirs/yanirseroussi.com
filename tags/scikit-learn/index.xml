<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>scikit-learn on Yanir Seroussi | Data science and beyond</title><link>https://yanirs.github.io/yanirseroussi.com/tags/scikit-learn/</link><description>Recent content in scikit-learn on Yanir Seroussi | Data science and beyond</description><generator>Hugo -- gohugo.io</generator><language>en-au</language><copyright>&amp;copy; [Yanir Seroussi](https://yanirseroussi.com/about/)&amp;nbsp;&amp;nbsp;|</copyright><lastBuildDate>Mon, 29 Dec 2014 02:30:06 +0000</lastBuildDate><atom:link href="https://yanirs.github.io/yanirseroussi.com/tags/scikit-learn/index.xml" rel="self" type="application/rss+xml"/><item><title>Stochastic Gradient Boosting: Choosing the Best Number of Iterations</title><link>https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</link><pubDate>Mon, 29 Dec 2014 02:30:06 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</guid><description>In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).</description></item><item><title>Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)</title><link>https://yanirs.github.io/yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/</link><pubDate>Wed, 19 Nov 2014 09:17:34 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/</guid><description>Messy data, buggy software, but all in all a good learning experience...
Early last year, I had some free time on my hands, so I decided to participate in yet another Kaggle competition. Having never done any price forecasting work before, I thought it would be interesting to work on the Blue Book for Bulldozers competition, where the goal was to predict the sale price of auctioned bulldozers. I&amp;rsquo;ve done alright, finishing 9th out of 476 teams.</description></item></channel></rss>