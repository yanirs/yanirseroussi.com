<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>predictive modelling on Yanir Seroussi | Data science and beyond</title><link>https://yanirs.github.io/yanirseroussi.com/tags/predictive-modelling/</link><description>Recent content in predictive modelling on Yanir Seroussi | Data science and beyond</description><generator>Hugo -- gohugo.io</generator><language>en-au</language><lastBuildDate>Sun, 08 Jan 2017 20:02:30 +0000</lastBuildDate><atom:link href="https://yanirs.github.io/yanirseroussi.com/tags/predictive-modelling/index.xml" rel="self" type="application/rss+xml"/><item><title>Customer lifetime value and the proliferation of misinformation on the internet</title><link>https://yanirs.github.io/yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/</link><pubDate>Sun, 08 Jan 2017 20:02:30 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/</guid><description>Suppose you work for a business that has paying customers. You want to know how much money your customers are likely to spend to inform decisions on customer acquisition and retention budgets. You&amp;rsquo;ve done a bit of research, and discovered that the figure you want to calculate is commonly called the customer lifetime value. You google the term, and end up on a page with ten results (and probably some ads).</description></item><item><title>Diving deeper into causality: Pearl, Kleinberg, Hill, and untested assumptions</title><link>https://yanirs.github.io/yanirseroussi.com/2016/05/15/diving-deeper-into-causality-pearl-kleinberg-hill-and-untested-assumptions/</link><pubDate>Sat, 14 May 2016 19:57:03 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2016/05/15/diving-deeper-into-causality-pearl-kleinberg-hill-and-untested-assumptions/</guid><description>Background: I have previously written about the need for real insights that address the why behind events, not only the what and how. This was followed by a fairly popular post on causality, which was heavily influenced by Samantha Kleinberg's book Why: A Guide to Finding and Using Causes. This post continues my exploration of the field, and is primarily based on Kleinberg's previous book: Causality, Probability, and Time.
The study of causality and causal inference is central to science in general and data science in particular.</description></item><item><title>Why you should stop worrying about deep learning and deepen your understanding of causality instead</title><link>https://yanirs.github.io/yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/</link><pubDate>Sun, 14 Feb 2016 11:04:11 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/</guid><description>Everywhere you go these days, you hear about deep learning&amp;rsquo;s impressive advancements. New deep learning libraries, tools, and products get announced on a regular basis, making the average data scientist feel like they&amp;rsquo;re missing out if they don&amp;rsquo;t hop on the deep learning bandwagon. However, as Kamil Bartocha put it in his post The Inconvenient Truth About Data Science, 95% of tasks do not require deep learning. This is obviously a made up number, but it&amp;rsquo;s probably an accurate representation of the everyday reality of many data scientists.</description></item><item><title>The joys of offline data collection</title><link>https://yanirs.github.io/yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/</link><pubDate>Sun, 24 Jan 2016 00:32:25 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/</guid><description>Many modern data scientists don&amp;rsquo;t get to experience data collection in the offline world. Recently, I spent a month sailing down the northern Great Barrier Reef, collecting data for the Reef Life Survey project. In addition to being a great diving experience, the trip helped me obtain general insights on data collection and machine learning, which are shared in this article.
The Reef Life Survey project Reef Life Survey (RLS) is a citizen scientist project, led by a team from the University of Tasmania.</description></item><item><title>The hardest parts of data science</title><link>https://yanirs.github.io/yanirseroussi.com/2015/11/23/the-hardest-parts-of-data-science/</link><pubDate>Mon, 23 Nov 2015 04:14:21 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/11/23/the-hardest-parts-of-data-science/</guid><description>Contrary to common belief, the hardest part of data science isn&amp;rsquo;t building an accurate model or obtaining good, clean data. It is much harder to define feasible problems and come up with reasonable ways of measuring solutions. This post discusses some examples of these issues and how they can be addressed.
The not-so-hard parts Before discussing the hardest parts of data science, it&amp;rsquo;s worth quickly addressing the two main contenders: model fitting and data collection/cleaning.</description></item><item><title>Miscommunicating science: Simplistic models, nutritionism, and the art of storytelling</title><link>https://yanirs.github.io/yanirseroussi.com/2015/10/19/nutritionism-and-the-need-for-complex-models-to-explain-complex-phenomena/</link><pubDate>Mon, 19 Oct 2015 00:02:32 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/10/19/nutritionism-and-the-need-for-complex-models-to-explain-complex-phenomena/</guid><description>I recently finished reading the book In Defense of Food: An Eater&amp;rsquo;s Manifesto by Michael Pollan. The book criticises nutritionism – the idea that one should eat according to the sum of measured nutrients while ignoring the food that contains these nutrients. The key argument of the book is that since the knowledge derived using food science is still very limited, completely relying on the partial findings and tools provided by this science is likely to lead to health issues.</description></item><item><title>The wonderful world of recommender systems</title><link>https://yanirs.github.io/yanirseroussi.com/2015/10/02/the-wonderful-world-of-recommender-systems/</link><pubDate>Fri, 02 Oct 2015 05:25:57 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/10/02/the-wonderful-world-of-recommender-systems/</guid><description>I recently gave a talk about recommender systems at the Data Science Sydney meetup (the slides are available here). This post roughly follows the outline of the talk, expanding on some of the key points in non-slide form (i.e., complete sentences and paragraphs!). The first few sections give a broad overview of the field and the common recommendation paradigms, while the final part is dedicated to debunking five common myths about recommender systems.</description></item><item><title>Learning about deep learning through album cover classification</title><link>https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/</link><pubDate>Mon, 06 Jul 2015 22:21:42 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/</guid><description>In the past month, I&amp;rsquo;ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.
Initial steps summary The following points were discussed in detail in the previous post on this project.</description></item><item><title>Hopping on the deep learning bandwagon</title><link>https://yanirs.github.io/yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/</link><pubDate>Sat, 06 Jun 2015 05:00:22 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/</guid><description>I&amp;rsquo;ve been meaning to get into deep learning for the last few years. Now, the stars having finally aligned and I have the time and motivation to work on a small project that will hopefully improve my understanding of the field. This is the first in a series of posts that will document my progress on this project.
As mentioned in a previous post on getting started as a data scientist, I believe that the best way of becoming proficient at solving data science problems is by getting your hands dirty.</description></item><item><title>First steps in data science: author-aware sentiment analysis</title><link>https://yanirs.github.io/yanirseroussi.com/2015/05/02/first-steps-in-data-science-author-aware-sentiment-analysis/</link><pubDate>Sat, 02 May 2015 08:31:10 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/05/02/first-steps-in-data-science-author-aware-sentiment-analysis/</guid><description>People often ask me what&amp;rsquo;s the best way of becoming a data scientist. The way I got there was by first becoming a software engineer and then doing a PhD in what was essentially data science (before it became such a popular term). This post describes my first steps in the field with the goal of helping others who are interested in making the transition from pure software engineering to data science.</description></item><item><title>Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)</title><link>https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/</link><pubDate>Wed, 11 Feb 2015 06:34:17 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/</guid><description>This is the second and last post summarising my team&amp;rsquo;s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.
To quickly recap the first post, Yandex released a 16GB dataset of query &amp;amp; click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results.</description></item><item><title>Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)</title><link>https://yanirs.github.io/yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/</link><pubDate>Thu, 29 Jan 2015 10:37:39 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/</guid><description>About a year ago, I participated in the Yandex search personalisation Kaggle competition. I started off as a solo competitor, and then added a few Kaggle newbies to the team as part of a program I was running for the Sydney Data Science Meetup. My team hasn&amp;rsquo;t done too badly, finishing 9th out of 194 teams. As is usually the case with Kaggle competitions, the most valuable part was the lessons learned from the experience.</description></item><item><title>Stochastic Gradient Boosting: Choosing the Best Number of Iterations</title><link>https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</link><pubDate>Mon, 29 Dec 2014 02:30:06 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</guid><description>In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).</description></item><item><title>Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)</title><link>https://yanirs.github.io/yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/</link><pubDate>Wed, 19 Nov 2014 09:17:34 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/</guid><description>Messy data, buggy software, but all in all a good learning experience...
Early last year, I had some free time on my hands, so I decided to participate in yet another Kaggle competition. Having never done any price forecasting work before, I thought it would be interesting to work on the Blue Book for Bulldozers competition, where the goal was to predict the sale price of auctioned bulldozers. I&amp;rsquo;ve done alright, finishing 9th out of 476 teams.</description></item><item><title>Greek Media Monitoring Kaggle competition: My approach</title><link>https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/</link><pubDate>Tue, 07 Oct 2014 03:21:35 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/</guid><description>A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.
Data &amp;amp; evaluation The data consists of articles scanned from Greek print media in May-September 2013.</description></item><item><title>Bandcamp recommendation and discovery algorithms</title><link>https://yanirs.github.io/yanirseroussi.com/2014/09/19/bandcamp-recommendation-and-discovery-algorithms/</link><pubDate>Fri, 19 Sep 2014 14:26:55 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/09/19/bandcamp-recommendation-and-discovery-algorithms/</guid><description>This is the third part of a series of posts on my Bandcamp recommendations (BCRecommender) project. Check out the first part for the general motivation behind this project and the second part for the system architecture. The main goal of the BCRecommender project is to help me find music I like. This post discusses the algorithmic approaches I took towards that goal. I&amp;rsquo;ve kept the descriptions at a fairly high-level, without getting too much into the maths, as all recommendation algorithms essentially try to model simple intuition.</description></item><item><title>How to (almost) win Kaggle competitions</title><link>https://yanirs.github.io/yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/</link><pubDate>Sun, 24 Aug 2014 12:40:53 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/</guid><description>Last week, I gave a talk at the Data Science Sydney Meetup group about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are here). Some of these tips were covered in my beginner tips post from a few months ago. Similar advice was also recently published on the Kaggle blog – it&amp;rsquo;s great to see that my tips are in line with the thoughts of other prolific kagglers.</description></item><item><title>Kaggle competition tips and summaries</title><link>https://yanirs.github.io/yanirseroussi.com/2014/04/05/kaggle-competition-summaries/</link><pubDate>Sat, 05 Apr 2014 23:46:10 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/04/05/kaggle-competition-summaries/</guid><description>Over the years, I&amp;rsquo;ve participated in a few Kaggle competitions and wrote a bit about my experiences. This page contains pointers to all my posts, and will be updated if/when I participate in more competitions.
General advice posts 10 Steps to Success in Kaggle Data Science Competitions (guest post on KDNuggets) How to (almost) win Kaggle competitions Kaggle beginner tips Solution posts Greek Media Monitoring Multilabel Classification [6th/120] – multi-label classification of pre-tokenised texts Personalised Web Search Challenge [9th/194] – reranking web search results in a personalised manner Blue Book for Bulldozers [9th/476] – forecasting auction sale price of bulldozers ICFHR 2012 – Arabic Writer Identification Competition [3rd/42] – classifying handwritten texts by the identity of the writer (Kaggle blog post) EMC Data Science Global Hackathon (Air Quality Prediction) [6th/110] – forecasting levels of air pollutants (Kaggle forum post)</description></item></channel></rss>