<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>predictive modelling | Yanir Seroussi | Data science and beyond</title><meta name=keywords content><meta name=description content="Posts on data science and related areas by Yanir Seroussi, an experienced data scientist and software engineer.
"><meta name=author content="Yanir Seroussi"><link rel=canonical href=https://yanirseroussi.com/tags/predictive-modelling/><meta name=google-site-verification content="aWlue7NGcj4dQpjOKJF7YKiAvw3JuHnq6aFqX6VwWAU"><link crossorigin=anonymous href=/assets/css/stylesheet.14c2944979911d0cdd8e64a58dfa90394ec943228a4c13c39d6ea94250330089.css integrity="sha256-FMKUSXmRHQzdjmSljfqQOU7JQyKKTBPDnW6pQlAzAIk=" rel="preload stylesheet" as=style><link rel=icon href=https://yanirseroussi.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yanirseroussi.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yanirseroussi.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yanirseroussi.com/apple-touch-icon.png><link rel=mask-icon href=https://yanirseroussi.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://yanirseroussi.com/tags/predictive-modelling/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="predictive modelling"><meta property="og:description" content="Posts on data science and related areas by Yanir Seroussi, an experienced data scientist and software engineer.
"><meta property="og:type" content="website"><meta property="og:url" content="https://yanirseroussi.com/tags/predictive-modelling/"><meta name=twitter:card content="summary"><meta name=twitter:title content="predictive modelling"><meta name=twitter:description content="Posts on data science and related areas by Yanir Seroussi, an experienced data scientist and software engineer.
"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yanirseroussi.com/ accesskey=h title="Yanir Seroussi | Data science and beyond (Alt + H)">Yanir Seroussi | Data science and beyond</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yanirseroussi.com/about/ title=About><span>About</span></a></li><li><a href=https://yanirseroussi.com/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://yanirseroussi.com/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>predictive modelling</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>Customer lifetime value and the proliferation of misinformation on the internet</h2></header><div class=entry-content><p>Suppose you work for a business that has paying customers. You want to know how much money your customers are likely to spend to inform decisions on customer acquisition and retention budgets. You’ve done a bit of research, and discovered that the figure you want to calculate is commonly called the customer lifetime value. You google the term, and end up on a page with ten results (and probably some ads)....</p></div><footer class=entry-footer><span title='2017-01-08 20:02:30 +0000 UTC'>January 8, 2017</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Customer lifetime value and the proliferation of misinformation on the internet" href=https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Diving deeper into causality: Pearl, Kleinberg, Hill, and untested assumptions</h2></header><div class=entry-content><p>Background: I have previously written about the need for real insights that address the why behind events, not only the what and how. This was followed by a fairly popular post on causality, which was heavily influenced by Samantha Kleinberg's book Why: A Guide to Finding and Using Causes. This post continues my exploration of the field, and is primarily based on Kleinberg's previous book: Causality, Probability, and Time.
The study of causality and causal inference is central to science in general and data science in particular....</p></div><footer class=entry-footer><span title='2016-05-14 19:57:03 +0000 UTC'>May 14, 2016</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Diving deeper into causality: Pearl, Kleinberg, Hill, and untested assumptions" href=https://yanirseroussi.com/2016/05/15/diving-deeper-into-causality-pearl-kleinberg-hill-and-untested-assumptions/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Why you should stop worrying about deep learning and deepen your understanding of causality instead</h2></header><div class=entry-content><p>Everywhere you go these days, you hear about deep learning’s impressive advancements. New deep learning libraries, tools, and products get announced on a regular basis, making the average data scientist feel like they’re missing out if they don’t hop on the deep learning bandwagon. However, as Kamil Bartocha put it in his post The Inconvenient Truth About Data Science, 95% of tasks do not require deep learning. This is obviously a made up number, but it’s probably an accurate representation of the everyday reality of many data scientists....</p></div><footer class=entry-footer><span title='2016-02-14 11:04:11 +0000 UTC'>February 14, 2016</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Why you should stop worrying about deep learning and deepen your understanding of causality instead" href=https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>The joys of offline data collection</h2></header><div class=entry-content><p>Many modern data scientists don’t get to experience data collection in the offline world. Recently, I spent a month sailing down the northern Great Barrier Reef, collecting data for the Reef Life Survey project. In addition to being a great diving experience, the trip helped me obtain general insights on data collection and machine learning, which are shared in this article.
The Reef Life Survey project Reef Life Survey (RLS) is a citizen scientist project, led by a team from the University of Tasmania....</p></div><footer class=entry-footer><span title='2016-01-24 00:32:25 +0000 UTC'>January 24, 2016</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to The joys of offline data collection" href=https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>The hardest parts of data science</h2></header><div class=entry-content><p>Contrary to common belief, the hardest part of data science isn’t building an accurate model or obtaining good, clean data. It is much harder to define feasible problems and come up with reasonable ways of measuring solutions. This post discusses some examples of these issues and how they can be addressed.
The not-so-hard parts Before discussing the hardest parts of data science, it’s worth quickly addressing the two main contenders: model fitting and data collection/cleaning....</p></div><footer class=entry-footer><span title='2015-11-23 04:14:21 +0000 UTC'>November 23, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to The hardest parts of data science" href=https://yanirseroussi.com/2015/11/23/the-hardest-parts-of-data-science/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Miscommunicating science: Simplistic models, nutritionism, and the art of storytelling</h2></header><div class=entry-content><p>I recently finished reading the book In Defense of Food: An Eater’s Manifesto by Michael Pollan. The book criticises nutritionism – the idea that one should eat according to the sum of measured nutrients while ignoring the food that contains these nutrients. The key argument of the book is that since the knowledge derived using food science is still very limited, completely relying on the partial findings and tools provided by this science is likely to lead to health issues....</p></div><footer class=entry-footer><span title='2015-10-19 00:02:32 +0000 UTC'>October 19, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Miscommunicating science: Simplistic models, nutritionism, and the art of storytelling" href=https://yanirseroussi.com/2015/10/19/nutritionism-and-the-need-for-complex-models-to-explain-complex-phenomena/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>The wonderful world of recommender systems</h2></header><div class=entry-content><p>I recently gave a talk about recommender systems at the Data Science Sydney meetup (the slides are available here). This post roughly follows the outline of the talk, expanding on some of the key points in non-slide form (i.e., complete sentences and paragraphs!). The first few sections give a broad overview of the field and the common recommendation paradigms, while the final part is dedicated to debunking five common myths about recommender systems....</p></div><footer class=entry-footer><span title='2015-10-02 05:25:57 +0000 UTC'>October 2, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to The wonderful world of recommender systems" href=https://yanirseroussi.com/2015/10/02/the-wonderful-world-of-recommender-systems/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Learning about deep learning through album cover classification</h2></header><div class=entry-content><p>In the past month, I’ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.
Initial steps summary The following points were discussed in detail in the previous post on this project....</p></div><footer class=entry-footer><span title='2015-07-06 22:21:42 +0000 UTC'>July 6, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Learning about deep learning through album cover classification" href=https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Hopping on the deep learning bandwagon</h2></header><div class=entry-content><p>I’ve been meaning to get into deep learning for the last few years. Now, the stars having finally aligned and I have the time and motivation to work on a small project that will hopefully improve my understanding of the field. This is the first in a series of posts that will document my progress on this project.
As mentioned in a previous post on getting started as a data scientist, I believe that the best way of becoming proficient at solving data science problems is by getting your hands dirty....</p></div><footer class=entry-footer><span title='2015-06-06 05:00:22 +0000 UTC'>June 6, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Hopping on the deep learning bandwagon" href=https://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>First steps in data science: author-aware sentiment analysis</h2></header><div class=entry-content><p>People often ask me what’s the best way of becoming a data scientist. The way I got there was by first becoming a software engineer and then doing a PhD in what was essentially data science (before it became such a popular term). This post describes my first steps in the field with the goal of helping others who are interested in making the transition from pure software engineering to data science....</p></div><footer class=entry-footer><span title='2015-05-02 08:31:10 +0000 UTC'>May 2, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to First steps in data science: author-aware sentiment analysis" href=https://yanirseroussi.com/2015/05/02/first-steps-in-data-science-author-aware-sentiment-analysis/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)</h2></header><div class=entry-content><p>This is the second and last post summarising my team’s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.
To quickly recap the first post, Yandex released a 16GB dataset of query & click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results....</p></div><footer class=entry-footer><span title='2015-02-11 06:34:17 +0000 UTC'>February 11, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)" href=https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)</h2></header><div class=entry-content><p>About a year ago, I participated in the Yandex search personalisation Kaggle competition. I started off as a solo competitor, and then added a few Kaggle newbies to the team as part of a program I was running for the Sydney Data Science Meetup. My team hasn’t done too badly, finishing 9th out of 194 teams. As is usually the case with Kaggle competitions, the most valuable part was the lessons learned from the experience....</p></div><footer class=entry-footer><span title='2015-01-29 10:37:39 +0000 UTC'>January 29, 2015</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)" href=https://yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Stochastic Gradient Boosting: Choosing the Best Number of Iterations</h2></header><div class=entry-content><p>In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations)....</p></div><footer class=entry-footer><span title='2014-12-29 02:30:06 +0000 UTC'>December 29, 2014</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Stochastic Gradient Boosting: Choosing the Best Number of Iterations" href=https://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)</h2></header><div class=entry-content><p>Messy data, buggy software, but all in all a good learning experience...
Early last year, I had some free time on my hands, so I decided to participate in yet another Kaggle competition. Having never done any price forecasting work before, I thought it would be interesting to work on the Blue Book for Bulldozers competition, where the goal was to predict the sale price of auctioned bulldozers. I’ve done alright, finishing 9th out of 476 teams....</p></div><footer class=entry-footer><span title='2014-11-19 09:17:34 +0000 UTC'>November 19, 2014</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)" href=https://yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Greek Media Monitoring Kaggle competition: My approach</h2></header><div class=entry-content><p>A few months ago I participated in the Kaggle Greek Media Monitoring competition. The goal of the competition was doing multilabel classification of texts scanned from Greek print media. Despite not having much time due to travelling and other commitments, I managed to finish 6th (out of 120 teams). This post describes my approach to the problem.
Data & evaluation The data consists of articles scanned from Greek print media in May-September 2013....</p></div><footer class=entry-footer><span title='2014-10-07 03:21:35 +0000 UTC'>October 7, 2014</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Greek Media Monitoring Kaggle competition: My approach" href=https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Bandcamp recommendation and discovery algorithms</h2></header><div class=entry-content><p>This is the third part of a series of posts on my Bandcamp recommendations (BCRecommender) project. Check out the first part for the general motivation behind this project and the second part for the system architecture. The main goal of the BCRecommender project is to help me find music I like. This post discusses the algorithmic approaches I took towards that goal. I’ve kept the descriptions at a fairly high-level, without getting too much into the maths, as all recommendation algorithms essentially try to model simple intuition....</p></div><footer class=entry-footer><span title='2014-09-19 14:26:55 +0000 UTC'>September 19, 2014</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Bandcamp recommendation and discovery algorithms" href=https://yanirseroussi.com/2014/09/19/bandcamp-recommendation-and-discovery-algorithms/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>How to (almost) win Kaggle competitions</h2></header><div class=entry-content><p>Last week, I gave a talk at the Data Science Sydney Meetup group about some of the lessons I learned through almost winning five Kaggle competitions. The core of the talk was ten tips, which I think are worth putting in a post (the original slides are here). Some of these tips were covered in my beginner tips post from a few months ago. Similar advice was also recently published on the Kaggle blog – it’s great to see that my tips are in line with the thoughts of other prolific kagglers....</p></div><footer class=entry-footer><span title='2014-08-24 12:40:53 +0000 UTC'>August 24, 2014</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to How to (almost) win Kaggle competitions" href=https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Kaggle competition tips and summaries</h2></header><div class=entry-content><p>Over the years, I’ve participated in a few Kaggle competitions and wrote a bit about my experiences. This page contains pointers to all my posts, and will be updated if/when I participate in more competitions.
General advice posts 10 Steps to Success in Kaggle Data Science Competitions (guest post on KDNuggets) How to (almost) win Kaggle competitions Kaggle beginner tips Solution posts Greek Media Monitoring Multilabel Classification [6th/120] – multi-label classification of pre-tokenised texts Personalised Web Search Challenge [9th/194] – reranking web search results in a personalised manner Blue Book for Bulldozers [9th/476] – forecasting auction sale price of bulldozers ICFHR 2012 – Arabic Writer Identification Competition [3rd/42] – classifying handwritten texts by the identity of the writer (Kaggle blog post) EMC Data Science Global Hackathon (Air Quality Prediction) [6th/110] – forecasting levels of air pollutants (Kaggle forum post)</p></div><footer class=entry-footer><span title='2014-04-05 23:46:10 +0000 UTC'>April 5, 2014</span>&nbsp;·&nbsp;Yanir Seroussi</footer><a class=entry-link aria-label="post link to Kaggle competition tips and summaries" href=https://yanirseroussi.com/kaggle/></a></article></main><footer class=footer><span>Text and figures licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank rel=noopener>CC BY-NC-ND 4.0</a> by <a href=https://yanirseroussi.com/about/>Yanir Seroussi</a>, except where noted otherwise  |</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><div class=mailing-list-container><form class=mailing-list action="https://yanirseroussi.us17.list-manage.com/subscribe/post?u=3c08aa3ff27dd92978019febd&amp;id=bc3ab705af" method=post target=_blank novalidate><label for=mailing-list-email>Get new post notifications</label>
<input type=text name=EMAIL id=mailing-list-email placeholder="Email address"><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_3c08aa3ff27dd92978019febd_bc3ab705af tabindex=-1></div><input type=submit value=Subscribe></form><div class=footer>Alternatively, <a href=https://github.com/yanirs/yanirseroussi.com rel=noopener target=_blank>watch on GitHub</a>
or <a href=https://yanirseroussi.com/index.xml>subscribe to RSS feed</a>.</div></div><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>