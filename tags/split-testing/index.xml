<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>split testing on Yanir Seroussi | Data science and beyond</title><link>https://yanirs.github.io/yanirseroussi.com/tags/split-testing/</link><description>Recent content in split testing on Yanir Seroussi | Data science and beyond</description><generator>Hugo -- gohugo.io</generator><language>en-au</language><lastBuildDate>Sun, 19 Jun 2016 10:32:15 +0000</lastBuildDate><atom:link href="https://yanirs.github.io/yanirseroussi.com/tags/split-testing/index.xml" rel="self" type="application/rss+xml"/><item><title>Making Bayesian A/B testing more accessible</title><link>https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/</link><pubDate>Sun, 19 Jun 2016 10:32:15 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/</guid><description>Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&amp;rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople.</description></item></channel></rss>