<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>gradient boosting on Yanir Seroussi | Data science and beyond</title><link>https://yanirs.github.io/yanirseroussi.com/tags/gradient-boosting/</link><description>Recent content in gradient boosting on Yanir Seroussi | Data science and beyond</description><generator>Hugo -- gohugo.io</generator><language>en-au</language><copyright>&amp;copy; [Yanir Seroussi](https://yanirseroussi.com/about/)&amp;nbsp;&amp;nbsp;|</copyright><lastBuildDate>Wed, 11 Feb 2015 06:34:17 +0000</lastBuildDate><atom:link href="https://yanirs.github.io/yanirseroussi.com/tags/gradient-boosting/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)</title><link>https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/</link><pubDate>Wed, 11 Feb 2015 06:34:17 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/</guid><description>This is the second and last post summarising my team&amp;rsquo;s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.
To quickly recap the first post, Yandex released a 16GB dataset of query &amp;amp; click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results.</description></item><item><title>Stochastic Gradient Boosting: Choosing the Best Number of Iterations</title><link>https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</link><pubDate>Mon, 29 Dec 2014 02:30:06 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</guid><description>In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).</description></item><item><title>Fitting noise: Forecasting the sale price of bulldozers (Kaggle competition summary)</title><link>https://yanirs.github.io/yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/</link><pubDate>Wed, 19 Nov 2014 09:17:34 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2014/11/19/fitting-noise-forecasting-the-sale-price-of-bulldozers-kaggle-competition-summary/</guid><description>Messy data, buggy software, but all in all a good learning experience...
Early last year, I had some free time on my hands, so I decided to participate in yet another Kaggle competition. Having never done any price forecasting work before, I thought it would be interesting to work on the Blue Book for Bulldozers competition, where the goal was to predict the sale price of auctioned bulldozers. I&amp;rsquo;ve done alright, finishing 9th out of 476 teams.</description></item></channel></rss>