<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>statistics on Yanir Seroussi | Data science and beyond</title><link>https://yanirs.github.io/yanirseroussi.com/tags/statistics/</link><description>Recent content in statistics on Yanir Seroussi | Data science and beyond</description><generator>Hugo -- gohugo.io</generator><language>en-au</language><copyright>&amp;copy; [Yanir Seroussi](https://yanirseroussi.com/about/)&amp;nbsp;&amp;nbsp;|</copyright><lastBuildDate>Mon, 24 Aug 2020 01:35:17 +0000</lastBuildDate><atom:link href="https://yanirs.github.io/yanirseroussi.com/tags/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Many is not enough: Counting simulations to bootstrap the right way</title><link>https://yanirs.github.io/yanirseroussi.com/2020/08/24/many-is-not-enough-counting-simulations-to-bootstrap-the-right-way/</link><pubDate>Mon, 24 Aug 2020 01:35:17 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2020/08/24/many-is-not-enough-counting-simulations-to-bootstrap-the-right-way/</guid><description>Previously, I encouraged readers to test different approaches to bootstrapped confidence interval (CI) estimation. Such testing can done by relying on the definition of CIs: Given an infinite number of independent samples from the same population, we expect a ci_level CI to contain the population parameter in exactly ci_level percent of the samples. Therefore, we run &amp;ldquo;many&amp;rdquo; simulations (num_simulations), where each simulation generates a random sample from the same population and runs the CI algorithm on the sample.</description></item><item><title>Bootstrapping the right way?</title><link>https://yanirs.github.io/yanirseroussi.com/2019/10/06/bootstrapping-the-right-way/</link><pubDate>Sun, 06 Oct 2019 06:48:07 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2019/10/06/bootstrapping-the-right-way/</guid><description>Bootstrapping the right way is a talk I gave earlier this year at the YOW! Data conference in Sydney. You can now watch the video of the talk and have a look through the slides. The content of the talk is similar to a post I published on bootstrapping pitfalls, with some additional simulations.
The main takeaways shared in the talk are:
Don&amp;rsquo;t compare single-sample confidence intervals by eye Use enough resamples (15K?</description></item><item><title>Hackers beware: Bootstrap sampling may be harmful</title><link>https://yanirs.github.io/yanirseroussi.com/2019/01/08/hackers-beware-bootstrap-sampling-may-be-harmful/</link><pubDate>Mon, 07 Jan 2019 21:07:56 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2019/01/08/hackers-beware-bootstrap-sampling-may-be-harmful/</guid><description>Bootstrap sampling techniques are very appealing, as they don&amp;rsquo;t require knowing much about statistics and opaque formulas. Instead, all one needs to do is resample the given data many times, and calculate the desired statistics. Therefore, bootstrapping has been promoted as an easy way of modelling uncertainty to hackers who don&amp;rsquo;t have much statistical knowledge. For example, the main thesis of the excellent Statistics for Hackers talk by Jake VanderPlas is: &amp;ldquo;If you can write a for-loop, you can do statistics&amp;rdquo;.</description></item><item><title>The most practical causal inference book I’ve read (is still a draft)</title><link>https://yanirs.github.io/yanirseroussi.com/2018/12/24/the-most-practical-causal-inference-book-ive-read-is-still-a-draft/</link><pubDate>Mon, 24 Dec 2018 02:37:50 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2018/12/24/the-most-practical-causal-inference-book-ive-read-is-still-a-draft/</guid><description>I&amp;rsquo;ve been interested in the area of causal inference in the past few years. In my opinion it&amp;rsquo;s more exciting and relevant to everyday life than more hyped data science areas like deep learning. However, I&amp;rsquo;ve found it hard to apply what I&amp;rsquo;ve learned about causal inference to my work. Now, I believe I&amp;rsquo;ve finally found a book with practical techniques that I can use on real problems: Causal Inference by Miguel Hernán and Jamie Robins.</description></item><item><title>Defining data science in 2018</title><link>https://yanirs.github.io/yanirseroussi.com/2018/07/22/defining-data-science-in-2018/</link><pubDate>Sun, 22 Jul 2018 08:27:43 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2018/07/22/defining-data-science-in-2018/</guid><description>I got my first data science job in 2012, the year Harvard Business Review announced data scientist to be the sexiest job of the 21st century. Two years later, I published a post on my then-favourite definition of data science, as the intersection between software engineering and statistics. Unfortunately, that definition became somewhat irrelevant as more and more people jumped on the data science bandwagon – possibly to the point of making data scientist useless as a job title.</description></item><item><title>Customer lifetime value and the proliferation of misinformation on the internet</title><link>https://yanirs.github.io/yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/</link><pubDate>Sun, 08 Jan 2017 20:02:30 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/</guid><description>Suppose you work for a business that has paying customers. You want to know how much money your customers are likely to spend to inform decisions on customer acquisition and retention budgets. You&amp;rsquo;ve done a bit of research, and discovered that the figure you want to calculate is commonly called the customer lifetime value. You google the term, and end up on a page with ten results (and probably some ads).</description></item><item><title>If you don’t pay attention, data can drive you off a cliff</title><link>https://yanirs.github.io/yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/</link><pubDate>Sun, 21 Aug 2016 21:34:17 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/</guid><description>You&amp;rsquo;re a hotshot manager. You love your dashboards and you keep your finger on the beating pulse of the business. You take pride in using data to drive your decisions rather than shooting from the hip like one of those old-school 1950s bosses. This is the 21st century, and data is king. You even hired a sexy statistician or data scientist, though you don&amp;rsquo;t really understand what they do. Never mind, you can proudly tell all your friends that you are leading a modern data-driven team.</description></item><item><title>Is Data Scientist a useless job title?</title><link>https://yanirs.github.io/yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/</link><pubDate>Thu, 04 Aug 2016 22:26:03 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/</guid><description>Data science can be defined as either the intersection or union of software engineering and statistics. In recent years, the field seems to be gravitating towards the broader unifying definition, where everyone who touches data in some way can call themselves a data scientist. Hence, while many people whose job title is Data Scientist do very useful work, the title itself has become fairly useless as an indication of what the title holder actually does.</description></item><item><title>Making Bayesian A/B testing more accessible</title><link>https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/</link><pubDate>Sun, 19 Jun 2016 10:32:15 +0000</pubDate><guid>https://yanirs.github.io/yanirseroussi.com/2016/06/19/making-bayesian-ab-testing-more-accessible/</guid><description>Much has been written in recent years on the pitfalls of using traditional hypothesis testing with online A/B tests. A key issue is that you&amp;rsquo;re likely to end up with many false positives if you repeatedly check your results and stop as soon as you reach statistical significance. One way of dealing with this issue is by following a Bayesian approach to deciding when the experiment should be stopped. While I find the Bayesian view of statistics much more intuitive than the frequentist view, it can be quite challenging to explain Bayesian concepts to laypeople.</description></item></channel></rss>