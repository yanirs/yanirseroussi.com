<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>machine learning on Yanir Seroussi | Data science and beyond</title><link>https://yanirseroussi.com/tags/machine-learning/</link><description>Recent content in machine learning on Yanir Seroussi | Data science and beyond</description><generator>Hugo -- gohugo.io</generator><language>en-au</language><copyright>Text and figures licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) by [Yanir Seroussi](https://yanirseroussi.com/about/), except where noted otherwise&amp;nbsp;&amp;nbsp;|</copyright><lastBuildDate>Sun, 20 Mar 2022 04:30:00 +0000</lastBuildDate><atom:link href="https://yanirseroussi.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Building useful machine learning tools keeps getting easier: A fish ID case study</title><link>https://yanirseroussi.com/2022/03/20/building-useful-machine-learning-tools-keeps-getting-easier-a-fish-id-case-study/</link><pubDate>Sun, 20 Mar 2022 04:30:00 +0000</pubDate><guid>https://yanirseroussi.com/2022/03/20/building-useful-machine-learning-tools-keeps-getting-easier-a-fish-id-case-study/</guid><description>Lessons learned building a fish ID web app with fast.ai and Streamlit, in an attempt to reduce my fear of missing out on the latest deep learning developments.</description></item><item><title>Use your human brain to avoid artificial intelligence disasters</title><link>https://yanirseroussi.com/2021/11/22/use-your-human-brain-to-avoid-artificial-intelligence-disasters/</link><pubDate>Mon, 22 Nov 2021 03:45:00 +0000</pubDate><guid>https://yanirseroussi.com/2021/11/22/use-your-human-brain-to-avoid-artificial-intelligence-disasters/</guid><description>Overview of a talk I gave at a deep learning course, focusing on AI ethics as the need for humans to think on the context and consequences of applying AI.</description></item><item><title>Defining data science in 2018</title><link>https://yanirseroussi.com/2018/07/22/defining-data-science-in-2018/</link><pubDate>Sun, 22 Jul 2018 08:27:43 +0000</pubDate><guid>https://yanirseroussi.com/2018/07/22/defining-data-science-in-2018/</guid><description>I got my first data science job in 2012, the year Harvard Business Review announced data scientist to be the sexiest job of the 21st century. Two years later, I published a post on my then-favourite definition of data science, as the intersection between software engineering and statistics. Unfortunately, that definition became somewhat irrelevant as more and more people jumped on the data science bandwagon – possibly to the point of making data scientist useless as a job title.</description></item><item><title>Why you should stop worrying about deep learning and deepen your understanding of causality instead</title><link>https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/</link><pubDate>Sun, 14 Feb 2016 11:04:11 +0000</pubDate><guid>https://yanirseroussi.com/2016/02/14/why-you-should-stop-worrying-about-deep-learning-and-deepen-your-understanding-of-causality-instead/</guid><description>Everywhere you go these days, you hear about deep learning&amp;rsquo;s impressive advancements. New deep learning libraries, tools, and products get announced on a regular basis, making the average data scientist feel like they&amp;rsquo;re missing out if they don&amp;rsquo;t hop on the deep learning bandwagon. However, as Kamil Bartocha put it in his post The Inconvenient Truth About Data Science, 95% of tasks do not require deep learning. This is obviously a made up number, but it&amp;rsquo;s probably an accurate representation of the everyday reality of many data scientists.</description></item><item><title>Miscommunicating science: Simplistic models, nutritionism, and the art of storytelling</title><link>https://yanirseroussi.com/2015/10/19/nutritionism-and-the-need-for-complex-models-to-explain-complex-phenomena/</link><pubDate>Mon, 19 Oct 2015 00:02:32 +0000</pubDate><guid>https://yanirseroussi.com/2015/10/19/nutritionism-and-the-need-for-complex-models-to-explain-complex-phenomena/</guid><description>I recently finished reading the book In Defense of Food: An Eater&amp;rsquo;s Manifesto by Michael Pollan. The book criticises nutritionism – the idea that one should eat according to the sum of measured nutrients while ignoring the food that contains these nutrients. The key argument of the book is that since the knowledge derived using food science is still very limited, completely relying on the partial findings and tools provided by this science is likely to lead to health issues.</description></item><item><title>The wonderful world of recommender systems</title><link>https://yanirseroussi.com/2015/10/02/the-wonderful-world-of-recommender-systems/</link><pubDate>Fri, 02 Oct 2015 05:25:57 +0000</pubDate><guid>https://yanirseroussi.com/2015/10/02/the-wonderful-world-of-recommender-systems/</guid><description>I recently gave a talk about recommender systems at the Data Science Sydney meetup (the slides are available here). This post roughly follows the outline of the talk, expanding on some of the key points in non-slide form (i.e., complete sentences and paragraphs!). The first few sections give a broad overview of the field and the common recommendation paradigms, while the final part is dedicated to debunking five common myths about recommender systems.</description></item><item><title>Learning about deep learning through album cover classification</title><link>https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/</link><pubDate>Mon, 06 Jul 2015 22:21:42 +0000</pubDate><guid>https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/</guid><description>In the past month, I&amp;rsquo;ve spent some time on my album cover classification project. The goal of this project is for me to learn about deep learning by working on an actual problem. This post covers my progress so far, highlighting lessons that would be useful to others who are getting started with deep learning.
Initial steps summary The following points were discussed in detail in the previous post on this project.</description></item><item><title>Hopping on the deep learning bandwagon</title><link>https://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/</link><pubDate>Sat, 06 Jun 2015 05:00:22 +0000</pubDate><guid>https://yanirseroussi.com/2015/06/06/hopping-on-the-deep-learning-bandwagon/</guid><description>I&amp;rsquo;ve been meaning to get into deep learning for the last few years. Now, the stars having finally aligned and I have the time and motivation to work on a small project that will hopefully improve my understanding of the field. This is the first in a series of posts that will document my progress on this project.
As mentioned in a previous post on getting started as a data scientist, I believe that the best way of becoming proficient at solving data science problems is by getting your hands dirty.</description></item><item><title>First steps in data science: author-aware sentiment analysis</title><link>https://yanirseroussi.com/2015/05/02/first-steps-in-data-science-author-aware-sentiment-analysis/</link><pubDate>Sat, 02 May 2015 08:31:10 +0000</pubDate><guid>https://yanirseroussi.com/2015/05/02/first-steps-in-data-science-author-aware-sentiment-analysis/</guid><description>People often ask me what&amp;rsquo;s the best way of becoming a data scientist. The way I got there was by first becoming a software engineer and then doing a PhD in what was essentially data science (before it became such a popular term). This post describes my first steps in the field with the goal of helping others who are interested in making the transition from pure software engineering to data science.</description></item><item><title>Learning to rank for personalised search (Yandex Search Personalisation – Kaggle Competition Summary – Part 2)</title><link>https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/</link><pubDate>Wed, 11 Feb 2015 06:34:17 +0000</pubDate><guid>https://yanirseroussi.com/2015/02/11/learning-to-rank-for-personalised-search-yandex-search-personalisation-kaggle-competition-summary-part-2/</guid><description>This is the second and last post summarising my team&amp;rsquo;s solution for the Yandex search personalisation Kaggle competition. See the first post for a summary of the dataset, evaluation approach, and some thoughts about search engine optimisation and privacy. This post discusses the algorithms and features we used.
To quickly recap the first post, Yandex released a 16GB dataset of query &amp;amp; click logs. The goal of the competition was to use this data to rerank query results such that the more relevant results appear before less relevant results.</description></item><item><title>Is thinking like a search engine possible? (Yandex search personalisation – Kaggle competition summary – part 1)</title><link>https://yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/</link><pubDate>Thu, 29 Jan 2015 10:37:39 +0000</pubDate><guid>https://yanirseroussi.com/2015/01/29/is-thinking-like-a-search-engine-possible-yandex-search-personalisation-kaggle-competition-summary-part-1/</guid><description>About a year ago, I participated in the Yandex search personalisation Kaggle competition. I started off as a solo competitor, and then added a few Kaggle newbies to the team as part of a program I was running for the Sydney Data Science Meetup. My team hasn&amp;rsquo;t done too badly, finishing 9th out of 194 teams. As is usually the case with Kaggle competitions, the most valuable part was the lessons learned from the experience.</description></item><item><title>Stochastic Gradient Boosting: Choosing the Best Number of Iterations</title><link>https://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</link><pubDate>Mon, 29 Dec 2014 02:30:06 +0000</pubDate><guid>https://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/</guid><description>In my summary of the Kaggle bulldozer price forecasting competition, I mentioned that part of my solution was based on stochastic gradient boosting. To reduce runtime, the number of boosting iterations was set by minimising the loss on the out-of-bag (OOB) samples, skipping trees where samples are in-bag. This approach was motivated by a bug in scikit-learn, where the OOB loss estimate was calculated on the in-bag samples, meaning that it always improved (and thus was useless for the purpose of setting the number of iterations).</description></item></channel></rss>